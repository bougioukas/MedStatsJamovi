[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Medical Statistics with Jamovi",
    "section": "",
    "text": "Preface\nThis is a working draft.\nThis online textbook is for medical students, doctors, medical researchers, nurses, members of professions allied to medicine, and all others concerned with medical data.\nWhile statistics books focus on mathematics, this textbook focuses on using a computer to conduct data analysis. That means using a statistical software program, in this case the Jamovi software for statistics and graphics. Our aim is to keep a balance between mathematical rigor and readability as well as learning Jamovi and statistics simultaneously.\nMost of the examples discussed in this textbook are based on scientific studies whose data are publicly available. For each example, we provide the step-by-step application in Jamovi. Readers are encouraged to follow these steps while reading the textbook so that they can learn statistics through hands-on experience.\nAll sections of this textbook are reproducible as they were made using Quarto® which is an open-source scientific and technical publishing system built on Pandoc.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Medical Statistics with Jamovi",
    "section": "License",
    "text": "License\nThis textbook is free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "estimation.html#estimation-of-population-parametres",
    "href": "estimation.html#estimation-of-population-parametres",
    "title": "1  Estimation and Confidence Intervals",
    "section": "\n1.1 Estimation of population parametres",
    "text": "1.1 Estimation of population parametres\nPopulation and parameters\nIn the statistical sense a population is a theoretical concept used to describe an entire group of individuals (not necessarily people) that share a set of characteristics. Examples are the population of all patients with diabetes mellitus, all people with depression, or the population of all middle-aged women.\nResearchers are specifically interested in quantities such as population mean and population variance of random variables (characteristics) of such populations. These quantities are unknown in general. We refer to these unknown quantities as parameters. Here, we use parameters \\(μ\\) and \\(σ^2\\) to denote the unknown population mean and variance respectively. For example, researchers want to know what the mean depression score for the population would be if all people with depression were treated with a new anti-depression treatment.\nNote that for all the distributions we discussed in the previous chapter, the population mean and variance of a random variable are related to the unknown parameters of probability distribution assumed for that random variable. Indeed, for normal distributions \\(N(μ,σ^2)\\), which are widely used in statistics, the population mean and variance are exactly the same parameters used to specify the distribution.\n\n\n\n\n\n\nParametric Tests\n\n\n\nThe term parameter for a population characteristic explains why most of the statistical tests in this textbook are referred to as parametric tests. These tests are based on assumptions about the shape of the distribution and the parameters (i.e. mean and standard deviation), and most rely on the assumption of an approximately normal distribution.\n\n\n \nSample and sample statistics\nResearchers want to know the population parameter, the value that would be obtained if the entire population were actually studied. Of course, they don’t usualy have the resources and time to study, for example, every individual with depression in the world, so a population parameter value is generally not available. They must instead study a sample, a subset of the population that is intended to represent the population. In this case a sample statistic is used to estimate a population parameter (also named as point estimator since we estimate the parameter by a single value or point).\nIn most cases, the best way to get a sample that accurately represents the population is by taking a random sample from the population (Figure 1.1). When selecting a random sample, each individual in the population has the same chance of being selected for the sample.\n\n\n\n\nFigure 1.1: Parameters are referred to the population while statistics are referred to the sample.\n\n\n\n \nUsing the sample to etimate a population parameter\nThe researchers use the sample statistic value as an estimate of the population parameter value. The researchers are making an inference that the sample statistic is a value similar to the population parameter value based on the premise that the characteristics of those in the sample are similar to the characteristics of those in the entire population. When researchers use a sample statistic to infer the value of a population parameter, it is called inferential statistics.\nThe process is represented schematically in Figure 1.2. So, a sample is selected from population of interest to provide an estimate of a population parameter by using a sample statistic. If the sampling method used is random sample then we obtain an unbiased estimate of the population parameter.\n\n\n\n\nflowchart TB  \n    A((population)) -- sampling method --&gt; B((sample))\n    A -.-&gt; C[Population parameter]\n    B --&gt; D[\"point estimator &lt;br&gt; (a sample statistic)\"]\n    D --&gt; C\n\n\n\nFigure 1.2: Taking a sample of the population and using the sample to estimate a population parameter\n\n\n\nIn some circumstances the sample may consist of all the members of a specifically defined population. For practical reasons, this is only likely to be the case if the population of interest is not too large. If all members of the population can be assessed, then the estimate of the parameter concerned is derived from information obtained on all members and so its value will be the population parameter itself. The dotted arrow in Figure 1.2 connecting the population circle to population parameter box illustrates this. However, this situation will rarely be the case so, in practice, we take a sample which is often much smaller in size than the population under study.\n\n\n\n\n\n\nEstimation of population parametres\n\n\n\nPoint Estimation refers to the process of guessing the unknown value of a parameter (e.g., population mean, \\(\\mu\\)) using the observed data. For this, we use a sample statistic as a point estimator of the population parameter. Statistics do not depend on any unknown parameter, and given the observed data, we should be able to find their values. For example, the sample mean and sample variance are statistics.\n\n\nSome common population parameters (Greek Letters) and their corresponding sample statistics (or estimates) are described in Table 1.1.\n\n\nTable 1.1: Population parameters (Greek letters) and sample statistics\n\n\nPopulation parameter\nSample statistic\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\ns\n\n\nProportion\n\\(\\pi\\)\np\n\n\nRate\n\\(\\lambda\\)\nr"
  },
  {
    "objectID": "estimation.html#sample-distribution-vs-sampling-distribution",
    "href": "estimation.html#sample-distribution-vs-sampling-distribution",
    "title": "1  Estimation and Confidence Intervals",
    "section": "\n1.2 Sample Distribution vs Sampling Distribution",
    "text": "1.2 Sample Distribution vs Sampling Distribution\nSample Distribution\nThe sample distribution is simply the data distribution of the sample which is randomly taken from the population. We can calculate a sample statistic such as the sample mean from the data in the sample.\n\n\n\n\nFigure 1.3: Sample distribution of data from one sample.\n\n\n\n \nSampling Distribution\nThe sampling distribution is the distribution of the sample statistic (e.g., the sample mean) over many samples drawn from the same population (i.e., repeated sampling).\n\n\n\n\nFigure 1.4: Sampling distribution of a statistic (e.g. sample mean) from many samples.\n\n\n\n\n\n\n\n\n\nSampling distribution\n\n\n\nThe sampling distribution is the theoretical distribution of possible values for a sample statistic."
  },
  {
    "objectID": "estimation.html#what-is-standard-error-se-of-the-mean",
    "href": "estimation.html#what-is-standard-error-se-of-the-mean",
    "title": "1  Estimation and Confidence Intervals",
    "section": "\n1.3 What is Standard Error (SE) of the mean?",
    "text": "1.3 What is Standard Error (SE) of the mean?\nThe standard deviation of the sampling distribution is known as the standard error (SE). There are multiple formulas for standard error depending of exactly what is our sampling distribution. The standard error of a mean is the population σ divided by the square root of the sample size n:\n\\[ SE = \\frac{\\sigma}{\\sqrt{n}} \\tag{1.1}\\]\nHowever, because we usually do not have access to the population parameter σ, we instead use the sample standard deviation sd, as it is an estimate of the population standard deviation.\n\\[ SE = \\frac{sd}{\\sqrt{n}}\\]\nThe Standard Error (SE) of the mean is a metric that describes the variability of sample means in the sampling distribution. SE gives us an indication of the uncertainty attached to the estimate of the population mean when taking only a sample - very uncertain when the sample size is small."
  },
  {
    "objectID": "estimation.html#central-limit-theorem-clm",
    "href": "estimation.html#central-limit-theorem-clm",
    "title": "1  Estimation and Confidence Intervals",
    "section": "\n1.4 Central Limit Theorem (CLM)",
    "text": "1.4 Central Limit Theorem (CLM)\nThe Central Limit Theorem (CLM) in statistics states that, given a sufficiently large sample size, the sampling distribution of the mean for a variable will approximate a normal distribution regardless of that variable’s distribution in the population.\nA hypothetical population\nThe importance of central limit theorem is that it works no matter the underlying distribution of the population data. The underlying population data could be noisy and central limit theorem will still hold. To illustrate this, let’s draw some data (100,000 observations):\n\n\n\n\nFigure 1.5: A hypothetical population of 100,000 observations.\n\n\n\nHere are some descriptive statistics to show how ugly these data are:\n\n\n   vars     n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 1e+05 40.16 40.31     24   37.71 35.58   0 100   100 0.39    -1.56 0.13\n\n\nIf we knew nothing else from the data other than the descriptive statistics above, we would likely guess the data would look anything other than “normal” no matter how many different values there are. There is a clear bimodality problem in these data. Namely, that “average” (i.e. the mean) doesn’t look “average” at all.\nThe data, we have just created above, will serve as the entire population (N=100,000) of data from which we can sample.\nThe Sampling Distribution of the means\nNow, what if we get 50,000 samples, each sample consisting of just 10 observations, save the means of those samples, and draw their histogram?\nThe distribution of sample means (as a density plot) converges on a normal distribution where the provided location and scale parameters are from 50,000 sample means. Further, the center of the distribution is converging on the known population mean. The true population mean 40.16 (red dashed line) is very close to the mean of the 50,000 sample means.\n\n\n\n\nFigure 1.6: The distribution of 50,000 sample means, each of size=10.\n\n\n\n \n\n\n\n\n\n\nProperties of the sampling distribution of the mean\n\n\n\n\nThe mean of the sampling distribution is the same as the mean of the population.\nThe standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases.\nThe shape of the sampling distribution becomes normal as the sample size increases regardless of the population distribution of the variable."
  },
  {
    "objectID": "estimation.html#the-confidence-intervals",
    "href": "estimation.html#the-confidence-intervals",
    "title": "1  Estimation and Confidence Intervals",
    "section": "\n1.5 The confidence intervals",
    "text": "1.5 The confidence intervals\nWe will base the definition of confidence interval on two ideas:\n\nOur point estimate (e.g., mean from the sample) is the most plausible value of the actual parameter, so it makes sense to build the confidence interval around the point estimate.\nThe plausibility of a range of values can be defined from the sampling distribution of the estimate.\n\nFor the case of the mean, the Central Limit Theorem states that its sampling distribution is Normal. Additionally, recall that the standard deviation of the sampling distribution of the mean is the standard error (SE) of the mean.\nIn this case, and in order to define an interval, we can make use of a well-known result from probability that applies to normal distributions: 95% of the distribution of sample means lies within \\(\\pm 1.96\\) standard errors (the standard deviation of this distribution) of the population mean.\nIf the interval spreads out \\(\\pm 1.96\\) standard errors from a normally distributed point estimate, intuitively we can say that we are roughly 95% confident that we have captured the true but unknown population parameter.\nThe formula for the confidence interval (CI) of mean looks like this:\n\\[ 95\\%CI=\\bar{x} \\ \\pm 1.96 \\ SE_{\\bar{x}} = \\bar{x} \\ \\pm 1.96  \\frac{\\sigma}{\\sqrt{n}}  \\tag{1.2}\\]\nand if the population standard deviation σ is unknown, the the sample standard deviation, sd, is used in the formula Equation 1.2:\n\\[ 95\\%CI=\\bar{x} \\ \\pm 1.96 \\ SE_{\\bar{x}} = \\bar{x} \\ \\pm 1.96  \\frac{sd}{\\sqrt{n}}  \\tag{1.3}\\]\nThe real meaning of “confidence” is not evident and it must be understood from the point of view of the generating process. The confidence interval is based on the concept of repetition of the study under consideration. Thus, suppose we took many (infinite) samples from a population and built a 95% confidence interval from each sample. Then about 95% of those intervals would contain the population parameter.\nNow, we can present the confidence intervals of 100 randomly generated samples of size = 10 from our hypothetical population (Figure 1.7). Each horizontal bar is a confidence interval (CI), centered on a sample mean (point). The intervals all have the same length, but are centered on different sample means as a result of random sampling from the population. The five bold confidence intervals do not cover the true population mean (the vertical red dashed line \\(\\mu\\) = 40.16). This is what we would expect using a 95% confidence level– approximately 95% of the intervals covering the population mean.\n\n\n\n\nFigure 1.7: 100 Sample Means of Size 10 (with 95% Intervals) from the Population.\n\n\n\nNext, we will create the confidence intervals of 100 randomly generated samples of size = 50 from our population (Figure 1.8):\n\n\n\n\nFigure 1.8: 100 Sample Means of Size 50 (with 95% Intervals) from the Population.\n\n\n\nIncreasing the sample size not only converges the sample statistic (the points) on the population parameter (red dashed line) but decreases the uncertainty around the estimate (the CIs become narrower)."
  },
  {
    "objectID": "lab4.html#the-sampling-distribution-of-mean-and-the-central-limit-theorem-clt",
    "href": "lab4.html#the-sampling-distribution-of-mean-and-the-central-limit-theorem-clt",
    "title": "2  LAB IV: Estimation and Confidence Interval",
    "section": "\n2.1 The Sampling Distribution of mean and the Central Limit Theorem (CLT)",
    "text": "2.1 The Sampling Distribution of mean and the Central Limit Theorem (CLT)\nIn this Lab we will learn the Central Limit Theorem (CLT), which is the basis for many statistical concepts. We are going to explore this concept with the help of a Shiny application. So, clink on the following link CLM.\nA Shiny app opens in a web window as shown below (Figure 2.1):\n\n\nFigure 2.1: The Shiny application simulating the Central limit Theorem for Means\n\nTo the left is the interactive panel with radio buttons and slider bars, and to the right there are three tabs: Population Distribution, Samples, and Sampling Distribution.\nFirst we are asked to choose from a Normal, Uniform, Right Skewed or Left Skewed Population distribution (Parent distribution) from the left panel. Let’s select Right skewed and then High skew from the drop down menu with the name Skew (Figure 2.2).\n\n\nFigure 2.2: The case of a high right skewed population distribution\n\nNext we set the value of the Sample size slider bar to 5 and the Number of samples to 1000 and we select the Sampling distribution tab. We observe that the sampling distribution is right skewed with mean approximately equal to population mean (Figure 2.3).\n\n\nFigure 2.3: Distribution of means of 1000 random samples, each consisting of 5 observations from a high right skewed population distribution\n\nNow, try to increase the sample size to 30 (Figure 2.4):\n\n\nFigure 2.4: Distribution of means of 1000 random samples, each consisting of 30 observations from a high right skewed population distribution\n\nand then increase it to 200 (Figure 2.5):\n\n\nFigure 2.5: Distribution of means of 1000 random samples, each consisting of 200 observations from a high right skewed population distribution\n\nWe observe that the sampling distribution becomes closer and closer to Normal and the standard error of the mean, SE, (the standard deviation of sample means) gets smaller as the sample size increases. The important point is that whatever the parent distribution of a variable, the distribution of the sample means will be nearly Normal, as long as the samples are large enough.\n \n\n\n\n\n\n\nProperties of the sampling distribution of the mean\n\n\n\n\nThe mean of the sampling distribution is the same as the mean of the population.\nThe standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases.\n\nAccording to the Central Limit Theorem (CLM), the shape of the sampling distribution becomes normal as the sample size increases regardless of the variable’s population distribution."
  },
  {
    "objectID": "lab4.html#the-confidence-interval-of-mean",
    "href": "lab4.html#the-confidence-interval-of-mean",
    "title": "2  LAB IV: Estimation and Confidence Interval",
    "section": "\n2.2 The confidence interval of mean",
    "text": "2.2 The confidence interval of mean\nWe are going to explore the concept of confidence interval (CI) of mean with the help of a Shiny application. So, clink on the following link CIs.\nA Shiny app opens in a web window as shown below (Figure 2.6):\n\n\nFigure 2.6: Shiny application that simulates the concept of confidence interval (CI) of mean\n\nTo the left is the interactive panel with radio buttons and drop down menus, and to the right there are two tabs: Plots and About.\nFirst we are asked to choose if we want the Confidence Interval Graph only or the Confidence Interval Graph Plus Sampling Distribution of the Mean. Let’s select the first choice and set the value of the Number of Simulated Samples to one and the Sample Size to 10 from the drop down menus (Figure 2.7).\nWe observe at the Plot tab that a horizontal bar has been created which represents the confidence interval (CI), centered on the sample mean (point). In this case, the 95%CI for the sample mean includes the true value of the population mean (it crosses the solid vertical line) and it is drawn as a black line (Figure 2.7).\n\n\nFigure 2.7: Confidence Interval Graph with one random sample of 10 observations selected from a normal population distribution\n\nNow, try to increase the Number of Simulated Samples to 100 (Figure 2.8):\n\n\nFigure 2.8: Confidence Interval Graph with 100 random samples, each consisting of 10 observations from a normal population distribution\n\nWe observe that 5 out of 100 confidence intervals (red horizontal lines) do not include the true population mean (the solid vertical line) (Figure 2.8). This is what we would expect – that the 95% confidence interval will not include the true population mean 5% of the time.\n\n\n\n\n\n\n95%CI of the mean interpretation\n\n\n\nFor 95% of the calculated confidence intervals it will be true to say that the population mean, μ, lies within this interval. The problem is, as Figure 2.8 shows, with a single study we just do not know which one of these 100 intervals we will obtain and hence we will not know if it includes μ. Unfortunately, the CIs ARE NOT pre-labelled with “I am poor CI and do not include the population mean: do not choose me!”. Therefore, we usually interpret a 95% confidence interval of mean as the range of values within which we are 95% confident that the true population mean lies.\n\n\nNext, we create the confidence intervals of 100 randomly generated samples of size = 50 from the population (Figure 2.9):\n\n\nFigure 2.9: Confidence Interval Graph with 100 random samples, each consisting of 50 observations from a normal population distribution\n\nWe observe that the sample means are closer to the true population mean and the 95%CIs of the mean become narrower (Figure 2.8) increasing the sample size."
  }
]