[
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Medical Statistics with Jamovi",
    "section": "License",
    "text": "License\nThis textbook is free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "When we have finished this chapter, we should be able to:"
  },
  {
    "objectID": "introduction.html#how-to-use-this-online-textbook",
    "href": "introduction.html#how-to-use-this-online-textbook",
    "title": "1  Introduction",
    "section": "\n1.1 How to use this online textbook",
    "text": "1.1 How to use this online textbook\nThis online textbook includes two Parts, “Medical Statistics” and “Jamovi LAB”. For each theoretical chapter (e.g., “Descriptive Statistics”) in first part there is a practical lab with Jamovi in the second part (e.g., “Lab II:Descriptive Statistics”).\nAdditionally, throughout this online textbook there are some inserts or callouts (colored “boxes”) that are designed to bring out some of the important information and help our learning. These inserts are distinguished by different colors:\n\\(\\color{green}{Green\\ color}:\\) Tips and notes that provide important information for a topic.\n\n\n\n\n\n\nA title\n\n\n\nSome tip or important information.\n\n\n \n\\(\\color{orange}{Orange\\ color}:\\) Objectives of the chapter.\n\n\n\n\n\n\nLearning objectives\n\n\n\n\nObjective 1\nObjective 2\nObjective 3\n\n\n\n \n\\(\\color{orange}{Orange\\ color\\ with\\ a\\ rocket\\ icon}:\\) Additional information that can be expanded by the user (click on it).\n\n\n\n\n\n\n A title\n\n\n\n\n\nSome additional information.\n\n\n\n \n\\(\\color{#CC5500}{Deep\\ Orange\\ color}:\\) Assumptions for a statistical test.\n\n\n\n\n\n\nA title\n\n\n\nSome assumptions.\n\n\n \n\\(\\color{red}{Red\\ color\\ with\\ an\\ exclamation\\ mark\\ icon}:\\) Point caution that we should pay attention to.\n\n\n\n\n\n\nA title\n\n\n\nSome point of caution.\n\n\n \n\\(\\color{blue}{Blue\\ color}:\\) Examples provide step-by-step approaches to answering questions.\n\n\n\n\n\n\nExample: A title\n\n\n\nSome example.\n\n\n \n\\(\\color{grey}{Grey\\ color}:\\) Exercises with embedded answers.\n\n\n\n\n\n\nExercise 1.1\n\n\n\nThis is the first exercise.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis is a possible answer to the exercise."
  },
  {
    "objectID": "introduction.html#statistics-and-medicine",
    "href": "introduction.html#statistics-and-medicine",
    "title": "1  Introduction",
    "section": "\n1.2 Statistics and Medicine",
    "text": "1.2 Statistics and Medicine\nImportance of Statistics in Medical field\nAlthough some healthcare professionals may not carry out medical research, they will definitely be consumers of medical research. Thus, it is incumbent on them to be able to discern high quality research studies from low quality, to be able to verify whether the conclusions of a study are valid and to understand the limitations in methods of a study. The current emphasis on evidence-based medicine (EBM) requires that healthcare professionals consider critically all evidence about whether a specific treatment works and this requires basic statistical knowledge.\nStatistics is not only a discipline in its own right but it is also a fundamental tool for investigation in all biological and medical sciences. As such, any serious investigator in these fields must have a grasp of the basic principles. With modern computer facilities there is little need for familiarity with the technical details of statistical calculations. However, a healthcare professional should understand when such calculations are valid, when they are not and how they should be interpreted.\nThe use of statistical methods pervades the medical literature. In a survey of 350 original articles published in three UK journals of general practice: British Medical Journal (General Practice Section), British Journal of General Practice and Family Practice, over a one-year period, Rigby et al. (2004) found that 66% used some form of statistical analysis. Another review by Strasak et al. (2007) of 91 original research articles published in The New England Journal of Medicine (one of the prestigious peer-reviewed medical journals) found an even higher percentage (95%) of using inferential statistics, for example, hypothesis testing and deriving estimates. It appears, therefore, that the majority of papers published in these journals require some statistical knowledge for a complete understanding.\nWhy use Statistics?\nTo students schooled in the ‘hard’ sciences of physics and chemistry it may be difficult to appreciate the variability of biological data. If one repeatedly puts blue litmus paper into acid solutions it turns red 100% of the time, not most (say 95%) of the time. In contrast, if one gives aspirin to a group of people with headaches, not all of them will experience relief. Penicillin was perhaps one of the few ‘miracle’ cures where the results were so dramatic that little evaluation was required. Absolute certainty in medicine is rare.\nMeasurements on human subjects seldom give exactly the same results from one occasion to the next. For example, O’Sullivan et al. (1999), found that systolic blood pressure (SBP) in normal healthy children has a wide range, with 95% of children having SBPs below 130 mmHg when they were resting, rising to 160 mmHg during the school day, and falling again to below 130 mmHg at night. Furthermore, Hansen et al. (2010) in a study of over 8000 subjects found that increasing variability in blood pressure over 24 hours was a significant and independent predictor of mortality and cardiovascular and stroke events.\nThis variability is also inherent in responses to biological hazards. Most people now accept that cigarette smoking causes lung cancer and heart disease, and yet nearly everyone can point to an apparently healthy 80-year-old who has smoked for many years without apparent ill effect. Although it is now known from the report of Doll et al. (2004) that about half of all persistent cigarette smokers are killed by their habit, it is usually forgotten that until the 1950s, the cause of the rise in lung cancer deaths was a mystery and commonly associated with general atmospheric pollution such as the exhaust fumes of cars. It was not until the carefully designed and statistically analysed case–control and cohort studies of Richard Doll and Austin Bradford Hill and others, that smoking was identified as the true cause. Enstrom (2003) moved the debate on to ask whether or not passive smoking causes lung cancer. This is a more difficult question to answer since the association is weaker. However, an overview of reviews by Cao et al. (2015) has now shown that it is a major health problem and scientists at the International Agency for Research on Cancer (IARC) have concluded that there is sufficient evidence that second-hand smoke causes lung cancer (IARC 2012). Restrictions on smoking in public places have been imposed to smokers.\nWith such variability, it follows that in any comparison made in a medical context, such as people on different treatments, differences are almost bound to occur. These differences may be due to real effects, random variation or variation in some other factor that may affect an outcome. It is the job of the analyst to decide how much variation should be ascribed to chance or other factors, so that any remaining variation can be assumed to be due to a real effect. This is the art of statistics."
  },
  {
    "objectID": "introduction.html#the-discipline-of-statistics",
    "href": "introduction.html#the-discipline-of-statistics",
    "title": "1  Introduction",
    "section": "\n1.3 The discipline of statistics",
    "text": "1.3 The discipline of statistics\nStatistics is an empirical or practical method for collecting, organizing, summarizing, and presenting data, and for making inferences about the population from which the data are drawn.\nThe discipline of statistics includes two main branches:\n\ndescriptive statistics\ninferential statistics\n\n\n\n\n\n\nflowchart LR\n  \n    A[Statistics]--- B[Descriptive statistics]\n    A --- C[Inferential statistics]\n    B --- D[Measures of frequency: <br/> e.g., frequency, percentage.]\n    B --- E[Measures of location <br/> and dispersion: <br/> e.g., mean, standard deviation.]\n    C --- H[Estimation]\n    C --- I[Hypothesis Testing]\n    style A color:#980000, stroke:#333,stroke-width:4px\n    \n    \n\n\n\n\n\nFigure 1.1: The discipline of statistics and its two branches, descriptive statistics and inferential statistics\n\n\n\n\nDescriptive Statistics\nThe methods and techniques of descriptive statistics aim at summarizing large quantities of data by a few numbers, in a way that highlights the most important numerical features of the data.\nDescriptive statistics include measures of frequency and measures of location and dispersion. They also include a description of the general shape of the distribution of the data. These terms will be explained in Chapter 2.\nInferential Statistics\nInferential statistics include estimation and hypothesis testing. They aim at generalizing a measure taken on a small number of cases that have been observed, to a larger set of cases that have not been observed. We could reformulate this aim, and say that inferential statistics aim at generalizing observations made on a sample to a whole population (see Chapter 4 and Chapter 5)."
  },
  {
    "objectID": "introduction.html#data-and-variables",
    "href": "introduction.html#data-and-variables",
    "title": "1  Introduction",
    "section": "\n1.4 Data and Variables",
    "text": "1.4 Data and Variables\nBiomedical Data\nBiomedical data have unique features compared with data in other domains. The data may include administrative health data, biomarker data, biometric data (for example, from wearable technologies) and imaging, and may originate from many different sources, including Electronic Health Records (EHRs), clinical registries, biobanks, the internet and patient self-reports.\nBiomedical data can be transformed into information. This information can become knowledge if the researchers and clinicians understand it (Figure 1.2).\n\n\n\n\nFigure 1.2: From data to knowledge.\n\n\n\n\nThere are three main data structures: structured data, unstructured data, and semi-structured data.\nStructured data is generally tabular data that is represented by columns and rows in a database.\nSemi-Structured data is a form of structured data that does not obey the tabular structure, yet does have some structural properties. Emails, for example, are semi-structured by sender, recipient,subject, date, time etc. and they are also organized into folders, like Inbox, Sent, Trash, etc.\nUnstructured data usually open text (such as social media posts), images, videos, etc., that have no predetermined organization or design.\nIn this textbook we use data organized in a structured format (spreadsheets). In statistics, tabular data refers to data that are organized in a table with rows and columns. A row is a observation (or record), which corresponds to the statistical unit of the dataset. The columns are the variables (or characteristics) of interest.\n \nVariables\nA variable is a quantity or property that is free to vary, or take on different values. To gain information on a variable, it is necessary to design and conduct experiments.\nResearchers design experiments to test if changes to one or more variables are associated with changes to another variable of interest. For example, if researchers think a new therapy is more effective than the usual care for the treatment of foot corns, they could design an experiment to test this hypothesis; participants should randomly assigned to one of two groups: one (the experimental group) receiving the new treatment that is being tested, and the other (the comparison group or control) receiving an alternative (conventional) treatment. In this experiment, the type of treatment each participant received (i.e., new treatment vs. conventional treatment) is the independent variable (IV), while the number of patients with completely healed corns is the dependent variable (DV) or the outcome variable.\nIn a randomized controlled trial (RCT), Farndon et al. (2013) investigated the effectiveness of salicylic acid plasters compared with usual scalpel debridement for treatment of foot corns at three months. They recorded 20 variables for 202 patients; the data are presented in the table of Figure 1.3:\n\n\n\n\n\n\nExample: Raw data of 20 variables for 202 patients with foot corns treated with salicylic acid plasters or scalpel debridement in Farndon et al. (2013) study\n\n\n\n\n\n\nFigure 1.3: Table with raw data as recorded in Farndon et al. (2013) study.\n\n\n\nIn this example, the main independent variable of interest is the group (salicylic acid plasters Vs usual scalpel debridement) and the dependent variable is the complete_healed (completely healed corns at three months post-randomization).\n\n\n\n\n\n\n\n\nIndependent Vs Dependent variables\n\n\n\nAn independent variable is the variable that is changed or controlled in a scientific experiment to test the effects on another variable.\nA dependent (outcome) variable is the variable being tested in a scientific experiment and is affected by the independent variable(s) of interest."
  },
  {
    "objectID": "introduction.html#types-of-data",
    "href": "introduction.html#types-of-data",
    "title": "1  Introduction",
    "section": "\n1.5 Types of Data",
    "text": "1.5 Types of Data\nData in variable can be either categorical or numerical (otherwise known as qualitative and quantitative) in nature (Figure 1.4).\n\n\n\n\nFigure 1.4: Broad classification of the different types of data with examples.\n\n\n\n\n \n\n\n\n\n\n\nNote\n\n\n\nThe degree of measurement accuracy and the type of data are both important in relation to carrying out a proper statistical analysis.\n\n\n \nCategorical Data\nA. Nominal Data\nNominal categorical data are data that one can name and put into categories. They are not measured but simply counted. They often consist of unordered ‘either–or’ type observations which have two categories and are often know as binary. For example: dead or alive; cured or not cured; pregnant or not pregnant. However, nominal categorical data often can have more than two categories, for example, blood group A, B, AB, O; country of origin; ethnic group; eye color. In Farndon et al. (2013) study gender is a binary variable (male or female), and the center where the the participants were treated is a nominal variable with seven levels (Central, Manor, Jordanthorpe, Limbrick, Firth Park, Huddersfield, Darnall).\n\n\n\n\n\n\nNumerical representation of categories are just codes\n\n\n\nWe can denote a male and female as 1 and 2 for gender and denote A, B, AB and O, as 1, 2, 3, and 4 for blood type. Unlike numerical data, the numbers representing different categories do not have mathematical meaning (they are just codes).\n\n\n \nΒ. Ordinal Data\nIf there are more than two categories of classification it may be possible to order them in some way. For example, after treatment a patient may be either improved, the same or worse. Another example of an ordinal variable is the variable pain where a subject is asked to describe their pain verbally as minimal, moderate, severe, or unbearable. In Farndon et al. (2013) study, the smoking history variable has three categories coded as non-smoker = 0, previous smoker = 1, and current smoker = 2. We know that someone who is a current smoker has more recent exposure to tobacco than someone who is an ex-smoker and someone who has never smoked. However, without further knowledge (of the current and past levels of tobacco consumption) it would be wrong to treat the codes of the categories as numerical quantities, as one cannot say that someone who is current smoker has twice the levels of tobacco consumption as someone who is a previous smoker.\n\n\n\n\n\n\nCollapsion of categories leads to a loss of information\n\n\n\nOrdinal data are often reduced to two categories to simplify analysis and presentation, which may result in a considerable loss of information.\n\n\n \nNumerical Data\nA. Discrete (or count) Data\nFarndon et al. (2013) study recorded the number of corns each participant had at the start of the trial, since this can only be a whole number or integer value, for example, 0, 1, 2, or 3 in this trial, this is termed discrete or count data. Other examples are often counts per unit of time such as the number of deaths in a hospital per year, the number of visits to the GP in a year, or the number of attacks of asthma a person has per month. In dentistry, a common measure is the number of decayed, filled or missing teeth (DFM).\nThe difference between discrete data and the ordinal data described earlier can be seen by considering an example of each:\n\n\n\n\n\n\nExample: Ordinal Vs Discrete data\n\n\n\nOrdinal categorical: Stage of breast cancer: I II III IV\nDiscrete numerical: Number of children: 0 1 2 3 4 5+\nWe cannot say that stage IV is twice as bad as stage II nor that the difference between stages I and II is equivalent to that between stages III and IV. In contrast, three children are three times as many as one, and a difference of one means the same throughout the range of values.\nIn practice discrete data are often treated in statistical analyses as if they were ordered categories. This is not wrong, but it may not be getting the most out of the data. Conversely, when ordered categories are numbered, as with stage of disease, the temptation to treat these numbers as statistically meaningful must be resisted. For example, it is not sensible to calculate the average stage of cancer. The only information the numbers contain is in the ordering, which would be conveyed equally by calling them A, B, C, D and so on.\n\n\n \nΒ. Continuous (or measured) Data\nSuch data are measurements with units that can, in theory at least, take any value within a given range (they are restricted by the accuracy of the measuring instrument). These data contain the most information, and are the ones most commonly used in statistics. Examples of continuous data in Farndon et al. (2013) study are: age, corn size, and EQ-5D health-related quality of life questionnaire.\nSometimes it is reasonable to treat discrete data as if they were continuous, at least as far as statistical analysis goes. While age is a continuous measurement, age at last birthday is discrete. In studies of adults with ages ranging from, say, 16 to 80, no harm is done in considering age in years as a continuous measurement (and this is standard practice), but for studies of pre-school children it would be better to use age in months. Heart rate (in beats per minute) is another discrete measurement that is usually regarded as continuous. Although the essential requirement for this change of status is that there should be a large number of different possible values, in practice we do not worry too much about analysing discrete measurements as if they were continuous.\n\n\n\n\n\n\nCategorization of numerical data leads to a loss of information\n\n\n\nFor simplicity, it is often the case in medicine that continuous data are dichotomized to make nominal data. For example, the diastolic blood pressure (DBP), which is continuous, is converted into hypertension (>90 mmHg) and normotension (≤90 mmHg). There are two main reasons for doing this:\n\nIt is easier to describe a population by the proportion of people affected, for example, the proportion of people in the population with hypertension is 10%.\nIt helps to make a decision: if a person has hypertension, then they will get treatment, and this is easier if high blood pressure has been categorized.\n\nSome statisticians have termed the habit of categorizing continuous variables as “dichotomania”, which they regard as poor research practice since it leads to a loss of information and assumes a discontinuous relationship that is unlikely in nature.\nOne can also divide a continuous variable into more than two groups. For example, DBP may be categorized as low, normal, or high; age can be divided into age bands of equal lengths of, say 10 years such as: 0-9, 10-19, 20-29, etc.\nWhen categorizing continuous data authors should give an indication as to why they chose these cut-off points, and a reader has to be very wary to guard against the fact that the cuts may be chosen to make a particular point.\n\n\n \n\n\n\n\n\n\nRecord the actual values\n\n\n\nIt is best to record the actual value of blood pressure, haemoglobin, etc. It is easy to convert to categories in the analysis, but the raw data cannot be retrieved later if only categories are recorded. Information is lost with no compensatory gain. Indeed, the statistical analysis of continuous data is more powerful, and often simpler.\nWhen some calculation is necessary to derive the observation of interest this should be done by the computer. Thus it is much better to record date of birth and date of examination for subsequent calculation of age rather than to rely on mental arithmetic."
  },
  {
    "objectID": "introduction.html#exercises",
    "href": "introduction.html#exercises",
    "title": "1  Introduction",
    "section": "\n Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 1.1\n\n\n\nWhich are the main branches of statistics?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe two branches of statistics are descriptive statistics and inferential statistics.\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.2\n\n\n\nA. What is the difference between dependent and independent variables?\nB. Try to identify the independent and dependent variables in the following example study:\n“Examining the difference between paracetamol and aspirin in the relief of pain experienced by migraine sufferers.”\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nA. A dependent variable is the variable being tested and measured in a scientific experiment (outcome). An independent variable is the variable that is changed or controlled in a scientific experiment to test the effects on the dependent variable.\nB. Independent variable is the type of therapy (paracetamol or aspirin) and dependent variable is measured experience of pain relief.\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.3\n\n\n\nWhich of the following is a qualitative nominal variable:\n\nValue of blood pH\nSensitivity of a diagnostic test (poor, moderate, high)\nHospitalization expenses in a hospital\nAssignment of a person in one of three groups in a clinical trial\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nd. Assignment of a person in one of three groups in a clinical trial\n\n\n\n\n\n\n\n\n\n\n\nExercise 1.4\n\n\n\nWhich of the following is a quantitative continuous variable:\n\nNumber of doctors in a hospital\nAn individual’s health status (healthy/diseased)\nBlood cholesterol levels (mg/DL)\nResult of a biopsy (positive/negative)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nc. Blood cholesterol levels (mg/DL)\n\n\n\n\n\n\n\n\n\nCao, Shiyi, Chen Yang, Yong Gan, and Zuxun Lu. 2015. “The Health Effects of Passive Smoking: An Overview of Systematic Reviews Based on Observational Epidemiological Evidence.” Edited by Yan Li. PLOS ONE 10 (10): e0139907. https://doi.org/10.1371/journal.pone.0139907.\n\n\nDoll, Richard, Richard Peto, Jillian Boreham, and Isabelle Sutherland. 2004. “Mortality in Relation to Smoking: 50 Years’ Observations on Male British Doctors.” BMJ 328 (7455): 1519. https://doi.org/10.1136/bmj.38142.554479.ae.\n\n\nEnstrom, J. E. 2003. “Environmental Tobacco Smoke and Tobacco Related Mortality in a Prospective Study of Californians, 1960-98.” BMJ 326 (7398): 1057–50. https://doi.org/10.1136/bmj.326.7398.1057.\n\n\nFarndon, Lisa J, Wesley Vernon, Stephen J Walters, Simon Dixon, Mike Bradburn, Michael Concannon, and Julia Potter. 2013. “The Effectiveness of Salicylic Acid Plasters Compared with ‘Usual’ Scalpel Debridement of Corns: A Randomised Controlled Trial.” Journal of Foot and Ankle Research 6 (1): 40. https://doi.org/10.1186/1757-1146-6-40.\n\n\nHansen, Tine W., Lutgarde Thijs, Yan Li, Jose'Boggia, Masahiro Kikuya, Kristina Bjor̈klund-Bodega, Tom Richart, et al. 2010. “Prognostic Value of Reading-to-Reading Blood Pressure Variability Over 24 Hours in 8938 Subjects From 11 Populations.” Hypertension 55 (4): 1049–57. https://doi.org/10.1161/hypertensionaha.109.140798.\n\n\nO’Sullivan, J J, G Derrick, P Griggs, R Foxall, M Aitkin, and C Wren. 1999. “Ambulatory Blood Pressure in Schoolchildren.” Archives of Disease in Childhood 80 (6): 529–32. https://doi.org/10.1136/adc.80.6.529.\n\n\nRigby, Alan S, Gillian K Armstrong, Michael J Campbell, and Nick Summerton. 2004. “A Survey of Statistics in Three UK General Practice Journal.” BMC Medical Research Methodology 4 (1). https://doi.org/10.1186/1471-2288-4-28.\n\n\nStrasak, Alexander M, Qamruz Zaman, Gerhard Marinell, Karl P Pfeiffer, and Hanno Ulmer. 2007. “The Use of Statistics in Medical Research.” The American Statistician 61 (1): 47–55. https://doi.org/10.1198/000313007x170242."
  },
  {
    "objectID": "descriptive.html",
    "href": "descriptive.html",
    "title": "2  Descriptive statistics",
    "section": "",
    "text": "Descriptive statistics are used to describe and organize the basic characteristics of the data in a study. The classical descriptive statistics allow us to have a quick glance of the central tendency and the extent of dispersion of values. They are useful in understanding a data distribution and in comparing data distributions and are usually presented in tables and graphs.\nFor example, Table 2.1 presents a typical summary table of the basic characteristics (variables) of patients entered into Farndon et al. (2013) randomized controlled trial (RCT). This study investigated the effectiveness of salicylic acid plasters compared with usual scalpel debridement for treatment of foot corns.\nIn this example since we have 101 patients in each randomized group the percentages are almost the same as the raw counts. However, for most studies we are unlikely to have exactly 100 participants in each group!"
  },
  {
    "objectID": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "href": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "title": "2  Descriptive statistics",
    "section": "\n2.1 Summarizing Categorical Data (Frequency Statistics)",
    "text": "2.1 Summarizing Categorical Data (Frequency Statistics)\nBinary data are the simplest type of data. Each individual has a label which takes one of two values such as male or female, corn healed or not healed. A simple summary would be to count the different types of labels and find the frequencies. The set of frequencies of all the possible categories is called the frequency distribution of the variable.\nHowever, a raw count is rarely useful. For example, in Table 2.1 there are more non-smokers in the scalpel group (40 out of 99 or 40%) compared to corn plaster group (34 out of 98 or 35%). It is only when this count is expressed as a proportion (relative frequency) that it becomes useful. Hence the first step to analyzing categorical data is to count the number of observations in each category (frequencies) and express them as proportions of the total sample size (relative frequencies).\n\n\n\n\n\n\nExample - Distribution of treatment center for 202 patients with foot corns (Farndon et al. 2013)\n\n\n\nOne of the categorical variables recorded in Farndon et al. (2013) study was the treatment center. Trial participants were treated at one of seven centers and the corresponding categories as displayed in Table 2.2. The first column shows category (treatment center) names, whilst the second shows the number of individuals in each category together with its percentage contribution to the total. Table 2.2 clearly shows that the majority (54.5%) of patients were treated at the “Central” treatment center.\n\n\nTable 2.2: Frequency and percentage distributions of treatment center for 202 patients with foot corns\n\nTreatment center\nFrequency\nPercentage\n\n\n\nCentral\n110\n54.5%\n\n\nManor\n33\n16.3%\n\n\nJordanthorpe\n24\n11.9%\n\n\nLimbrick\n9\n4.4%\n\n\nFirth Park\n11\n5.4%\n\n\nHuddersfield\n9\n4.5%\n\n\nDarnall\n6\n3.0%\n\n\nTotal\n202\n100.0%\n\n\n\n\nOf note, the percentages add up to 100%.\n\n\n \nIn addition to tabulating each variable separately, we might be interested in whether the distribution of patients across each center is the same for each randomized group.\n\n\n\n\n\n\nExample - Distribution of treatment center by randomized group for 202 patients with foot corns (Farndon et al. 2013)\n\n\n\nTable 2.3 shows the distribution of the number of patients treated at center by randomized group; in this case it can be said that the treatment center has been cross-tabulated with randomized group. Table 2.3 is an example of a contingency table with seven rows (representing treatment center) and two columns (randomized group). Note that we are interested in the distribution of patients across the seven centers in each randomized group (to see whether or not we have similar numbers of patients randomized to each treatment within each center), and so the percentages add to 100 down each column, rather than across the rows.\n\n\nTable 2.3: Cross-tabulation distribution of treatment center by randomized group for 202 patients with foot corns\n\n\nCorn plaster, n(%)\nScalpel, n(%)\nAll, n(%)\n\n\n\nCentral\n58 (57)\n52 (51)\n110 (54.5)\n\n\nManor\n13 (13)\n20 (20)\n33 (16.3)\n\n\nJordanthorpe\n10 (10)\n14 (14)\n24 (11.9)\n\n\nLimbrick\n3 (3)\n6 (6)\n9 (4.4)\n\n\nFirth Park\n7 (7)\n4 (4)\n11 (5.4)\n\n\nHuddersfield\n5 (5)\n4 (4)\n9 (4.5)\n\n\nDarnall\n5 (5)\n1 (1)\n6 (3.0)\n\n\nTotal\n101 (100)\n101 (100)\n202 (100)\n\n\n\n\n\n\n \nHow to report descriptive statistics for categorical data?\nDisplay the number and proportion of cases that fall into each category. The following format is recommended for reporting descriptive statistics for categorical data:\n\n\n\n\n\n\nRecommendations for reporting numbers and percentages\n\n\n\n\n\nTable 2.4: Reporting numbers and percentages (examples relevant to the data provided in Farndon et al. 2013 study)\n\n\n\n\n\n\nRecommendation\nCorrect expression\n\n\n\nNumbers\n\n\n\nIn a sentence, numbers less than 10 are words.\nSmoking history was missing from three patients in the corn plaster study group.\n\n\n\n\n\n\nIn a sentence, numbers 10 or more are numbers.\nThere are 34 non-smokers patients in the corn plaster group.\n\n\n\n\n\n\nUse words to express any number that begins a sentence, title or heading.\n\nThirty-four non-smokers patients recorded in the cord plaster group.\n\n\n\n\n\n\nPercentages\n\n\n\nReport percentages to only one decimal place if the sample size is larger than 100.\nIn the sample of 202 patients, 4.5% were treated at the “Limbrick” treatment center.\n\n\n\n\n\n\nReport percentages with no decimal places if the sample size is less than 100.\nIn the sample of 98 patients in the corn plaster group, 35% were non-smokers.\n\n\n\n\n\n\nDo not use percentages if the sample size is less than 20.\nFrom 16 previous smokers in the scalpel group, 7 were females."
  },
  {
    "objectID": "descriptive.html#displaying-categorical-data",
    "href": "descriptive.html#displaying-categorical-data",
    "title": "2  Descriptive statistics",
    "section": "\n2.2 Displaying Categorical Data",
    "text": "2.2 Displaying Categorical Data\nWhile frequency tables are extremely useful, the best way to investigate a dataset is to plot it. For categorical variables, such as gender and treatment center, it is straightforward to present the number in each category, usually indicating the frequency and percentage of the total number of patients. When shown graphically this is called a bar plot.\n \nA. Simple Bar Plot\nA simple bar plot is an easy way to make comparisons across categories. Figure 2.1 shows the centers where 202 patients with foot corns were treated in the trial of Farndon et al. (2013). Along the horizontal axis (x-axis) are the different treatment centers whilst on the vertical axis (y-axis) is the percentage. The height of each bar represents the percentage of the total patients in that category. For example, it can be seen that the percentage of participants who were treated in the “Central” center was about 55%.\n\n\n\n\nFigure 2.1: Bar plot showing where 202 patients with corns were treated (Farndon et al. 2013).\n\n\n\n\n\n\n\n\n\n\nBasic Properties of Simple Bar plot\n\n\n\n\nAll bars should have equal width and should have equal space between them.\nThe height of the bar is equivalent to the data they represent.\nThe bars must be plotted against a common zero-valued baseline.\n\n\n\n \nB. Side-by-side and Grouped Bar Plots\nIf the sample is further classified into whether the patient was treated with corn plasters or scalpel then it becomes impossible to present the data as a single bar plot. We could present the data as a side by side bar plot (see Figure 2.2) but is preferable to present the data in one graph with the same scales and axes to make the visual comparisons easier (grouped bar plot) (see Figure 2.3).\n\n\n\n\nFigure 2.2: Side-by-side bar plot showing where 202 patients with corns were treated by randomized group (Farndon et al. 2013).\n\n\n\n\n\n\n\n\nFigure 2.3: Grouped bar plot showing where 202 patients with corns were treated by randomized group (Farndon et al. 2013).\n\n\n\n\n\n\n\n\n\n\nReport the actual total sample sizes for each group\n\n\n\nIf we do use the relative frequency scale as we have, then it is recommended to report the actual total sample sizes for each group (e.g., in the legend or caption). In this way, given the total sample size and relative frequency (from the height of the bars) we can work out the actual numbers treated in each center.\n\n\n \nC. Stacked Bar Plot\nUnlike a side-by-side or grouped graphs, Stacked Bar Plots segment their bars. A 100% Stack Bar Plot shows the percentage-of-the-whole of each group and are plotted by the percentage of each value to the total amount in each group. This makes it easier to see if relative differences exist between quantities in each group (see Figure 2.4).\n\n\n\n\nFigure 2.4: A horizontal 100% stacked bar plot showing the distribution of gender by randomized group (Farndon et al. 2013).\n\n\n\n\nIn Figure 2.4 the bars are divided into two segments only (i.e., female and male) so it is easy to read the values of each segment and to compare a specific segment through the entire set of bars (in our case the percentages are equal). This comparison can be easily made because each segment is aligned through the entire set of bars (female to the left and men to the right). If more segments were added, however, the segments in the middle would not be aligned to the left or right, which would make comparisons difficult (see Figure 2.5).\n\n\n\n\nFigure 2.5: A horizontal 100% stacked bar plot showing the distribution of treatment centers by randomized group (Farndon et al. 2013).\n\n\n\n\n\n\n\n\n\n\nStacked bar plots tend to become confusing when the variable has many levels\n\n\n\nOne issue to consider when using stacked bar plots is the number of variable levels: when dealing with many categories, stacked bar plots tend to become rather confusing."
  },
  {
    "objectID": "descriptive.html#summarizing-numerical-data",
    "href": "descriptive.html#summarizing-numerical-data",
    "title": "2  Descriptive statistics",
    "section": "\n2.3 Summarizing Numerical Data",
    "text": "2.3 Summarizing Numerical Data\nA quantitative measurement contains more information than a categorical one, and so summarizing these data is more complex. One chooses summary statistics to condense a large amount of information into a few intelligible numbers, the sort that could be communicated verbally. The two most important pieces of information about a quantitative measurement are ‘where is it?’ and ‘how variable is it?’ These are categorized as measures of location (or sometimes ‘central tendency’) and measures of spread or variability.\n\n\n\n\n\n\nTwo summary measures should be reported for a numerical variable\n\n\n\nA measure of location (where the center of the distribution of the values is located) and variability (how widely the values are spread above and below the central value) provides an informative but brief summary of a set of observations.\n\n\n \nMeasures of Location\nA. Sample Mean or Average\nLet \\(x_1, x_2,...,x_{n-1}, x_n\\) be a set of n measurements. The arithmetic sample mean or average, \\(\\bar{x}\\) (pronounced x bar), is simply the sum of the observations divided by their number n, thus:\n\\[\n\\bar{x}= \\frac{Sum \\ of \\ all \\ sample \\ values }{Size \\ of \\ sample}= \\frac{x_1 + x_2 + ... + x_{n-1} + x_n}{n}\n\\]\nThis formula is entirely correct, but it’s too long, so we make use of the summation symbol \\(\\scriptstyle\\sum\\) to shorten it:\n\\[\\bar{x}=\\frac{\\sum_{i=1}^{n}x_{i}}{n}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i} \\tag{2.1}\\]\nIn the above Equation 2.1, \\(x_{i}\\) represents the individual sample values and \\({\\sum_{i=1}^{n}x_{i}}\\) their sum. The Greek letter \\({\\Sigma}\\) (sigma) is the Greek capital ‘S’ and stands for ‘sum’ and simply means ‘add up the n observations \\(x_{i}\\) from the 1st to the last (nth)’.\nUsually, we cannot measure the population mean \\({\\mu}\\), which is the unknown constant that we want to estimate using the sample mean \\(\\bar{x}\\).\n \n\n\n\n\n\n\nExample: Calculation of the Mean - Corn size data (mm)\n\n\n\nIn the RCT by Farndon et al. (2013), the baseline size of the corn (as its widest diameter in mm) was measured by a podiatrist (foot specialist). Consider the following 16 baseline corn sizes selected from the patients:\nData: 2, 2, 6, 3, 4, 2, 2, 5, 3, 4, 1, 2, 6, 3, 10, 3\nThe sum of the 16 observations is:\n2 + 2 + 6 + 3 + 4 + 2 + 2 + 5 + 3 + 4 + 1 + 2 + 6 + 3 + 10 + 3 = 58\nThus, the arithmetic mean is:\n\\(\\bar{x}\\) = 58/16 = 3.625 mm or 3.6 mm. It is usual to report one more decimal place for the mean than the data recorded.\n\n\nThe major advantage of the mean is that it uses all the data values, while the main disadvantage is its sensitivity to very large or very small values, which might be outliers (unusual values). For example, if we entered “100 mm” instead of “10 mm”, for the 15th patient, in the calculation of the mean, we would find the mean changed from 3.6 to 9.2. It does not necessarily follow, however, that outliers should be excluded from the final data summary, or that they result from a human error. Outliers can be legitimate anomalies that are vital for capturing information on the subject of interest.\nIf the data are binary and are coded 0 or 1, then \\(\\bar{x}\\) is the proportion of individuals with value 1, and this can also be expressed as a percentage. In Farndon et al. (2013) data, the cases in which the corn was healed are coded as ‘1s’ and the cases in which the corn was not healed as ‘0s’. The corn had healed in 52 out of 189 patients (0.28 or 28%), which is equal to the “mean” of this variable 0.28.\n\n\n\n\n\n\nAdvantages and Disadvantages of arithmetic mean\n\n\n\nAdvantages of mean\n\nis simple to understand and easy to calculate\nuses all the data values in the calculation\nis algebraically defined and thus mathematically manageable\nhas a known sampling distribution (see Chapter 4)\n\nDisadvantages of mean\n\nis highly affected by the presence of a few abnormally high or abnormally low values (outliers)\nis not an appropriate average for highly skewed (asymmetrical) distributions\ncannot be determined if any item of observation is missing\ncannot be determined easily by inspection of the data\n\n\n\n \nB. Median of the sample\nThe sample median, md, is an alternative measure of location, which is less sensitive to outliers. For observed values \\(x_1, x_2,...,x_{n-1}, x_n\\) the median is calculated by first sorting the observed values (i.e., arranging them in an ascending/descending order) and selecting the middle one. If the sample size n is odd, the median is the number at the middle of the ordered observations. If the sample size is even, the median is the average of the two middle numbers.\nTherefore, the sample median, md, of n observations is:\n\nthe \\(\\frac{n+1}{2}\\)th ordered value, \\(md=x_{\\frac{n+1}{2}}\\), if n is odd.\nthe average of the \\(\\frac{n}{2}\\)th and \\(\\frac{n+1}{2}\\)th ordered values, \\(md=\\frac{1}{2}(x_{\\frac{n}{2}}+x_{\\frac{n+1}{2}})\\), if n is even.\n\n\n\n\n\n\n\nExample: Calculation of the Median - Corn size data (mm)\n\n\n\n\n1st case: Even observations\n\nWe have selected 16 baseline corn sizes:\noriginal data: 2, 2, 6, 3, 4, 2, 2, 5, 3, 4, 1, 2, 6, 3, 10, 3\nWe arrange the data in an ascending order:\nordered data: 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6, 6, 10\nAs the number of observations are even (n=16), the median is the average of the two middle ordered numbers (the eighth and ninth): (3+3)/2=3 mm.\n \n\n2nd case: Odd observations\n\nSuppose we select an additional 17th subject with corn size of 10 mm, so the data are as following:\noriginal data: 2, 2, 6, 3, 4, 2, 2, 5, 3, 4, 1, 2, 6, 3, 10, 3, 10\nWe arrange the data in an ascending order:\nordered data: 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 5, 6, 6, 10, 10\nThe median would be the 9th ordered observation, which is also 3 mm.\n\n\n \n\n\n\n\n\n\nAdvantages and Disadvantages of median\n\n\n\nAdvantages\n\nThe median has the advantage that is not affected by outliers, so for example, the median in the data would be unaffected by replacing the largest corn size of 10 mm with 100 mm.\n\nDisadvantages\n\nHowever, it does not take into account the precise value of each observation and hence does not use all information available in the data.\nAdditionally, it is not a good measure of central tendency when there are heavy ties in the data.\n\n\n\n \nC. Mode of the sample\nA third measure of location is termed the mode. This is the value that occurs most frequently, or, if the data are grouped, the group with the highest frequency. It is not used much in statistical analysis, since its value depends on the accuracy with which the data are measured and ignores most of the information; although it may be useful for categorical data to describe the most frequent category. Note that some datasets do not have a mode because each value occurs only once.\nHowever, the expression ‘bimodal’ distribution is used to describe a distribution with two peaks in it. This can be caused by mixing two or more populations together. For example, height might appear to have a bimodal distribution if one had men and women in the population.\n\n\n\n\n\n\nExample: Calculation of the Mode - Corn size data (mm)\n\n\n\noriginal data: 2, 2, 6, 3, 4, 2, 2, 5, 3, 4, 1, 2, 6, 3, 10, 3\nIn the 16 patients with corns, 5 patients have corn size of 2 mm; thus, the modal corn size is 2 mm.\n\n\n \nMeasures of Dispersion\nWe also need a numerical way of summarizing the amount of spread or variability in a dataset. The tree main approaches to quantifying variability are: the range, the interquartile range (IQR), and the standard deviation.\nA. Range of the sample\nThe simplest way to describe the spread of a dataset is to report the minimum (lowest) and maximum (highest) values. The range is defined as the difference between the largest and the smallest observations in a sample. For some data it is very useful, because one would want to know these numbers, for example in a sample the age of the youngest and oldest participant. However, if outliers are present it may give a distorted impression of the variability of the data, since only two of the data points are included in making the estimate. Thus the range is affected by extreme values at each end of the data.\n\n\n\n\n\n\nExample: Calculation of the Range - Corn size data (mm)\n\n\n\nThe range for the corn size data is 1 to 10 mm or described by a single number 10-1 = 9 mm.\n\n\n \nB. Quartiles and Interquartile Range of the sample\nThe quartiles, namely the lower quartile (\\(Q_{1}\\)), the median (\\(Q_{2}\\)) and the upper quartile (\\(Q_{3}\\)), split sorted data into four equal parts. That is there will be approximately equal numbers of observations in the four sections (and exactly equal if the sample size is divisible by four and the measures are all distinct). The quartiles are calculated in a similar way to the median; first order the data and then count the appropriate number from the bottom. The \\(Q_{1}\\) is the value below which 25% of the observations may be found, while the \\(Q_{3}\\) is the value above which the top 25% of the observations may be found (meaning that 75% of the data falls below the \\(Q_{3}\\)).\nThe interquartile range (IQR) is a useful measure of variability and is given by the difference of the lower and upper quartiles (IQR=\\(Q_{3}\\)-\\(Q_{1}\\)). It indicates the spread of the middle 50% (75%-25%) of the data.The IQR is an especially good measure of variability for skewed distributions or distributions with outliers.\nThe median and the quartiles are examples of percentiles - points which split the distribution of data into percentages above or below a certain value. The median is the 50th percentile, the \\(Q_{1}\\) is the 25th percentile, and the the \\(Q_{3}\\) is the 75th percentile.\n\n\n\n\n\n\nExample: Calculation of the Quartiles and Interquartile Range - Corn size data (mm)\n\n\n\nThe \\(Q_{1}\\) lies somewhere between the \\(\\color{red}{fourth}\\) and \\(\\color{blue}{fifth}\\) ordered observations (2+2)/2= 2mm. The median, \\(Q_{2}\\), is the average of the \\(\\color{blue}{eighth}\\) and \\(\\color{green}{ninth}\\) ordered observations (3+3)/2=3 mm. Similarly, The \\(Q_{3}\\) lies somewhere between the \\(\\color{green}{12th}\\) and \\(\\color{orange}{13th}\\) ordered observations (4+5)/2= 4.5 mm.\nSo, the IQR for the corn size data is 2.0 to 4.5 mm or described by a single number 2.5 mm.\n\n\nTable 2.5: Calculating quartiles for the corn size data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrdered data\n\\(\\color{red}{1}\\)\n\\(\\color{red}{2}\\)\n\\(\\color{red}{2}\\)\n\\(\\color{red}{\\textbf{2}}\\)\n\\(\\color{blue}{\\textbf{2}}\\)\n\\(\\color{blue}{2}\\)\n\\(\\color{blue}{3}\\)\n\\(\\color{blue}{\\textbf{3}}\\)\n\\(\\color{green}{\\textbf{3}}\\)\n\\(\\color{green}{3}\\)\n\\(\\color{green}{4}\\)\n\\(\\color{green}{\\textbf{4}}\\)\n\\(\\color{orange}{\\textbf{5}}\\)\n\\(\\color{orange}{6}\\)\n\\(\\color{orange}{6}\\)\n\\(\\color{orange}{10}\\)\n\n\n\n\n\n \nC. Sample Variance and Standard Deviation\nFor an individual with an observed value \\(x_{i}\\) the distance from the mean is \\(x_{i}-\\bar{x}\\). With n such observations we have a set of n differences, one for each individual. The sum of the differences, \\({\\sum_{i=1}^{n}(x_{i}-\\bar{x})}\\) is always zero. However, if we square the distances before we sum them, we get always a positive quantity.This sum is then divided by n-1 and thus gives an average measure for the square of the deviation from the sample mean. This quantity is called the sample variance and is defined as Equation 2.2:\n\\[variance = s^2 = \\frac{\\sum\\limits_{i=1}^n (x -\\bar{x})^2}{n-1} \\tag{2.2}\\]\nThe variance is expressed in square units, so we can take the square root to return to the original units. This gives us the standard deviation (usually abbreviated as sd) defined as Equation 2.3:\n\\[sd=s = \\sqrt\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^2}{n-1} \\tag{2.3}\\]\nExamining this expression it can be seen that if all the x’s were the same, then they would equal x and so sd would be zero. If the x’s were widely scattered about x, then sd would be large. In this way sd reflects the variability in the data. Both, variance and standard deviation, are sensitive to outliers and thus they are inappropriate for skewed data.\n\n\n\n\n\n\nExample: Calculation of the Variance and Standard Deviation - Corn size data (mm)\n\n\n\nConsider the 16 observations from the corn data. The calculations to work out the standard deviation are given in the Table 2.6.\n\n\nTable 2.6: Calculating variance and standard deviation for the corn size data\n\n\n\n\n\n\n\n\nid\nCorn size (mm)\nMean\nDifference from mean \\((x_{i}-\\bar{x})\\)\n\nSquare of difference from mean \\((x_{i}-\\bar{x})^2\\)\n\n\n\n\n1\n2\n3.625\n-1.625\n2.641\n\n\n2\n2\n3.625\n-1.625\n2.641\n\n\n3\n6\n3.625\n2.375\n5.641\n\n\n4\n3\n3.625\n-0.625\n0.391\n\n\n5\n4\n3.625\n0.375\n0.141\n\n\n6\n2\n3.625\n-1.625\n2.641\n\n\n7\n2\n3.625\n-1.625\n2.641\n\n\n8\n5\n3.625\n1.375\n1.891\n\n\n9\n3\n3.625\n-0.625\n0.391\n\n\n10\n4\n3.625\n0.375\n0.141\n\n\n11\n1\n3.625\n-2.625\n6.891\n\n\n12\n2\n3.625\n-1.625\n2.641\n\n\n13\n6\n3.625\n2.375\n5.641\n\n\n14\n3\n3.625\n-0.625\n0.391\n\n\n15\n10\n3.625\n6.375\n40.641\n\n\n16\n3\n3.625\n-0.625\n0.391\n\n\nSum\n58\n3.625\n0.000\n75.756\n\n\n\n\nFrom the Equation 2.3 we have:\n\\[sd = \\sqrt\\frac{\\sum_{i=1}^{16}(x_{i}-\\bar{x})^2}{16-1} = \\sqrt\\frac{75.756}{15}= \\sqrt{5.05}=2.247 \\ or \\ 2.3 \\ mm\\]\nNote that the majority of this sum is contributed by one observation, the value of 10 mm from participant 15, which is the observation further from the mean. This shows that much of the value of an sd is derived from the outlying observation.\n\n\n \nWhy is the standard deviation useful?\nIt turns out in many situations that about 95% of observations will be within two standard deviations of the mean. This is known as a reference range or interval and it is this characteristic of the standard deviation which makes it so useful. It holds for a large number of measurements commonly made in medicine. In particular it holds for data that follow a Normal distribution (see also Chapter 3). For example, if the age of participants in the corn plaster group is normally distributed, we would expect the majority of participants in this treatment group to have age between 58.5 - 2 \\(\\times\\) 15.6 and 58.5 + 2 \\(\\times\\) 15.6 or 27.3 and 89.7 years.\n \nHow to report descriptive (or summary) statistics for numerical data?\nSample mean and median convey different impressions of the location of data in presence of skewness (or outliers).\n\nIf the distribution is symmetric (mean=median=mode) (Figure 2.6 b), then in general the mean is the better summary statistic (see also Chapter 3).\nIf the distribution is skewed to the left (Figure 2.6 a) or right (Figure 2.6 c) then the median is less influenced by the tails (see also Chapter 3).\n\n\n\n\n\n(a) Left skewed distribution.\n\n\n\n\n(b) Symmetric distribution.\n\n\n\n\n(c) Right skewed distribution\n\n\n\nFigure 2.6: Types of distribution according to the symmetry.\n\n\nThus, the following format is recommended for reporting summary statistics for numerical data:\nA. Mean (sd) for data with symmetric distribution. A distribution, or dataset, is symmetric if its left and right sides are mirror images.\nB. Median (Q1, Q3) for those with skewed (or asymmetrical) distribution.  \n\n\n\n\n\n\nRecommendations for reporting summary statistics for numerical data\n\n\n\nTable 2.7 presents recommendations for reporting summary statistics for numerical data.\n\n\nTable 2.7: Recommendations for reporting summary statistics (examples relevant to the data provided in Farndon et al. 2013 study)\n\n\n\n\n\n\nRecommendation\nCorrect expression\n\n\n\nDo not imply greater precision than the measurement instrument\nOnly use one decimal place more than the basic unit of measurement when reporting statistics (means, medians, standard deviations, inter-quartile ranges, etc.), for example, the mean age of participants in the corn plaster group was 58.5 years.\n\n\nFor ranges use ‘to’ or a comma but not ‘-’ to avoid confusion with a minus sign. Also use the same number of decimal places as the summary statistic\n\nThe mean (sd) age of participants in the corn plaster group was 58.5 years (15.6).\nThe median (IQR) EQ-5D of participants in the corn plaster group was 0.73 (0.59, 0.80)"
  },
  {
    "objectID": "descriptive.html#displaying-numerical-data",
    "href": "descriptive.html#displaying-numerical-data",
    "title": "2  Descriptive statistics",
    "section": "\n2.4 Displaying Numerical Data",
    "text": "2.4 Displaying Numerical Data\nThe best way for examining the distribution of numerical data is to generate an appropriate graph.\nA. Histogram and density plot\nThe most common way of depicting a frequency distribution of a continuous variable is with a histogram.\nA histogram (Figure 2.7 a) is a plot that depicts the distribution of a numeric variable’s values as a series of bars without space between them. Each bar typically covers a range of numeric values called a bin or class; a bar’s height indicates the frequency of observations with a value within the corresponding bin. A density plot (Figure 2.7 b) is a smoothed, continuous version of a histogram estimated from the data. In a density plot the total area under the curve integrates to one.\nFigure 2.7 shows the distribution of age for the participants in Farndon et al. (2013) study. The vertical scale shows (a) frequency (histogram) or (b) probability density (density plot).\n\n\n\n\nFigure 2.7: Distribution of age of the 202 participants in Farndon et al. (2013) study (a) histogram (b) density plot.\n\n\n\n\nA histogram (or density plot) gives information about:\n\nHow the data are distributed: (a) left-skewed, (b) symmetric (e.g., normal distribution), (c) right-skewed and if there are any outliers.\nThe amount of variability in the data.\nWhere the peaks of the distribution are.\n\n \n\n\n\n\n\n\nChoose an appropriate number of bins\n\n\n\nWhile tools that can generate histograms usually have some default algorithms for selecting bin boundaries, we will likely want to play around with the binning parameters to choose something that is representative of our data.\nChoice of number of bins has an inverse relationship with the bin width Figure 2.8. The smaller the number of bins, the larger bin width there will be to cover the whole range of data.\nIt is worth taking some time to test out different number of bins to see how the distribution looks in each one, then choose the plot that represents the data best.\n\n\n\n\nFigure 2.8: Histogram of age for different number of bins (a) bins = 50, (b) bins = 15, and (c) bins = 8.\n\n\n\n\nIf we have too many bins, then the data distribution will look rough, and it will be difficult to discern the signal from the noise (Figure 2.8 a). On the other hand, with too few bins, the histogram will lack the details needed to discern any useful pattern from the data (Figure 2.8 b).\n\n\nWe can also create a histogram or density plot by group. Figure 2.9 depicts the probability density of age by treatment group.\n\n\n\n\nFigure 2.9: Histogram of age of participants by treatment group in Farndon et al. (2013) study.\n\n\n\n\n \n\n\n\n\n\n\nHistograms must be plotted with a zero-valued baseline\n\n\n\nAn important aspect of histograms is that they must be plotted with a zero-valued baseline. Since the frequency of data in each bin is implied by the height of each bar, changing the baseline or introducing a gap in the scale will skew the perception of the distribution of data.\n\n\n \nB. Box Plot\nA box plot chart is another graph that can be used for conveying location and variation information for continuous data, particularly for detecting changes between different groups of data before any formal analyses are performed.\nA box plot (aka box and whisker plot) uses boxes and lines to depict the distributions of one or more groups of numerical data. Box limits indicate the range of the central 50% of the data, with a central line marking the median value. Lines extend from each box to capture the range of the remaining data, with dots placed past the line edges to indicate outliers.\n\n\n\n\nFigure 2.10: Broad classification of the different types of data with examples\n\n\n\n\nIn Figure 2.10 the distance between \\({Q3}\\) and \\({Q1}\\) is the interquartile range (IQR) and plays a major part in how long the whiskers extending from the box are. Each whisker extends to the furthest data point in each wing that is within 1.5 times the IQR. Any data point further than that distance is considered an outlier, and is marked with a dot. There are other ways of defining the whisker lengths, which are not presented in this textbook.\nFigure 2.11 illustrates a box plot that presents the age of participants in Farndon et al. (2013) study.\n\n\n\n\nFigure 2.11: A box plot of age of the participants in Farndon et al. (2013) study.\n\n\n\n\nFigure 2.12 illustrates a grouped box plot that presents the age by treatment group.\n\n\n\n\nFigure 2.12: A box plot of age of the participants by treatment group in Farndon et al. (2013) study.\n\n\n\n\n \n\n\n\n\n\n\nIdentifying Outliers in the data based on quartiles\n\n\n\nAn outlier is a data value significantly far removed from the main body of a dataset. We say any value outside of the following interval is an outlier:\n\\[(Q_1 - 1.5 \\times IQR, \\ Q_3 + 1.5 \\times IQR)\\]\n\n\n \nC. Raincloud Plot\nThere are many variations of the boxplot. For example, there is a way to combine raw data (dots), probability density, and key summary statistics such as median, and relevant intervals of a range of likely values for the population parameter, in an appealing and flexible format with minimal redundancy, using the raincloud plot (Figure 2.13):\n\n\n\n\nFigure 2.13: A raincloud plot of age of the participants by treatment group in Farndon et al. (2013) study."
  },
  {
    "objectID": "descriptive.html#exercises",
    "href": "descriptive.html#exercises",
    "title": "2  Descriptive statistics",
    "section": "\n2.5 Exercises",
    "text": "2.5 Exercises\n\nFirst Exercise\nSecond Exercise\n\n\n\n\n\nFarndon, Lisa J, Wesley Vernon, Stephen J Walters, Simon Dixon, Mike Bradburn, Michael Concannon, and Julia Potter. 2013. “The Effectiveness of Salicylic Acid Plasters Compared with ‘Usual’ Scalpel Debridement of Corns: A Randomised Controlled Trial.” Journal of Foot and Ankle Research 6 (1): 40. https://doi.org/10.1186/1757-1146-6-40."
  },
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "3  Probability Distributions - Normal distribution",
    "section": "",
    "text": "In nature, people often encounter two types of phenomena:\n\nOne is the deterministic phenomenon, which is characterized by conditions under which the result is completely predictable, that is, the same result is observed each time the experiment is conducted. For example, water at 100°C under standard atmospheric pressure inevitably boils.\nThe other is the random phenomenon, which is characterized by conditions under which the result cannot be determined with certainty before it occurs, that is, one of several possible outcomes is observed each time the experiment is conducted. For example, when a coin is tossed, the outcome is either heads H or tails T, but unknown before the coin is tossed. Die rolling is also a random phenomenon, whose outcome is an integer from 1 to 6, unknown before the die is rolled. Likewise, for a bi-allelic gene A, the possible alleles are A and a, and the possible corresponding genotypes are AA, Aa, and aa.\n\nThe process of obtaining an observation or making a measurement for a random phenomenon/process is called a random experiment (briefly, an experiment), and is denoted by E.\nThe sample space Ω is defined as the set of all possible outcomes of the experiment. In the case of the roll of a die, the sample space can be written as the set of the six possible outcomes, Ω = {1, 2, 3, 4, 5, 6}.\nDifferent experiments will have different sample spaces that can be written in an equivalent way (flipping a coin: Ω ={H, T}, flipping two coins: Ω ={HH, HT, TH, TT}, testing for possible genotypes of a bi-allelic gene A: Ω ={AA, Aa, aa}).\nA random event A, or event A for short, is a sub-set of Ω, A ⊂ Ω, and it represents a number of possible outcomes for the experiment. In the case of the roll of a die, the event “even number” may be represented by A = {2, 4, 6}, and the event “odd number” as B = {1, 3, 5}. In the case of flipping two coins, an event could be that exactly one of the coins lands Heads, A = {HT, TH}.\n\n\n\n\n\n\nTwo events always exist for an experiment\n\n\n\nFor each experiment, two events always exist: the sample space itself which comprises all possible outcomes and the empty set that contains no outcomes represented as A = ∅ and called the impossible event.\n\n\n\n\n\n\n\n\nBasic types and operations of events using set theory\n\n\n\nSimple and Compound Events: If an event consists of a single outcome from the sample space, it is termed a simple event. The event of getting less than 2 on rolling a fair die, denoted as A = {1}, is an example of a simple event. If an event consists of more than a single outcome from the sample space, it is called a compound event. An example of a compound event in probability is rolling a fair die and getting an odd number, A = {1, 3, 5}.\nUnion of Events: The union symbol (∪) is used to denote the OR event. For any two events A and B, “at least one of A and B occurs” is also an event. This event is called the union of A and B, and is denoted by A∪ B, which includes only A occurring, only B occurring, and A and B occurring simultaneously.\nIn the experiment of rolling a single die, find the union of the events A : “the number rolled is even” and B : “the number rolled is greater than two.” Since the outcomes that are in either A={2,4,6} or B={3,4,5,6} or both are 2,3,4,5, and 6, that means A ∪ B={2,3,4,5,6} .\nIntersection of Events: The intersection symbol (∩) is used to denote the AND event. For any two events A and B, “A and B occur simultaneously” is also an event. This event is called the intersection of A and B, and is denoted by A ∩ B.\nFor example, A = {1, 2, 3, 4}, B = {2, 3, 5, 6} then A ∩ B = {2, 3}.\nComplement Events: For any event A, “event A does not occur” is also an event. This event is called the complement of A or the inverse of event A, and is denoted by \\(\\bar{A}\\).\nMutually Exclusive Events (or incompatible events or disjoint): For any two events A and B, if events A and B cannot occur simultaneously, that is, A ∩ B =∅, then A and B are called mutually exclusive events or incompatible events.\nVenn diagrams allow us to understand compound events:"
  },
  {
    "objectID": "distributions.html#probability",
    "href": "distributions.html#probability",
    "title": "3  Probability Distributions - Normal distribution",
    "section": "\n3.2 Probability",
    "text": "3.2 Probability\nThe concept of probability is used in day-to-day life which stands for the probability of occurring or non-occurring of events.\nThe first step towards determining the probability of an event is to establish a number of basic rules that capture the meaning of probability. The probability of an event is required to satisfy three axioms defined by Kolmogorov:\n\n\n\n\n\n\nThe Kolmogorov Axioms\n\n\n\n\nThe probability of an event A is a non-negative number, P(A) ≥ 0\n\nThe probability of all possible outcomes, or sample space, is P(Ω) = 1\n\nIf A and B are two mutually exclusive events or incompatible events, then P(A ∪ B) = P(A) + P(B) and P(A ∩ B) = 0.\n\n\n\n\n\n\n\n\n\nExample - Dice: Mutually exclusive events\n\n\n\nSuppose we throw a six-die what is the probability of rolling either 5 or 6?\nThe probability of rolling a 6 is 1/6 and the probability of a 5 is also 1/6. We cannot take a 5 and 6 at the same time (these events are mutually exclusive) so:\nP(rolling a 5 or 6) = P(rolling a 5) + P(rolling a 6) = 1/6 + 1/6 = 2/6 = 1/3\n\n\nThese axioms should be regarded as the basic “ground rules” of the theory of probability, but they provide no guidance on how event probabilities should be assigned. For this purpose, there are two major avenues available. One is based on the repetition of the experiments a large number of times under the same conditions, and goes under the name of the frequentist approach. The other is based on a more theoretical knowledge of the experiment, but without the experimental requirement of a large number of repetitions, and is referred to as the Bayesian approach.\n \nDefinition of Probability\nA. Frequentist approach\nConsider performing an experiment a large number N of times, under the same experimental conditions. The occurrence of the event A is indicated as the number N(A). The probability of event A is given by:\n\\[ P(A) = \\lim_{N\\to\\infty} \\frac{N(A)}{N} \\tag{3.1}\\]\nthat is, the probability is the relative frequency of occurrence of a given event from many repetitions of the same experiment.\nThe obvious limitation of this definition is the need to perform the experiment a large number of times. This requirement is not only time consuming but also requires that the experiment be repeatable in the first place, which may or may not be possible. The limitation of this method is evident by considering a coin toss: no matter the number of tosses, the occurrence of heads up will never be exactly 50%, which is what one would expect based on an empirical knowledge of the experiment at hand Figure 3.1.\n\n\n\n\nFigure 3.1: Coin Flips Simulation showing long-term probability close to 0.5.\n\n\n\n\nTherefore, we may say that the probability of an event is the relative frequency of this set of outcomes over an indefinitely large number of experiments.\n\\[ P(A) \\approx  \\frac{number\\ of\\ times \\ A\\ occured}{total\\ number\\ of\\  experiments} \\tag{3.2}\\]\n\n\n\n\n\n\nExamples: Probability based on on Frequentist approach\n\n\n\nPrevalense of blood type\nSampling many (say, 100,000) people in Greece, finding that roughly 45,000 of them had blood type O. \\[ P(O) \\approx  \\frac{number\\ of\\ people \\ with\\ blood\\ type\\ O}{total\\ number\\ of\\  sample}= \\frac{45,000}{100,000}=0.45\\] is the estimate for the probability for the event “having blood type O.”\nPrevalence of a disease\nSuppose the prevalence of diabetes in the population is 1%. The prevalence of a disease is the number of people in a population with the disease at a certain time divided by the number of people in the population. If a trial was then conducted by randomly selecting one person from the population and testing him or her for diabetes, the individual would be expected to be diabetic with probability 0.01. If this type of sampling of individuals from the population were repeated, then the proportion of diabetics in the total sample taken would be expected to be approximately 1%.\n\n\nB. Bayesian approach\nAnother method to assign probabilities is to use knowledge of the experiment, both theoretical and experimental, but without the need for extensive experimental data. The probability assigned to an event represents the degree of belief that the event will occur in a given try of the experiment, and it implies an element of subjectivity which will become more evident with Bayes’ theorem.\n\n\n\n\n\n\nExample - Coin toss experiment\n\n\n\nIn the coin toss experiment, the determination of the subjective probability for events “Heads up” or “Tails up” relies on the knowledge that the coin is unbiased, and therefore it must be true that P(T) = P(H). With this information, we can then simply use the Kolmogorov axioms to state that P(T) + P(H) = 1, and therefore obtain the intuitive result that P(T) = P(H) = 1/2.\n\n\nIn this textbook, we’ll focus on “Frequentist” approach of probability.\n \nFundamental Properties of Probability\nThe following properties are useful to assign and manipulate event probabilities.\n\n\n\n\n\n\nFundamental Properties of Probability\n\n\n\n\nThe probability of the null event is zero, P(∅) = 0.\nThe probability of the complement event A satisfies the property:\n\n\\[P(A') = 1 − P(A) \\tag{3.3}\\]\nThe complement of event A is shown by a little mark after the letter such as \\(A'\\) (or sometimes \\(A^{c}\\) or \\(\\bar{A}\\)):\n\nThe probability of the union of two events satisfies the general property that (Addition Rule of Probability) :\n\n\\[P(A ∪ B) = P(A) + P(B) − P(A ∩ B) \\tag{3.4}\\]\n\n\n \n\n\n\n\n\n\nExample - Dice: The complement probability of the event A\n\n\n\nSuppose we throw a six-die, what is the complement probability of the event A “rolling either 5 or 6”?\n\n\n\n\nThe complement of Event A = {5, 6} is \\(A'\\) = {1, 2, 3, 4} and the complement probability is:\n\\(P(A')\\) = 1-P(rolling a 5 or 6) = 1-P(rolling a 5) - P(rolling a 6) = 1- 1/6 - 1/6 = 1 - 2/6 = 1 - 1/3 = 2/3\n\n\n \nThe Conditional Probability\nThe conditional probability is indicated as P(A|B) or A given B. The following relationship defines the conditional probability:\n\\[P(A ∩ B) = P(A|B) · P(B) \\tag{3.5}\\]\nor\n\\[ P(A|B)=  \\frac{P(A ∩ B)}{P(B)} \\tag{3.6}\\]\n\n\n\n\n\n\nExample - Dice: Conditional probability\n\n\n\nCalculate the probability of obtaining 8 as the sum of two rolls of a die, given that the first roll was a 3.\nThe sample space of the experement consists of all ordered pairs of numbers from 1 to 6. That is, S = {(1, 1), (1, 2),… , (1, 6), (2, 1),… , (6, 6)}.\nIt is useful to define the following two events:\n\nA = {The sum of two rolls is 8}.\nB = {The first roll shows 3, and the second any number}.\n\nEvent A is given by outcomes A={(2,6), (3,5), (4,4), (5,3), (6,2)} :\n\n\n1st roll\n2\n3\n4\n5\n6\n\n\n2nd roll\n6\n5\n4\n3\n2\n\n\nSum\n8\n8\n8\n8\n8\n\n\nand since each combination has a probability of 1/36, P(A) = 5/36.\nEvent B is given by outcomes B={(3,1), (3,2), (3,3), (3,4), (3,5), (3, 6)}. The probability of event B is P(B) = 6/36 = 1/6.\nAlso, the event A ∩ B occurs if the first roll is a 3 and the sum is 8, which can clearly occur only if a sequence of (3,5) takes place, thus with probability P(A ∩ B) = 1/36.\nAccording to the definition of conditional probability Equation 3.6, the probability of interest is:\n\\(P(A|B) = \\frac{P(A ∩ B)}{P(B)} = \\frac{1/36}{1/6} = \\frac{1}{6}\\)\nTherefore, the occurrence of 3 in the first roll has increased the probability of A from P(A) = 5/36= 0.139 to P(A|B) = 1/6= 0.166.\n\n\n \nStatistical Independence\nThe concept of statistical independence among events means that the occurrence of one event has no influence on the occurrence of other events. Consider, for example, rolling two dice, one after the other: the outcome of one die is independent of the other and the two tosses are said to be statistically independent.\nOn the other hand, consider rolling two dice, and being interested in the following pair of events: the first is the outcome of the roll of die 1 and the second is the sum the rolls of die 1 and die 2. It is clear that the outcome of the second event—e.g., the sum of both dice—depends on the first toss and the two events are not independent.\nTwo events A and B are said to be statistically independent if:\n\\[P(A ∩ B) = P(A) · P(B) \\tag{3.7}\\]\nEquation 3.7, known as Multiplication Rule of Probability, follows directly from Equation 3.5. In fact, if A and B are statistically independent, then the conditional probability is P(A|B) = P(A), i.e., the occurrence of B has no influence on the occurrence of A.\n\n\n\n\n\n\nExample - Statistical independence with dice\n\n\n\nDetermine the probability of obtaining two 3s when rolling two dice. This event can be decomposed in two events:\n\nA = {die 1 shows 3, and die 2 shows any number},\nB = {die 2 shows 3, and die 1 shows any number}.\n\nIt is natural to assume that P(A) = 1/6, P(B) = 1/6, and state that the two events A and B are independent by nature, since each event involves a different die, which has no knowledge of the outcome of the other one; the same would be true also of the same die tossed two times. The event of interest is A ∩ B, and the definition of probability of two statistically independent events leads to \\(P(A ∩ B) = P(A) · P(B) = 1/6 * 1/6 = 1/36\\).\nThis result can be confirmed by a direct count of all possible outcomes in the toss of two dice, and the fact that there is only one combination out of 36 that gives rise to two consecutive 3s.\n\n\n \nBayes’ theorem\nThe Bayes’ theorem can be written as:\n\\[P(A|B) = \\frac{P(B|A)· P(A)}{P(B)} \\tag{3.8}\\]\nwhere A and B are events and \\(P(B)\\neq 0\\).\n\n\n\n\n\n\n Probabilities involved in Bayes’ theorem (Advanced)\n\n\n\n\n\nThe experiment B can be considered as the data collected in a given experiment. The event A is a model that is used to describe the data.\nAccordingly, the probabilities involved in Bayes’ theorem can be interpreted as follows:\n\nP(B|A) is the probability, or likelihood L, of the data given the specified model. Notice how P(B|A) means that the model A is given, or known.\nP(A) is the probability of the model A, without any knowledge of the data.This term is interpreted as a prior probability, or the degree of belief that the model is true before the measurements are made.\nP(B) is the probability of collecting the dataset B.\nFinally, P(A|B) is the posterior probability of the model after the data have been collected.The posterior probability is the ultimate goal of the analysis since it describes the probability of the model based on the collection of data.\n\nThis interpretation of Bayes’ theorem is the foundation of Bayesian statistics, and it can be summarized as:\nPosterior probability ∝ Likelihood × Prior probability\nBayes’ theorem provides a way to update the prior knowledge of model parameters given the measurements, leading to posterior estimates of parameters. One key feature of Bayesian statistics is that the calculation of probability is based on a prior probability, which may rely on a subjective interpretation of what is known about the experiment before any measurements are made. Therefore, great attention must be paid to the assignment of prior probabilities and the effect of priors on the final results of the analysis.\n\n\n\n\n\n\n\n\n\nExample - Probability of developing lung cancer for smokers\n\n\n\nSuppose that the probability of having lung cancer is P(C) = 0.001 and that the probability of being a smoker is P(SM) = 0.25\nFurther, suppose we know that if a person has lung cancer, the probability of being a smoker increases to P(SM|C) = 0.4. We are, however, interested in the probability of developing lung cancer if a person is a smoker, P(C|SM).\nFrom Equation 3.8:\n\\[P(C|SM) = \\frac{P(SM|C)· P(C)}{P(SM)} = \\frac{0.4· 0.001}{0.25}= 0.0016\\] Therefore, the probability of lung cancer increases from 0.001 to 0.0016 for smokers. That is, the probability becomes 60%, (0.0016-0.001)/0.001= 0.6, higher than the overall probability of lung cancer."
  },
  {
    "objectID": "distributions.html#random-variables",
    "href": "distributions.html#random-variables",
    "title": "3  Probability Distributions - Normal distribution",
    "section": "\n3.3 Random Variables",
    "text": "3.3 Random Variables\nFormally, a random variable X assigns a numerical value to each possible outcome of a random phenomenon. For instance, we can define X based on possible genotypes of a bi-allelic gene A as follows:\n\\[X={\\begin{cases}0,&for\\ genotype\\ AA\\\\1,&for\\ genotype\\ Aa\\\\2,&for\\ genotype\\ aa\\end{cases}}\\]\nIn this case, the random variable assigns 0 to the outcome AA, 1 to the outcome Aa, and 2 to the outcome aa.\nThe set of values that a random variable can assume is called its range. For the above example, the range of X is {0, 1, 2}.\nAfter we define a random variable, we can find the probability for its possible value based on the underlying random phenomenon. This way, instead of talking about the probability for different outcomes and events, we can talk about the probability of different values for a random variable.\nAssume that the individual probabilities for different genotypes are P(AA) = 0.49, P(Aa) = 0.42, and P(aa) = 0.09. Then, instead of saying P(AA) = 0.49, i.e., the genotype is AA with probability 0.49, we can say that P(X = 0) = 0.49, i.e., X is equal to 0 with probability of 0.49. Likewise, P(X = 1) = 0.42 and P(X = 2) = 0.09.\nNote that the total probability for the random variable is still 1. In what follows, we write P(X) to denote the probability of a random variable X in general without specifying any value or range of values. The probability rules we discussed earlier also apply to random variables. Specifically, concepts such as independence and conditional probability are defined similarly for random variables as they are defined for random events. For example, when two random variables do not affect each other’s probabilities, we say that they are independent.\nA random variable is also expected to have a theoretical distribution, e.g., Normal, Poisson, etc., according to the nature of the variable itself and the method of measurement.\nEach distribution is entirely defined by several specific parameters. The parameter values determine the location and shape of the curve on the plot of distribution, and each unique combination of parameter values produces a unique distribution curve.\n\n\n\n\n\n\nWhat do we need to know for a random variable?\n\n\n\nTo fully understand a random variable, we need to know:\n\n\nevery possible value, or the interval of values of the random variable.\nthe probability corresponding to each possible value or value ranges (probability distribution).\n\n\n\nFor the random variable X defined based on genotypes, the probability distribution can be simply specified as follows:\n\\[P(X=x)={\\begin{cases}0.49,&for\\ x=0\\\\0.42,&for\\ x=1\\\\0.09,&for\\ x=2\\end{cases}}\\] Here, x denotes a specific value (i.e., 0, 1, or 2) of the random variable. Probability distributions are specified differently for different types of random variables. In the following, we divide the random variables into two major groups: discrete and continuous. Then, we provide several examples for each group."
  },
  {
    "objectID": "distributions.html#probability-distributions-for-discrete-outcomes",
    "href": "distributions.html#probability-distributions-for-discrete-outcomes",
    "title": "3  Probability Distributions - Normal distribution",
    "section": "\n3.4 Probability distributions for Discrete Outcomes",
    "text": "3.4 Probability distributions for Discrete Outcomes\nFor discrete random variables, the probability distribution is fully defined by the probability mass function (pmf). This is a function that specifies the probability of each possible value within range of random variable.\n \nBernoulli distribution\nBinary random variables are abundant in scientific studies. Bernoulli distribution applies to events that have one trial and two possible outcomes.\nA Bernoulli event is one for which the probability the event occurs (success; X=1) is p and the probability the event does not occur (failure; X=0) is 1-p. As before, the probability for all possible values is one: P(X = 0) + P(X = 1) = 1.\nA Bernoulli trial is an instantiation of a Bernoulli event. So long as the probability of success or failure remains the same from trial to trial (i.e., each trial is independent of the others), a sequence of Bernoulli trials is called a Bernoulli process.\n\n\n\n\n\n\nDefinition-Bernoulli distribution\n\n\n\nThe binary random variable X with possible values 0 and 1 has a Bernoulli distribution with parameter p , where P(X = 1) = p and P(X = 0) = 1 − p . We denote this as X ∼ Bernoulli(p), where 0 ≤ p ≤ 1.\nHere, p is the unknown parameter. If p were known, we could fully specify the probability mass function:\n\\[P(X=x)={\\begin{cases}q=1-p,&for\\ x=0\\\\p,&for\\ x=1\\end{cases}}\\]\nwhere \\({0\\leq p\\leq 1}\\).\nThe mean of a binary random variable, X, with Bernoulli(p) distribution is p. We show this as:\n\\[μ = p \\tag{3.9}\\]\nIn this case, the mean can be interpreted as the proportion of the population who have the outcome of interest.\nFurthermore, the variance of a random variable with Bernoulli(p) distribution is:\n\\[ \\sigma^2= pq=p(1-p)= μ(μ-1) \\tag{3.10}\\]\n\\[ \\sigma= \\sqrt{p(1-p)}= \\sqrt{μ(μ-1)} \\tag{3.11}\\]\n\n\n\n\n\n\n\n\nExample-Bernoulli distribution: breast cancer\n\n\n\nFor example, let X be a random variable representing the five-year survival status of breast cancer patients, where X = 1 if the patient survived and X = 0 otherwise. Suppose that the probability of survival is p = 0.8: P(X = 1) = 0.8. Therefore, the probability of not surviving is P(X = 0) = 1 − p = 0.2. Then X has a Bernoulli distribution with parameter p = 0.8, and we denote this as\n\\[X ∼ Bernoulli(0.8)\\] The pmf for this distribution is:\n\\[P(X=x)={\\begin{cases}0.2,&for\\ x=0\\\\0.8,&for\\ x=1\\end{cases}}\\] Additionally, we can plot pmf for visualizing the distribution Figure 3.2.\n\n\n\n\nFigure 3.2: Plot of the pmf for Bernoulli(0.8) distribution.\n\n\n\n\nThe height of each bar is the probability of the corresponding value on the horizontal axis. The height of the bar is 0.2 at X = 0 and 0.8 at X = 1. Since the probability for all possible values of the random variable is 1, the bar heights add up to 1.\nIn the above example, from Equation 3.9 we take μ = 0.8. Therefore, we expect 80% of patients survive.\nFrom Equation 3.10 the variance of the random variable is \\(σ^2 = 0.8 × 0.2 = 0.16\\), and its standard deviation is \\(σ = 0.4\\). This reflects the extent of variability in survival status from one person to another. For this example, the amount of variation is rather small. Therefore, we expect to see many survivals (X = 1) with occasional death (X = 0). For comparison, suppose that the probability of survival for bladder cancer is θ = 0.6. Then, the variance becomes \\(σ^2 = 0.6×(1−0.6) = 0.24\\). This reflects a higher variability in the survival status for bladder cancer patients compared to that of breast cancer patients.\n\n\n \nBinomial distribution\nThe binomial distribution is an important theoretical distribution with wide applications in biomedicine. Many biological phenomena can be described using a binomial distribution.\nThe Bernoulli distribution represents the success or failure of a single Bernoulli trial. The Binomial Distribution represents the number of successes and failures in \\(n\\) independent Bernoulli trials for some given value of n.\n\n\n\n\n\n\nDefinition-Binomial distribution\n\n\n\nThe random variable X representing the number of times the outcome of interest occurs in \\(n\\) Bernoulli trials has a \\(Binomial(n, p)\\) distribution, where p is the probability of the outcome of interest (success). A binomial distribution is defined by the number of Bernoulli trials \\(n\\) and the probability of the outcome of interest p for the underlying Bernoulli trials.\n\\[ P(X=x) = {{n}\\choose{x}} \\cdot p^xq^{n-x} \\tag{3.12}\\]\nwhere x = 0, 1, … , n, \\({0\\leq p\\leq 1}\\), q = 1 − p and \\(\\binom{n}{x} = \\frac{n!}{x!(n-x)!}\\)\nNote that: \\(n! = 1\\cdot 2 \\cdot 3\\cdot \\ldots \\cdot (n-2)\\cdot (n-1)\\cdot n\\)\nIf is a binomial random variable with parameters and n and p, then\n\\[μ = np \\tag{3.13}\\]\n\\[σ^2 = npq \\tag{3.14}\\]\n\\[σ = \\sqrt{npq} \\tag{3.15}\\]\n\n\n\n\n\n\n\n\nExample-Binomial distribution: breast cancer\n\n\n\nSuppose that we plan to recruit a group of 50 patients with breast cancer and study their survival within five years from diagnosis. We represent the survival status for these patient by a set of Bernoulli random variables \\(X_1, . . . , X_{50}\\). (For each patient, the outcome is either 0 or 1). Assuming that all patients have the same survival probability, p = 0.8, and the survival status of one patient does not affect the probability of survival for another patient, \\(X_1, . . . , X_{50}\\) form a set of 50 Bernoulli trials.\nNow we can create a new random variable X representing the number of patients out of 50 who survive for five years. The number of survivals is the number of 1s in the set of Bernoulli trials. This is the same as the sum of Bernoulli trials, whose values are either 0 or 1:\n\\[X=\\sum_{i=1}^{50}X_{i} \\tag{3.16}\\]\nwhere \\(X_i = 1\\) if the ith patient survive and \\(X_i = 0\\) otherwise.\nSince X can be any integer number from 0 (no one survives) through 50 (everyone survives), its range is {0, 1, . . . , 50}. The range is a countable set. Therefore, the random variable X is discrete. The probability distribution of X is a binomial distribution, shown as:\n\\[X ∼ Binomial(50, 0.8)\\]\nThe pmf of Binomial(50, 0.8) distribution specifies the probability of 0 through 50 survivals.\nAccording to Equation 3.12 we have:\n\n\nX\n0\n1\n…\n34\n35\n36\n…\n40\n…\n\n\nP(X)\n0\n0\n…\n0.02\n0.03\n0.05\n…\n0.14\n…\n\n\n\n\n\n\nFigure 3.3: Plot of the pmf for Binomial(50, 0.8) distribution.\n\n\n\n\nAs before, the height of each bar is the probability of the corresponding value on the x-axis. For example, the probability of 40 survivals (out of 50) is P(X=40)=0.14. Also, since the probability for all possible values of the random variable is 1, the bar heights add up to 1. Note that for numbers below 30 and above 48, the probability is almost zero.\nNow suppose that we are interested in the probability that either 34 or 35 or 36 patients survive. Since the underlying event include three possible outcomes, 34, 35, and 36, we obtain the probability by adding the individual probabilities for these outcomes:\n\\[P(34≤X ≤ 36) = P(X = 34) + P(X = 35) + P(X = 36) = 0.02 + 0.03 + 0.05 = 0.1\\]\nFor the breast cancer example and Equation 3.13, the mean of the random variable is 50×0.8 = 40. If we recruit 50 patients, we expect 40 people survive over five years. Of course, the actual number of survivals can change from one group to another (e.g., if we take another group of 50 patients). According to Equation 3.14, the variance of X in the above example is 50 × 0.8 × 0.2 = 8, which shows the extent of the variation of the random variable around its mean.\n\n\nPoisson distribution\nSo far, we have discussed the Bernoulli distribution for binary variables, and the binomial distribution for the number of times the outcome of interest (one of the two possible categories of the binary variable) occur within a set of n Bernoulli trials.\nWhile a random variable with a Binomial distribution is a count variable (e.g., number of people survived), its range is restricted to include integers from 0 through n only. For example, the number of survivals in a group of n = 50 cancer patients cannot exceed 50.\nNow, suppose that we are investigating the number of physician visits for each person in one year. Although very large numbers such as 100 are quite unlikely, there is no theoretical and prespecified upper limit to this random variable. Theoretically, its range is the set of all nonnegative integers.\n\n\n\n\n\n\nDefinition-Poisson distribution\n\n\n\nRandom variables representing counts within temporal and/or spacial limits but without prespecified upper limits are often assumed to have Poisson distributions. The range of these variables is the set of all non negative integers (i.e., the lower limit is zero, but there is no upper limit). A Poisson distribution is specified by a parameter λ, which is interpreted as the rate of occurrence within a time period or space limit. We show this as X ∼ Poisson(λ), where λ is a positive real number (λ>0).\n\\[ P(X=x)={\\frac {\\lambda ^{x}e^{-\\lambda }}{x!}}  \\tag{3.17}\\]\nwhere x = 0, 1, … +∞, λ > 0.\nThe mean and variance of a random variable with Poisson(λ) distribution are the same and equal to λ. That is, μ = λ and \\(σ^2 = λ\\).\n\n\n\n\n\n\n\n\nExample-Poisson distribution: physician visits\n\n\n\nAs an example, assume that the rate of physician visits per year is 2.5:\n\\[X ∼ Poisson(2.5)\\] Therefore, the population mean and variance of this variable is 2.5.\nAccording to Equation 3.17 the resulting probability table is:\n\n\nX\n0\n1\n2\n3\n5\n6\n\n\nP(X)\n0.08\n0.21\n0.26\n0.21\n0.07\n0.03\n\n\nThe resulting plot of the pmf shows the probability of each possible value, which is any integer from 0 to infinity Figure 13.8. In this case, the probability of values above 8 becomes almost 0.\n\n\n\n\nFigure 3.4: Plot of the pmf for Poisson(2.5) distribution.\n\n\n\n\nFor this example, the probability that a person does not visit her/his physician within a year is P(X = 0) = 0.08, while the probability of one visit per year increases to P(X = 1) = 0.21.\nNow suppose that we want to know the probability of up to three visits per year: P(X ≤ 3). This is the probability that a person visit her/his physician 0, or 1, or 2, or 3 times within one year. As before, we add the individual probabilities for the corresponding outcomes: P(X ≤ 3) = 0.08 + 0.21 + 0.26 + 0.21 = 0.76.\nThe population mean and variance of this variable is 2.5 visits per year."
  },
  {
    "objectID": "distributions.html#probability-distributions-for-continuous-outcomes",
    "href": "distributions.html#probability-distributions-for-continuous-outcomes",
    "title": "3  Probability Distributions - Normal distribution",
    "section": "\n3.5 Probability distributions for Continuous Outcomes",
    "text": "3.5 Probability distributions for Continuous Outcomes\nFor discrete random variables, the pmf provides the probability of each possible value. For continuous random variables, the number of possible values is uncountable, and the probability of any specific value is zero.\nTherefore, instead of talking about the probability of any specific value x for continuous random variable X, we talk about the probability that the value of the random variable is within a specific interval from x1 to x2; we show this probability as P(x1 ≤ X ≤ x2).\n\\[ P(x_1\\leq X \\leq x_2)=\\int_{x_1}^{x_2}f(x)dx  \\tag{3.18}\\]\nwhere f(x) is the probability density functions (pdf) of X .\nClearly, in Equation 3.18, the probability of a certain point value in X is zero, and the area under the probability density curve of the interval (−∞, +∞) should be 1.\nNormal Distribution\nThere are several important probability distributions in statistics. However, the normal distribution might be the most important. First, Galileo informally described a normal distribution in 1632 when discussing the random errors from observations of celestial phenomena. However, Galileo existed before the time of differential equations and derivatives. We owe its formalization to Carl Friedrich Gauss, which is why the normal distribution is often called a Gaussian distribution. A very familiar example is the height for adult people that approximates a normal distribution very well.\nA normal distribution is the familiar “bell curve” and it’s a way of formalizing a distribution where observations cluster around some central tendency. Observations farther from the central tendency occur less frequently (Figure 3.5).\n\n\n\n\nFigure 3.5: The Normal probability distribution (often called Gaussian or bell-shaped distribution).\n\n\n\n\n\n\n\n\n\n\nDefinition of Normal Distribution\n\n\n\nGauss’ normal distribution, technically a density function, is a distribution defined by two parameters, mean \\(\\mu\\) and variance \\(\\sigma^2\\). The mean, \\(\\mu\\), represents the population mean and is a “location parameter”, which defines the central tendency. The variance, \\(\\sigma^2\\) is the “scale parameter”, which defines the width of the distribution and how short the distribution is. It’s formally given as Equation 3.19:\n\\[ f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}  \\tag{3.19}\\]\nwhere \\(\\pi=3.14\\) and \\(e = 2.718\\).\n\n\nPopulations with small values of the standard deviation, \\(\\sigma\\), have a distribution concentrated close to the centre, \\(\\mu\\); those with large standard deviation, \\(\\sigma\\), have a distribution widely spread along the measurement axis Figure 3.6.\n\n\n\n\n(a) Effect of changing mean.\n\n\n\n\n(b) Effect of changing standard deviation.\n\n\n\nFigure 3.6: Probability distribution functions of the Normal distributions with different means and standard deviations.\n\n\n \n\n\n\n\n\n\nProperties of Normal Distribution\n\n\n\n\n\n\n\nFigure 3.7: The area underneath a Normal Distribution\n\n\n\n\nThe Normal distribution has the properties summarized as follows:\n\nBell shaped and symmetrical around the mean. Shape statistics, skewness and excess kurtosis are zero.\nThe peak of the curve lies above the mean.\nAny position along the horizontal axis (x-axis) can be expressed as a number of standard deviations from the mean.\nAll three measures of central tendency mean, the median, and the mode will be the same.\nMuch of the area (68%) of the distribution is between -1 \\(\\sigma\\) below the mean and +1 \\(\\sigma\\) above the mean, the large majority (95%) between -1.96 \\(\\sigma\\) below the mean and +1.96 \\(\\sigma\\) above the mean (often used as a reference range), and almost all (99%) between -2.58 \\(\\sigma\\) below the mean and +2.58 \\(\\sigma\\) above the mean. The total area under the curve equals to 1 (or 100%).\n\n\n\n \n\n\n\n\n\n\nExample-Normal distribution: systolic blood pressure\n\n\n\nSuppose we know that the population mean and standard deviation for systolic blood pressure (sbp) are μ = 125 mmHg and σ = 15 mmHg, respectively.\n\nAbout what percentage of population has sbp in the range μ±σ ?\n\nThe X random variable of sbp follows the normal distribution:\n\\[ X ∼ N(125, 15^2)\\]\nThus, the percentage of the distribution between -1 \\(\\sigma\\) below the mean and +1 \\(\\sigma\\) above the mean is:\n\\[ P(125 −15≤X≤ 125+15) = P(110≤X≤ 140) = 0.68\\ or\\ 68\\% \\]\n\nCalculate a 95% reference range for the sbp:\n\n\\[ (125 - 1.96\\times 15,\\ 125 + 1.96\\times 15) = (95.6, 154.4) \\]\n\n\n \nStandard Normal distribution\nIf the random variable X has a normal distribution with \\(\\mu\\) and standard deviation \\(\\sigma\\), then the standardized Normal deviate is:\n\\[ z= \\frac{x-\\mu}{\\sigma}  \\tag{3.20}\\]\nThe z (often called z-score) is a random variable that has a Standard Normal distribution, also called a z-distribution, i.e. a special normal distribution where \\(\\mu=0\\) and \\(\\sigma^2=1\\). In this case, Equation 3.19 is transformed as follows:\n\\[ f(z)={\\frac {1}{{\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}z^2}  \\tag{3.21}\\]\n\n\n\n\nFigure 3.8: Standard Normal Distribution\n\n\n\n\n\n\n\n\n\n\n Understanding the formula of Standard Normal distribution\n\n\n\n\n\nWe can break down individual components of a z-distribution (Equation 3.20) and explain them until they seem more accessible.\nFirst, we know from algebra that the formula \\(\\ {\\frac {1}{2}}z^{2}\\) is a basic parabola (notice the square term). Adding a minus sign just flips the basic parabola \\(\\ {\\frac {1}{2}}z^{2}\\) downward and we take a negative parabola \\(\\ -{\\frac {1}{2}}z^{2}\\).\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\nFigure 3.9: (a) A basic parabola and (b) a negative parabola.\n\n\n\n\nSecond, exponentiating the negative parabola (\\(\\  e^{-{\\frac {1}{2}}z^{2}}\\)) makes it asymptote to 0.\n\n\n\n\nFigure 3.10: An exponentiated negative parabola.\n\n\n\n\nNotice the tails in the Figure 3.10 are asymptote to 0. “Asymptote” is a fancier way of saying the tails approximate 0 but never touch or surpass 0. One way of thinking about this as we build toward its inferential implications is that deviations farther from the central tendency are increasingly “unlikely”.\nThird, and with the above point in mind, it should be clear that \\(\\ {\\frac {1}{{\\sqrt {2\\pi }}}}\\) will scale the height of the distribution. Observe that the height of the exponentiated parabola is at 1. That gets multiplied by \\(\\ {\\frac {1}{\\sqrt {2\\pi }}}\\) to equal about 0.398.\nFourth, the z-distribution is perfectly symmetrical around the zero. A given value of z will be as far from zero as -z.\n\n\n\n \nThe standard normal distribution is centered at zero and the probability that z is between 1 on either side of 0 is effectively 0.68. The ease of this interpretation is why researchers like to standardize their variables so that the mean is 0 and the standard deviation is 1.\n\n\n\n\n\n\nExample-Diastolic Blood Pressure\n\n\n\nZ-scores are often used in medical settings to assess how an individual’s blood pressure compares to the mean population blood pressure.\nFor example, the distribution of diastolic blood pressure for men is normally distributed with a mean of about 80 and a standard deviation of 20.\nIf a certain man has a diastolic blood pressure of 100, we would calculate their z-score (Equation 3.21) to be:\n\\(z = \\frac{(100 – 80)}{20} = 1\\)\nThis means that this man has a diastolic blood pressure that is 1 standard deviation above the mean.\n\n\n \nTo find the area under the curve between two z-scores, \\(z_1\\) and \\(z_2\\), we have to integrate the pdf Equation 3.21 as following:\n\\[ E=P(z_1\\leq Z\\leq z_2)={\\frac {1}{{\\sqrt {2\\pi }}}}\\int_{z_1}^{z_2}e^{-{\\frac {1}{2}}z^2}dz  \\tag{3.22}\\]\n \n\n\n\n\n\n\nExample-Area under the curve\n\n\n\nCalculate the area under the curve:\n\n\n\n\nFigure 3.11: Standard normal distribution with a shaded region\n\n\n\n\nOne solution is to use the Equation 3.22 with \\(z_1\\) = 1 and \\(z_2\\) = 2:\n\\[ E=P(1\\leq Z\\leq 2)={\\frac {1}{{\\sqrt {2\\pi }}}}\\int_{1}^{2}e^{-{\\frac {1}{2}}z^2}dz \\]\nIn this case, however, we can easily calculate the area using the properties of the normal distribution:\n\\(E = P(0\\leq Z\\leq 2) - P(0\\leq Z\\leq 1) \\approx 0.475 - 0.34 \\approx 0.135\\)\n\n\n \nt-distribution\nThe t-distribution (Student’s t-distribution) is a continuous probability distribution that is used in place of the Standard Normal distribution for small samples when the population variance, \\(σ^2\\), is unknown. William Sealy Gosset derived the t-distribution whilst working at the Guinness brewery, but was not permitted by his employer to publish under his own name so he decided to use the pseudonym “Student” for his published work.\n\n\n\n\n\n\nDefinition and properties of t-distribution\n\n\n\nA t-distribution is specified by only one parameter called the degrees of freedom (df). The t-distribution with df degrees of freedom is usually denoted as t(df), where df is a positive real number (df > 0) and equals to the sample size minus one.\nThe mean of this distribution is μ = 0, and the variance is determined by the degrees of freedom parameter, \\(σ^2 = df/(df −2)\\), which is of course defined when df > 2.\n\n\n\n\nFigure 3.12: t-distribution with df=3 Vs Standard Normal distribution\n\n\n\n\nSimilar to the standard normal distribution, the probability density curve for a t-distribution is unimodal and symmetric around its mean of μ = 0. However, the variance of a t-distribution is greater than the variance of the standard normal distribution: df/(df − 2) > 1. As a result, the probability density curve for a t -distribution approaches zero more slowly than that of the standard normal. We say that t-distributions have heavier tails than the standard normal distribution.\nBasic Properties of t-distribution:\n\nA t-distribution is symmetric about 0.\nA t-distribution extends indefinitely in both directions, approaching, but never touching, the horizontal axis as it does so.\nThe total area under a t-curve equals 1.\nAs the sample size (and degrees of freedom) becomes larger (>30), t-distributions look increasingly like the Standard Normal distribution.\n\n\n\n\n\n\n\n\n\nExample-Degrees of freedom\n\n\n\n\nDegrees of freedom, often represented by df, is the number of independent pieces of information used to calculate a statistic. It’s calculated as the sample size minus the number of restrictions.\n\nSuppose we take a random sample of 10 adults and measure their daily calcium intake. Let’s say we find that the sample mean is 820 mg.\nFor example, assume that the nine of the ten people in the sample have daily calcium intakes of 410, 1230, 870, 1110, 570, 390, 1030, 1080, and 630 mg. The tenth individual must have a daily calcium intake of 880 mg for the sample to have a mean of 820 mg.\nBecause of the restriction, only nine values in the sample are free to vary. Consequently, the final value isn’t free to vary; it only has one possible value. In this case, the degrees of freedom are df = 10-1 = 9.\n\n\n \nChi-squared distribution\nChi-square (\\(\\chi^2\\)-distribution) distributions are a family of continuous probability distributions. They’re widely used in hypothesis tests, including the chi-square goodness of fit test and the chi-square test of independence.\n\n\n\n\n\n\nDefinition and properties of chi-squared distribution\n\n\n\nThe chi-squared distribution (or \\(\\chi^2\\)-distribution) with n degrees of freedom (df=n) is the distribution of a sum of the squares of n independent Standard Normal random variables \\(Z_i\\).\n\\[X^2 =\\sum _{i=1}^{n}Z_{i}^{2} \\tag{3.23}\\]\n\n\n\n\nFigure 3.13: chi-squared distribution with df=10, 15, and 20\n\n\n\n\nThe mean of this distribution is μ = df, and the variance is determined by the degrees of freedom parameter, \\(σ^2 = 2df\\)\nBasic Properties of chi-squared distribution: The chi-squared distribution is always positive and its shape is uniquely determined by the degrees of freedom. The distribution becomes more symmetrical as the degrees of freedom increase and when df>50, the chi-squared distribution is very similar to the Normal distribution.\n\n\n \nF-distribution\nThe F-distribution is the ratio of two chi-squared distributions and is used in hypothesis testing of whether two observed samples have the same variance.\n\n\n\n\n\n\nDefinition and properties of F-distribution\n\n\n\nLet \\(X_n^2\\) and \\(X_m^2\\) be independent variates distributed as chi-squared with n and m degrees of freedom. \\[F_{n,m}=\\dfrac{X_{n}^{2}/n}{X_{m}^{2}/m}\\]\nThe mean of this distribution is:\n\\[\\frac {m} {(m - 2)} \\;,\\;\\;\\;\\ with\\;\\ m > 2\\]\n\n\n\n\nFigure 3.14: F-distribution with different df1 and df2\n\n\n\n\nBasic Properties of F-distribution:\nThe F-distribution is always positive, but the exact shape depends on the degrees of freedom for the two chi-squared distributions that determine it."
  },
  {
    "objectID": "distributions.html#descriptive-methods-for-assessing-normality",
    "href": "distributions.html#descriptive-methods-for-assessing-normality",
    "title": "3  Probability Distributions - Normal distribution",
    "section": "\n3.6 Descriptive Methods for Assessing Normality",
    "text": "3.6 Descriptive Methods for Assessing Normality\nIt is important to assess whether the distribution of a set of empirical data approximates the normal distribution. There are three simple and practical methods:\n\nHistogram: If the data are approximately normally distributed, the shape of the histogram is similar to the normal distribution curve.\nShape statistics: One way of measuring non-Normality skewness and kurtosis are two statistical moments for assessing Normality.\nQ-Q plot: A graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nBell-shaped Empirical Distribution\nThe normal distribution provides an adequate model for the relative frequency distributions of data (empirical distributions) collected from many different biomedical areas, such as adult height, weight, vital capacity, and red blood cell count. Moreover, many other distributions that are not normal themselves can be made approximately normal by transforming the data into a different scale.\n\n\n\n\n\n\nExample-Bell-shaped Empirical Distribution\n\n\n\nIf the shape of empirical distributions of random variables approximates the Gaussian distribution then the variables are considered to be distributed normally.\n\n\n\n\nFigure 3.15: Empirical relative frequency distribution of the birthweight.\n\n\n\n\n\n\n \nShape statistics and normality\nThere are two shape statistics that can indicate deviation from normality: skewness and kurtosis.\nA. Skewness\nSkewness is usually described as a measure of a distribution’s symmetry – or lack of symmetry. Skewness values that are negative indicate a tail to the left (Figure 3.16 a), zero value indicate a symmetric distribution (Figure 3.16 b), while values that are positive indicate a tail to the right (Figure 3.16 c).\nSkewness values between −1 and +1 indicate an approximate bell-shaped curve. Values from −1 to −3 or from +1 to +3 indicate that the distribution is tending away from a bell shape with >1 indicating moderate skewness and >2 indicating severe skewness. Any values above +3 or below−3 are a good indication that the variable is not normally distributed.\n\n\n\n\n(a) Left skewed distribution (negatively skewed). The mean and the meadian are too left to the mode.\n\n\n\n\n(b) Symmetric distribution (zero skewness). The mean, median and mode are the same.\n\n\n\n\n(c) Right skewed distribution (positively skewed). The mean and median are to the right of the mode.\n\n\n\nFigure 3.16: Types of distribution according to the summetry.\n\n\nB. Kurtosis\nThe other way that distributions can deviate from normality is kurtosis. The excess kurtosis parameter is a measure of the combined weight of the tails relative to the rest of the distribution. Kurtosis is associated indirect with the peak of the distribution (if the peak of the distribution is too high or too low compared to a “normal” distribution).\nDistributions with negative excess kurtosis are called platykurtic (Figure 3.17 a). If the measure of excess kurtosis is 0 the distribution is mesokurtic (Figure 3.17 b). Finally, distributions with positive excess kurtosis are called leptokurtic (Figure 3.17 c).\nA kurtosis value between −1 and +1 indicates normality and a value between −1 and −3 or between +1 and +3 indicates a tendency away from normality. Values below −3 or above +3 strongly indicate non-normality.\n\n\n\n\n(a) Platykurtic distribution (negative excess kurtosis).\n\n\n\n\n(b) Mesokurtic distribution (zero excess kurtosis).\n\n\n\n\n(c) Leptokurtic (positive excess kurtosis).\n\n\n\nFigure 3.17: Types of distribution according to the summetry.\n\n\n \nQ-Q plots\nThe normal Q–Q plot shows each data value plotted against the value that would be expected if the data came from a normal distribution. The values in the plot are the quantiles of the variable distribution plotted against the quantiles that would be expected if the distribution was normal. If the variable was normally distributed, the, points would fall directly on the straight line. Any deviations from the straight line indicate some degree of non-normality.\n\n\n\n\nFigure 3.18: Q-Q plot of Birthweight.\n\n\n\n\n\n\n\n\n\n\nProperties of an approximately bell-shaped empirical distribution\n\n\n\nIn an approximately bell-shaped empirical distribution:\n\nthe mean, the median and the mode have very close values\nthe histogram is symmetric about the mean\n“nearly all” values (99.7%) are within -3 and +3 standard deviations of the mean (Figure 3.7)\nthe measure of skewness takes values close to zero (symmetric distribution) (Figure 3.16 b). Particularly, values between −1 and +1 indicate an approximately bell-shaped curve.\nthe measure of excess kurtosis is close to 0 (mesokurtic) (Figure 3.17 b). A kurtosis value between −1 and +1 indicates normality.\nIn Q-Q plot (Figure 3.18) the points fall close to the straight line."
  },
  {
    "objectID": "distributions.html#exercises",
    "href": "distributions.html#exercises",
    "title": "3  Probability Distributions - Normal distribution",
    "section": "\n Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise 1.1\n\n\n\nIn 2012, 1,664,479 students took the SAT exam. The distribution of scores in the verbal section of the SAT had a mean μ=496 and a standard deviation σ=114 . Let X= a SAT exam verbal section score in 2012. Then \\(X∼N(496,114^2)\\).\nFind the z -scores for \\(x_1\\)=325 and \\(x_2\\)=366. Interpret each z -score. What can you say about \\(x_1\\)=325 and \\(x_2\\)=366 ?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe z-score (Equation 3.20) for \\(x_1\\)=325 is \\(z_1\\)=–1.5 .\nThe z-score (Equation 3.20) for \\(x_2\\)=366 is \\(z_2\\)=–1.14 .\nStudent 2 scored closer to the mean than Student 1 and, since they both had negative z -scores, Student 2 had the better score."
  },
  {
    "objectID": "estimation.html",
    "href": "estimation.html",
    "title": "4  Estimation and Confidence Intervals",
    "section": "",
    "text": "In the statistical sense a population is a theoretical concept used to describe an entire group of individuals (not necessarily people) that share a set of characteristics. Examples are the population of all patients with diabetes mellitus, all people with depression, or the population of all middle-aged women.\nResearchers are specifically interested in quantities such as population mean and population variance of random variables (characteristics) of such populations. These quantities are unknown in general. We refer to these unknown quantities as parameters. Here, we use parameters \\(μ\\) and \\(σ^2\\) to denote the unknown population mean and variance respectively. For example, researchers want to know what the mean depression score for the population would be if all people with depression were treated with a new anti-depression treatment.\nNote that for all the distributions we discussed in the previous chapter, the population mean and variance of a random variable are related to the unknown parameters of probability distribution assumed for that random variable. Indeed, for normal distributions \\(N(μ,σ^2)\\), which are widely used in statistics, the population mean and variance are exactly the same parameters used to specify the distribution.\n\n\n\n\n\n\nParametric Tests\n\n\n\nThe term parameter for a population characteristic explains why most of the statistical tests in this textbook are referred to as parametric tests. These tests are based on assumptions about the shape of the distribution and the parameters (i.e. mean and standard deviation), and most rely on the assumption of an approximately normal distribution.\n\n\n \n\nResearchers want to know the population parameter, the value that would be obtained if the entire population were actually studied. Of course, they don’t usualy have the resources and time to study, for example, every individual with depression in the world, so a population parameter value is generally not available. They must instead study a sample, a subset of the population that is intended to represent the population. In this case a sample statistic is used to estimate a population parameter (also named as point estimator since we estimate the parameter by a single value or point).\nIn most cases, the best way to get a sample that accurately represents the population is by taking a random sample from the population (Figure 4.1). When selecting a random sample, each individual in the population has the same chance of being selected for the sample.\n\n\n\n\nFigure 4.1: Parameters are referred to the population while statistics are referred to the sample.\n\n\n\n\n \n\nThe researchers use the sample statistic value as an estimate of the population parameter value. The researchers are making an inference that the sample statistic is a value similar to the population parameter value based on the premise that the characteristics of those in the sample are similar to the characteristics of those in the entire population. When researchers use a sample statistic to infer the value of a population parameter, it is called inferential statistics.\nThe process is represented schematically in Figure 4.2. So, a sample is selected from population of interest to provide an estimate of a population parameter by using a sample statistic. If the sampling method used is random sample then we obtain an unbiased estimate of the population parameter.\n\n\n\n\n\nflowchart TB  \n    A((population)) -- sampling method --> B((sample))\n    A -.-> C[Population parameter]\n    B --> D[\"point estimator <br> (a sample statistic)\"]\n    D --> C\n\n\n\n\n\n\nFigure 4.2: Taking a sample of the population and using the sample to estimate a population parameter\n\n\n\n\nIn some circumstances the sample may consist of all the members of a specifically defined population. For practical reasons, this is only likely to be the case if the population of interest is not too large. If all members of the population can be assessed, then the estimate of the parameter concerned is derived from information obtained on all members and so its value will be the population parameter itself. The dotted arrow in Figure 4.2 connecting the population circle to population parameter box illustrates this. However, this situation will rarely be the case so, in practice, we take a sample which is often much smaller in size than the population under study.\n\n\n\n\n\n\nEstimation of population parametres\n\n\n\nPoint Estimation refers to the process of guessing the unknown value of a parameter (e.g., population mean, \\(\\mu\\)) using the observed data. For this, we use a sample statistic as a point estimator of the population parameter. Statistics do not depend on any unknown parameter, and given the observed data, we should be able to find their values. For example, the sample mean and sample variance are statistics.\n\n\nSome common population parameters (Greek Letters) and their corresponding sample statistics (or estimates) are described in Table 4.1.\n\n\nTable 4.1: Population parameters (Greek letters) and sample statistics\n\n\nPopulation parameter\nSample statistic\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\ns\n\n\nProportion\n\\(\\pi\\)\np\n\n\nRate\n\\(\\lambda\\)\nr"
  },
  {
    "objectID": "estimation.html#sample-distribution-vs-sampling-distribution",
    "href": "estimation.html#sample-distribution-vs-sampling-distribution",
    "title": "4  Estimation and Confidence Intervals",
    "section": "\n4.2 Sample Distribution vs Sampling Distribution",
    "text": "4.2 Sample Distribution vs Sampling Distribution\nSample Distribution\nThe sample distribution is simply the data distribution of the sample which is randomly taken from the population. We can calculate a sample statistic such as the sample mean from the data in the sample.\n\n\n\n\nFigure 4.3: Sample distribution of data from one sample.\n\n\n\n\n \nSampling Distribution\nThe sampling distribution is the distribution of the sample statistic (e.g., the sample mean) over many samples drawn from the same population (i.e., repeated sampling).\n\n\n\n\nFigure 4.4: Sampling distribution of a statistic (e.g. sample mean) from many samples.\n\n\n\n\n\n\n\n\n\n\nSampling distribution\n\n\n\nThe sampling distribution is the theoretical distribution of possible values for a sample statistic."
  },
  {
    "objectID": "estimation.html#what-is-standard-error-se-of-the-mean",
    "href": "estimation.html#what-is-standard-error-se-of-the-mean",
    "title": "4  Estimation and Confidence Intervals",
    "section": "\n4.3 What is Standard Error (SE) of the mean?",
    "text": "4.3 What is Standard Error (SE) of the mean?\nThe standard deviation of the sampling distribution is known as the standard error (SE). There are multiple formulas for standard error depending of exactly what is our sampling distribution. The standard error of a mean is the population σ divided by the square root of the sample size n:\n\\[ SE = \\frac{\\sigma}{\\sqrt{n}} \\tag{4.1}\\]\nHowever, because we usually do not have access to the population parameter σ, we instead use the sample standard deviation sd, as it is an estimate of the population standard deviation.\n\\[ SE = \\frac{sd}{\\sqrt{n}}\\]\nThe Standard Error (SE) of the mean is a metric that describes the variability of sample means in the sampling distribution. SE gives us an indication of the uncertainty attached to the estimate of the population mean when taking only a sample - very uncertain when the sample size is small."
  },
  {
    "objectID": "estimation.html#central-limit-theorem-clm",
    "href": "estimation.html#central-limit-theorem-clm",
    "title": "4  Estimation and Confidence Intervals",
    "section": "\n4.4 Central Limit Theorem (CLM)",
    "text": "4.4 Central Limit Theorem (CLM)\nThe Central Limit Theorem (CLM) in statistics states that, given a sufficiently large sample size, the sampling distribution of the mean for a variable will approximate a normal distribution regardless of that variable’s distribution in the population.\nA hypothetical population\nThe importance of central limit theorem is that it works no matter the underlying distribution of the population data. The underlying population data could be noisy and central limit theorem will still hold. To illustrate this, let’s draw some data (100,000 observations):\n\n\n\n\n\n\n\nFigure 4.5: A hypothetical population of 100,000 observations.\n\n\n\n\nHere are some descriptive statistics to show how ugly these data are:\n\n\n   vars     n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 1e+05 40.16 40.31     24   37.71 35.58   0 100   100 0.39    -1.56 0.13\n\n\nIf we knew nothing else from the data other than the descriptive statistics above, we would likely guess the data would look anything other than “normal” no matter how many different values there are. There is a clear bimodality problem in these data. Namely, that “average” (i.e. the mean) doesn’t look “average” at all.\nThe data, we have just created above, will serve as the entire population (N=100,000) of data from which we can sample.\nThe Sampling Distribution of the means\nNow, what if we get 50,000 samples, each sample consisting of just 10 observations, save the means of those samples, and draw their histogram?\n\n\n\nThe distribution of sample means (as a density plot) converges on a normal distribution where the provided location and scale parameters are from 50,000 sample means. Further, the center of the distribution is converging on the known population mean. The true population mean 40.16 (red dashed line) is very close to the mean of the 50,000 sample means.\n\n\n\n\nFigure 4.6: The distribution of 50,000 sample means, each of size=10.\n\n\n\n\n \n\n\n\n\n\n\nProperties of the sampling distribution of the mean\n\n\n\n\nThe mean of the sampling distribution is the same as the mean of the population.\nThe standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases.\nThe shape of the sampling distribution becomes normal as the sample size increases regardless of the population distribution of the variable."
  },
  {
    "objectID": "estimation.html#the-confidence-intervals",
    "href": "estimation.html#the-confidence-intervals",
    "title": "4  Estimation and Confidence Intervals",
    "section": "\n4.5 The confidence intervals",
    "text": "4.5 The confidence intervals\nWe will base the definition of confidence interval on two ideas:\n\nOur point estimate (e.g., mean from the sample) is the most plausible value of the actual parameter, so it makes sense to build the confidence interval around the point estimate.\nThe plausibility of a range of values can be defined from the sampling distribution of the estimate.\n\nFor the case of the mean, the Central Limit Theorem states that its sampling distribution is Normal. Additionally, recall that the standard deviation of the sampling distribution of the mean is the standard error (SE) of the mean.\nIn this case, and in order to define an interval, we can make use of a well-known result from probability that applies to normal distributions: 95% of the distribution of sample means lies within \\(\\pm 1.96\\) standard errors (the standard deviation of this distribution) of the population mean.\nIf the interval spreads out \\(\\pm 1.96\\) standard errors from a normally distributed point estimate, intuitively we can say that we are roughly 95% confident that we have captured the true but unknown population parameter.\nThe formula for the confidence interval (CI) of mean looks like this:\n\\[ 95\\%CI=\\bar{x} \\ \\pm 1.96 \\ SE_{\\bar{x}} = \\bar{x} \\ \\pm 1.96  \\frac{\\sigma}{\\sqrt{n}}  \\tag{4.2}\\]\nand if the population standard deviation σ is unknown, the the sample standard deviation, sd, is used in the formula Equation 4.2:\n\\[ 95\\%CI=\\bar{x} \\ \\pm 1.96 \\ SE_{\\bar{x}} = \\bar{x} \\ \\pm 1.96  \\frac{sd}{\\sqrt{n}}  \\tag{4.3}\\]\nThe real meaning of “confidence” is not evident and it must be understood from the point of view of the generating process. The confidence interval is based on the concept of repetition of the study under consideration. Thus, suppose we took many (infinite) samples from a population and built a 95% confidence interval from each sample. Then about 95% of those intervals would contain the population parameter.\nNow, we can present the confidence intervals of 100 randomly generated samples of size = 10 from our hypothetical population (Figure 4.7). Each horizontal bar is a confidence interval (CI), centered on a sample mean (point). The intervals all have the same length, but are centered on different sample means as a result of random sampling from the population. The five bold confidence intervals do not cover the true population mean (the vertical red dashed line \\(\\mu\\) = 40.16). This is what we would expect using a 95% confidence level– approximately 95% of the intervals covering the population mean.\n\n\n\n\n\n\n\nFigure 4.7: 100 Sample Means of Size 10 (with 95% Intervals) from the Population.\n\n\n\n\nNext, we will create the confidence intervals of 100 randomly generated samples of size = 50 from our population (Figure 4.8):\n\n\n\n\n\n\n\nFigure 4.8: 100 Sample Means of Size 50 (with 95% Intervals) from the Population.\n\n\n\n\nIncreasing the sample size not only converges the sample statistic (the points) on the population parameter (red dashed line) but decreases the uncertainty around the estimate (the CIs become narrower)."
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "5  Foundations for statistical inference",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUnderstand the hypothesis testing\nKnow how to apply the one sample z-test"
  },
  {
    "objectID": "inference.html#hypothesis-testsing",
    "href": "inference.html#hypothesis-testsing",
    "title": "5  Foundations for statistical inference",
    "section": "\n5.1 Hypothesis Testsing",
    "text": "5.1 Hypothesis Testsing\nHypothesis testing is a method of deciding whether the data are consistent with the null hypothesis. The calculation of the p-value is an important part of the procedure. Given a study with a single outcome measure and a statistical test, hypothesis testing can be summarized in five steps.\n\n\n\n\n\n\nSteps of Hypothesis Testsing\n\n\n\nStep 1: State the null hypothesis, \\(H_{0}\\), and alternative hypothesis, \\(H_{1}\\), based on the research question.\n\nNOTE: We decide a non-directional \\(H_{1}\\) (also known as two-sided hypothesis) whether we test for effects in both directions (most common), otherwise a directional (also known as one-sided) hypothesis.\n\nStep 2: Set the level of significance, α (usually 0.05).\nStep 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic using the data.\n\nNOTE: There are two types of statistical tests and they are described as parametric and non-parametric. The parametric tests (e.g., t-test, ANOVA), make certain assumptions about the distribution of the unknown parameter of interest and thus the test statistic is valid under these assumptions. For non-parametric tests (e.g., Mann-Whitney U test, Kruskal-Wallis test), there are no such assumptions. Most nonparametric tests use some way of ranking the measurements. Non-parametric tests are about 95% as powerful as parametric tests.\n\nStep 4: Decide whether or not the result is statistically significant.\n\nThe p-value is the probability of obtaining the observed results, or something more extreme, if the null hypothesis is true.\n\nUsing the known distribution of the test statistic and according to the result of our statistical test, we calculate the corresponding p-value. Then we compare the p-value to the significance level α:\n\nIf p − value < α, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ α, do not reject the null hypothesis, \\(H_{0}\\).\n\nThe Table 5.1 demonstrates how to interpret the strength of the evidence. However, always keep in mind the size of the study being considered.\n\n\nTable 5.1: Strength of the evidence against \\(H_{0}\\).\n\np-value\nInterpretation\n\n\n\n\\(p \\geq{0.10}\\)\nNo evidence to reject \\(H_{0}\\)\n\n\n\n\\(0.05\\leq p < 0.10\\)\nWeak evidence to reject \\(H_{0}\\)\n\n\n\n\\(0.01\\leq p < 0.05\\)\nEvidence to reject \\(H_{0}\\)\n\n\n\n\\(0.001\\leq p < 0.01\\)\nStrong evidence to reject \\(H_{0}\\)\n\n\n\n\\(p < 0.001\\)\nVery strong evidence to reject \\(H_{0}\\)\n\n\n\n\n\nStep 5: Interpret the results.\n\n\nLet us discuss this procedure with an example.\n\n\n\n\n\n\nExample\n\n\n\nSuppose the mean heart rate in healthy adults is 68 beats per min with standard deviation 8 beats per min (bpm). Research was conducted to examine the pulse rate in patients with hyperthyroidism. Twenty patients were randomly enrolled with a mean of 82 and a standard deviation 11 bpm. Assuming that the heart rate follows a normal distribution, is the mean heart rate in hyperthyroidism patients different from that in healthy adults?\n\n\nThis type of question can be formulated in a hypothesis testing framework by specifying two hypotheses: a null and an alternative hypothesis.\n1. State the null and alternative hypotheses\nThe null hypothesis, denoted by \\(H_{0}\\), is the hypothesis that is to be tested. It is a statement indicating that there is no difference or association between conditions, groups, or variables.\nThe alternative hypothesis, denoted by \\(H_{1}\\), is the hypothesis that, in some sense, contradicts the null hypothesis. The \\(H_{1}\\) hypothesis (also called a research hypothesis) is the statement that supports a difference or association between conditions, groups, or variables.\nIn our example, the null hypothesis \\(H_{0}\\) is that the mean heart rate in hyperthyroidism patients μ is the same as the mean heart rate in healthy adults \\(μ_{0}\\). Here \\(μ_{0}\\) is known and μ is unknown; the alternative hypothesis \\(H_{1}\\) is that the mean heart rate in hyperthyroidism patients μ is not the same as the mean heart rate in healthy adults \\(μ_{0}\\). Note that \\(H_{1}\\) may have multiple options (more than, less than, or not the same as).\n\n\n\\(H_{0}\\) : μ=\\(μ_{0}\\), \\(H_{1}\\) : μ>\\(μ_{0}\\) (one-sided hypothesis; right-tailed)\n\n\\(H_{0}\\) : μ=\\(μ_{0}\\), \\(H_{1}\\) : μ<\\(μ_{0}\\) (one-sided hypothesis; left-tailed)\n\n\\(H_{0}\\) : μ=\\(μ_{0}\\), \\(H_{1}\\) : μ \\(\\neq\\) \\(μ_{0}\\) (two-sided hypothesis)\n\nIn our example, we state the two-sided hypothesis, i.e., assume that the mean heart rate in hyperthyroidism patients may deviate from 68 in either direction. That is:\n\n\n\\(H_{0}\\) : μ=68, \\(H_{1}\\) : μ \\(\\neq\\) 68\n\nThus, we have generated two mutually exclusive and all-inclusive possibilities. Either \\(H_{0}\\) or \\(H_{1}\\) will be true, but not both. So is our decision when we compare the probabilities of obtaining the sample data under each of these hypotheses. In practice, the testing procedure usually starts with \\(H_{0}\\), and when it is completed, the hypothesis testing will suggest either the rejection or non-rejection of \\(H_{0}\\), indirectly also determining \\(H_{1}\\).\nStep 2: Set the level of significance, α\nAfter stating the hypotheses, a significance level, denoted as α, is specified. It is defined as the probability of making a mistake — rejecting \\(H_{0}\\) when \\(H_{0}\\) is true (i.e., Type I error). The selection of α is arbitrary although in practice, values of 0.01, 0.05, or 0.10 are commonly used. Because the significance level α reflects the probability of rejecting a true \\(H_{0}\\), therefore this probability is deliberately selected.\nIn our example, we set the value α=0.05 for the level of significance (type I error).\nStep 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic using the data.\nThe one-sample z-test is used when we want to know whether the difference between the sample mean and the mean of a population is large enough to be statistically significant, that is, if it is unlikely to have occurred by chance. The test is considered robust for violations of normal distribution and it is usually applied to relatively large samples (N > 30) or when the population variance is known.\nMain assumption: The test statistic follows the Normal distribution.\nCalculation of the test statistic: \\[z = \\frac{\\bar{x} - \\mu_{o}}{SE_{z}}= \\frac{\\bar{x} - \\mu_{o}}{\\sigma/ \\sqrt{n}}=\\frac{82 - 68}{8/ \\sqrt{20}}=\\frac{14}{8/ 4.472}=7.826\\]\nStep 4: Decide whether or not the result is statistically significant.\nWe can calculate the p-value using a statistical program such as Jamovi. In our example, the p-value is <0.001 (so less than α=0.05) and \\(H_{0}\\) is rejected. We can also estimate the 95%CI of mean as following:\n\\[ 95\\% \\ CI= \\bar{x} \\ \\pm 1.96  \\frac{\\sigma}{\\sqrt{n}}= 82 \\ \\pm 1.96  \\frac{8}{\\sqrt{20}}= 82 \\ \\pm \\frac{15.68}{4.472} = 82 \\ \\pm 3.506 = [78.49, 85.51]\\]\nNote that, in this case, the value of null hypothesis, 68, is not included in the range of values of the 95% CI.\nStep 5: Interpret the results.\nThe mean heart rate (82 bpm) in hyperthyroidism patients is significantly higher than the heart rate in healthy adults (68 bpm)."
  },
  {
    "objectID": "inference.html#type-of-errors-and-power-of-the-test",
    "href": "inference.html#type-of-errors-and-power-of-the-test",
    "title": "5  Foundations for statistical inference",
    "section": "\n5.2 Type of Errors and Power of the test",
    "text": "5.2 Type of Errors and Power of the test\nA. Types of error in hypothesis testing\nType I error: we reject the null hypothesis when it is true (false positive), and conclude that there is an effect when, in reality, there is none. The maximum chance (probability) of making a Type I error is denoted by α (alpha). This is the significance level of the test; we reject the null hypothesis if our p-value is less than the significance level, i.e. if p < a (Figure 5.1).\nType II error: we do not reject the null hypothesis when it is false (false negative), and conclude that there is no effect when one really exists. The chance of making a Type II error is denoted by β (beta); its compliment, (1 - β), is the power of the test. The power, therefore, is the probability of rejecting the null hypothesis when it is false; i.e. it is the chance (usually expressed as a percentage) of detecting, as statistically significant, a real treatment effect of a given size (Figure 5.1).\n\n\n\n\nFigure 5.1: Types of error in hypothesis testing.\n\n\n\n\n\n\n\n\n\n\nFactors Influencing Power\n\n\n\nA. Effect Size: as effect size increases, power tends to increase\nIf we have too little power in a study, then we will not be able to detect clinically interesting differences or associations. If we have too much power in a study, then tiny, arbitrary differences or associations could be detected as statistically significant, and that is not in the best interest of science. So we want enough power for our test statistics to be significant when they encounter the smallest differences or weakest associations that have reached the threshold of being clinically noteworthy. The judgment about what is clinically noteworthy is not a statistical issue; it depends on the expertise of researchers within the applied area of study.\nFor example, a difference in means is an example of an effect size. How is effect size related to power? Let’s think about the effect of a low-dose aspirin on pain, compared with the effect of a prescription pain medication. Suppose a person is suffering from back pain. Which pill do you think will have a bigger effect on the person’s pain? We would hope that the prescription pain medication would be more effective than aspirin, resulting in a bigger reduction in pain than aspirin would provide. Which pill’s effect would be easier to detect? It should be easier to “see” the bigger effect, which came from the prescription pain medication. This is true in statistics, too. A larger effect size is easier for the test statistics to “see,” leading to a greater probability of a statistically significant result. In other words, as effect size increases, power tends to increase. If researchers are interested in detecting a small effect size because it is clinically important, they will need more power.\nB. Sample Size: as the sample size goes up, power generally goes up.\nThe factor that is most easily changed by researchers is sample size, and it has a huge effect on power. As \\(n\\) goes up, power generally goes up.\nThe most common question that researchers bring to a statistician is, “How many participants should I have in my study?” Sometimes researchers talk about calculating power, but in fact researchers calculate the sample size that will give them the amount of power that they want.\nC. Standard deviation: as variability decreases, power tends to increase\nA way to increase the statistical power would be to make the population standard deviation, σ, smaller, and that is not so easy to do.Generally speaking, variability can be reduced by controlling extraneous variables.\nFor example, in the glucosamine study, Wilkens et al. (2010) described the inclusion and exclusion criteria for participants. Among the inclusion criteria, the researchers accepted patients if they were 25 years or older, had nonspecific chronic back pain below the 12th rib for at least 6 months, and had a certain score or higher on a measure of self-reported low back pain. Participants were excluded if, among other things, they had certain diagnoses (pregnancy, spinal fracture, etc.) and prior use of glucosamine.\nIf the researchers let any adult into the study, there would be much more extraneous variability. For example, if people with spinal fractures were admitted to the study, their experience of back pain probably would differ considerably from similar people who did not have spinal fractures. The inclusion and exclusion criteria defining the sample would reduce some of this extraneous variability, allowing the researchers a better chance of detecting any effect of glucosamine. Thus, the researchers would have a better chance of finding statistical significance, meaning that the power would be greater.\nD. Significance Level α: as α goes up, power goes up.\nIt would be easier to find statistical significance with a larger significance level (e.g. α=0.1), compared with finding a significant result with a smaller α (e.g. α=0.05). All else being the same, as α goes up, power goes up. And as α goes down, like from 0.05 to 0.01, power goes down."
  },
  {
    "objectID": "inference.html#choosing-a-significance-level",
    "href": "inference.html#choosing-a-significance-level",
    "title": "5  Foundations for statistical inference",
    "section": "\n5.3 Choosing a significance level",
    "text": "5.3 Choosing a significance level\nReducing the error probability of one type of error increases the chance of making the other type. As a result, the significance level, α, is often adjusted based on the consequences of any decisions that might follow from the result of a significance test.\nBy convention, most scientific studies use a significance level of α = 0.05; small enough such that the chance of a Type I error is relatively rare (occurring on average 5 out of 100 times), but also large enough to prevent the null hypothesis from almost never being rejected. If a Type I error is especially dangerous or costly, a smaller value of α is chosen (e.g., 0.01). Under this scenario, very strong evidence against \\(H_{0}\\) is required in order to reject \\(H_{0}\\).\nConversely, if a Type II error is relatively dangerous, then a larger value of α is chosen (e.g., 0.10). Hypothesis tests with larger values of α will reject \\(H_{0}\\) more often. For example, in the early stages of assessing a drug therapy, it may be important to continue further testing even if there is not very strong initial evidence for a beneficial effect. If the scientists conducting the research know that any initial positive results will eventually be more rigorously tested in a larger study, they might choose to use α = 0.10 to reduce the chances of making a Type II error: prematurely ending research on what might turn out to be a promising drug.\nA government agency responsible for approving drugs to be marketed to the general population, however, would like to minimize the chances of making a Type I error— approving a drug that turns out to be unsafe or ineffective. As a result, they might conduct tests at significance level 0.01 in order to reduce the chances of concluding that a drug works when it is in fact ineffective.\n\n\n\n\nWilkens, Philip, Inger B. Scheel, Oliver Grundnes, Christian Hellum, and Kjersti Storheim. 2010. “Effect of Glucosamine on Pain-Related Disability in Patients With Chronic Low Back Pain and Degenerative Lumbar Osteoarthritis.” JAMA 304 (1): 45. https://doi.org/10.1001/jama.2010.893."
  },
  {
    "objectID": "two_samples.html",
    "href": "two_samples.html",
    "title": "6  Inference for numerical data: 2 samples",
    "section": "",
    "text": "Two samples may classified either as independent or dependent (paired) groups of observations (Figure 6.1).\n\n\n\nFigure 6.1: Depiction of independent Vs dependent (paired) samples\n\n\nIndependent samples\nTwo samples are independent (unrelated) if the measurements of one group are not related to or somehow paired or matched with the measurements of the other group.\nFor example, one group of participants is randomly assigned to treatment group, while a second and separate group of participants is randomly assigned to placebo group (randomized controlled trial). These two groups are independent because the individuals in the treatment group are in no way paired or matched with corresponding members in the placebo group .\nDependent samples\nTwo samples are dependent (paired or matched) if the measurements of one group are related to or somehow paired or matched with the measurements of the other group.\nFor example, two measurements that are taken at two different times from the same individuals (before-after design) are related."
  },
  {
    "objectID": "two_samples.html#two-sample-t-test-students-t-test",
    "href": "two_samples.html#two-sample-t-test-students-t-test",
    "title": "6  Inference for numerical data: 2 samples",
    "section": "6.2 Two-sample t-test (Student’s t-test)",
    "text": "6.2 Two-sample t-test (Student’s t-test)\nTwo sample t-test (Student’s t-test) can be used if we have two independent (unrelated) groups (e.g., males/females, treatment/non-treatment) and one quantitative variable of interest (e.g., age, weight, systolic blood pressure). For example, we may want to compare the age in males and females or the weights in two groups of children, each child being randomly allocated to receive either a dietary supplement or placebo.\n\n\n\n\n\n\nAssumptions for conducting a Student’s t-test\n\n\n\n\nThe groups are independent\nThe outcome of interest is continuous\nThe data is normally distributed in both groups\nThe data in both groups have equal variances (homogeneity of variance)\nThe observations are independent\n\n\n\n\n\n\n\n\n\nSteps of hypothesis testing for Student’s t-test\n\n\n\nStep 1: State the null hypothesis and alternative hypothesis\n\\(H_{0}\\): the population means in the two groups are equal (\\(μ_{1}=μ_{2}\\) or \\(μ_{1} - μ_{2} = 0\\)).\n\\(H_{1}\\): the population means in the two groups are not equal (\\(μ_{1} \\neq μ_{2}\\)).\nStep 2: Set the level of significance α = 0.05.\nStep 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic using the data.\nΤhe appropriate parametric statistical test, for testing \\(H_{0}\\), is the Student’s t-test. (NOTE: first check for normal distributions and homogeneity of variance).\nThe formula of the test is given by the t-statistic as follows:\n\\[t = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{SE_{\\bar{x}_{1} - \\bar{x}_{2}}} = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}} \\tag{6.1}\\] where \\(s_{p}\\) is an estimate of the pooled standard deviation of the two groups which is calculated by the following equation:\n\\[s_{p} = \\sqrt{\\frac{(n_{1}-1)s_{1}^2 + (n_{2}-1)s_{2}^2}{n_{1}+ n_{2}-2}} \\tag{6.2}\\]\nUnder the null hypothesis, the t-statistic follows the t-distribution with \\(n_{1}+ n_{2}-2\\) degrees of freedom (df).\nStep 4: Decide whether or not the result is statistically significant.\nBased on the calculated t-statistic (Equation 6.1), we have to decide whether to reject or fail to reject the \\(H_{0}\\). If the computed t-value falls in the rejection region (area of the two red tails), we reject \\(H_{0}\\).\n\n\n\nFigure 6.2: The two-tailed rejection region (area of the two red tails) equals to a=0.05\n\n\nIn practice, we use the p-value (as generated by Jamovi based on the value of the t-statistic Equation 6.1) to guide our decision:\n\nIf p − value < 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\nThe 95% confidence interval (CI) for the difference of the two means at significance level α=0.05, with df, and for a two-tailed t-test is given by:\n\\[ 95\\% \\ CI = \\bar{x}_{1} - \\bar{x}_{2} \\pm t_{df;0.05/2} \\cdot SE_{\\bar{x}_{1} - \\bar{x}_{2}} \\tag{6.3}\\]\n\nNote that, if the means are significantly different (reject \\(H_{0}\\)), the 95% CI of the difference in means will not include zero.\n\nStep 5: Interpretation of the results.\nReport the difference in means, the 95% CI, and the p-value of the test.\n\n\n\n\n\n\n\n\nNot equal variances-Welch’s t-test\n\n\n\nIf the assumption of equal variances is not satisfied, the t-statistic is given by the following expression (Welch’s t-test):\n\\[t = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{\\sqrt{\\frac{s_{1}^2}{n_{1}} + \\frac{s_{2}^2}{n_{2}}}} \\tag{6.4}\\]\nThe distribution of the test statistic is approximated as an ordinary t-distribution with the degrees of freedom df calculated using the Welch–Satterthwaite equation:\n\\[ df= \\frac{(s_{1}^2/n_{1} + s_{2}^2/n_{2})^2}{(s_{1}^2/n_{1})^2/(n_{1}-1)+(s_{2}^2/n_{2})^2/(n_{2}-1)} \\tag{6.5}\\]\n\n\n\n\n\n\n\n\nExample-Student’s t-test\n\n\n\nBlood pressure levels were measured in 100 diabetic and 100 non-diabetic men aged 40-49 years. Mean systolic blood pressure (sbp) was 146.4 mmHg with standard deviation 18.5 mmHg among the diabetics and 140.4 mmHg with standard deviation 16.8 mmHg among the non-diabetics. Supposed that the assumptions of Normality and constant variance are satisfied, perform a two-tailed two-sample t-test to compare the means in the two groups (α=0.05).\nStep 1: State the null hypothesis and alternative hypothesis\n\\(H_{0}\\): the population means of sbp in the two groups are equal (\\(μ_{1}=μ_{2}\\) or \\(μ_{1} - μ_{2} = 0\\)).\n\\(H_{1}\\): the population means of sbp in the two groups are not equal (\\(μ_{1} \\neq μ_{2}\\)).\nStep 2: We set the level of significance α = 0.05.\nStep 3: We calculate the test statistic.\nFirst, we calculate the pooled standard deviation of the two groups (Equation 6.2):\n\\[s_{p} = \\sqrt{\\frac{(n_{1}-1)s_{1}^2 + (n_{2}-1)s_{2}^2}{n_{1} + n_{2}-2}} = \\sqrt{\\frac{(100-1)18.5^2 + (100-1)16.8^2}{100 + 100-2}} \\approx 17.67\\]\nand then the test t-statistic from Equation 6.1:\n\\[t = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}} = \\frac{146.4 - 140.4}{17.67 \\cdot \\sqrt{\\frac{1}{100} + \\frac{1}{100}}} \\approx 2.4\\] The t-statistic follows the t-distribution with 198 degrees of freedom (df).\nUsing a statistical calculator for t-distribution (such as the distrACTION module from Jamovi), we can compute the probability \\(Pr(T \\geq 2.4)= 0.0087\\) (Figure 6.3). Then, the p-value for a two tailed test is 2*0.0087=0.0174 < 0.05 (reject \\(H_{0}\\)).\n\n\n\nFigure 6.3: We compute the probability \\(Pr(T \\geq 2.4)= 0.0087\\). The orange area is half of the p-value.\n\n\nThe 95% confidence interval (CI) of the difference at α=0.05, for the two-tailed t-test and for 198 degrees of freedom is (Equation 6.3):\n\\[ 95\\% \\ CI = \\bar{x}_{1} - \\bar{x}_{2} \\pm t_{198;0.05/2} \\cdot SE_{\\bar{x}_{1} - \\bar{x}_{2}}= 6 \\pm 1.972 \\cdot 2.5 \\approx [1.1, 10.9] \\ mmHg\\] Therefore, the 95% confidence interval for the difference in the two means ranges from 1.1 mmHg to 10.9 mmHg. Note that the zero is not included in the 95% CI of the difference in means.\nStep 5: Interpretation of the results.\nThe mean systolic blood pressure (146.4 mmHg, sd=18.5 mmHg) in diabetic men is significantly higher than the mean systolic blood pressure in non-diabetic men (140.6 mmHg, sd=16.8 mmHg) (MD=6 mmHg, 95% CI: 1.1 to 10.9, p=0.017).\n\n\n\n\n\n\n\n\nMann-Whitney U test\n\n\n\nWhen there is violation of normality, the Mann-Whitney U test can be used. This test compares two independent samples based on the ranks of the values and is often considered the non-parametric equivalent to the Student’s t-test."
  },
  {
    "objectID": "two_samples.html#paired-samples-t-test",
    "href": "two_samples.html#paired-samples-t-test",
    "title": "6  Inference for numerical data: 2 samples",
    "section": "6.3 Paired samples t-test",
    "text": "6.3 Paired samples t-test\nA paired t-test is used to assess whether the mean of the differences between the two related measurements, x and y, is significantly different from zero.\n\n\nTable 6.1: The differences between the two related measurements, x and y.\n\n\nx\ny\nd = x-y\n\n\n\n\n\\(x_{1}\\)\n\\(y_{1}\\)\n\\(d_{1}=x_{1}-y_{1}\\)\n\n\n\\(x_{2}\\)\n\\(y_{2}\\)\n\\(d_{2}=x_{2}-y_{2}\\)\n\n\n\\(x_{3}\\)\n\\(y_{3}\\)\n\\(d_{3}=x_{3}-y_{3}\\)\n\n\n.\n.\n.\n\n\n\\(x_{i}\\)\n\\(y_{i}\\)\n\\(d_{i}=x_{i}-y_{i}\\)\n\n\n.\n.\n.\n\n\n\\(x_{n}\\)\n\\(y_{n}\\)\n\\(d_{n}=x_{n}-y_{n}\\)\n\n\n\n\n\n\n\n\n\n\nAssumptions for conducting a paired t-test\n\n\n\n\nThe groups are dependent\nThe outcome of interest is continuous\nThe differences between the pairs of measurements, \\(d_{i}\\)s, are normally distributed\nThe \\(d_{i}\\)s are independent of each other\n\n\n\nBecause of the paired nature of the data, the two samples must be of the same size, \\(n\\). We have \\(n\\) differences \\(d\\), with sample mean \\(\\bar{d}\\) and standard deviation \\(s_{\\bar{d}}\\).\n\n\n\n\n\n\nSteps of hypothesis testing for paired t-test\n\n\n\nStep 1: State the null hypothesis and alternative hypothesis\n\\(H_{0}\\): the population mean change or difference is zero (\\(μ_{d}=0\\)).\n\\(H_{1}\\): the population mean change or difference is non-zero (\\(μ_{d} \\neq 0\\)).\nStep 2: Set the level of significance α = 0.05.\nStep 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic using the data.\nΤhe appropriate parametric statistical test, for testing \\(H_{0}\\), is the paired t-test. (NOTE: first check for normal distribution of the differences).\nThe formula of the test is given by the t-statistic as follows:\n\\[t = \\frac{\\bar{d}}{SE_{\\bar{d}}} = \\frac{\\bar{d}}{s_{\\bar{d}}/ \\sqrt{n}} \\tag{6.6}\\]\nwhere \\(s_{\\bar{d}}/ \\sqrt{n}\\) is the estimate of standard error and \\(n\\) is the number of pairs.\nUnder the null hypothesis, the t-statistic follows the t-distribution with \\(n-1\\) degrees of freedom (df).\nStep 4: Decide whether or not the result is statistically significant.\nBased on the calculated t-statistic (Equation 6.6), we have to decide whether to reject or fail to reject the \\(H_{0}\\). If the computed t-value falls in the rejection region (area of the two red tails), we reject \\(H_{0}\\).\n\n\n\nThe two-tailed rejection region (area of the two red tails) equals to a=0.05\n\n\nIn practice, we use the p-value (as generated by Jamovi based on the value of the t-statistic Equation 6.6) to guide our decision:\n\nIf p − value < 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ α, do not reject the null hypothesis, \\(H_{0}\\).\n\nThe 95% confidence interval (CI) for the differences at significance level α=0.05, with df, and for a two-tailed t-test is given by:\n\\[ 95\\% \\ CI = \\bar{d} \\pm t_{df;0.05/2} \\cdot SE_{\\bar{d}} \\tag{6.7}\\]\n\nNote that, if the mean of the differences is significantly different from zero (reject \\(H_{0}\\)), the 95% CI of the mean of the differences will not include zero.\n\nStep 5: Interpretation of the results.\nReport the mean of the differences, the 95% CI, and the p-value of the test.\n\n\n\n\n\n\n\n\nExample-Paired t-test\n\n\n\nSystolic blood pressure (sbp) levels were measured in 16 middle-aged men before and after a standard exercise. The mean change (after-before) in sbp following exercise was 6.6 mmHg (risen) and the standard deviation of the differences was 6.0 mmHg. Supposed that the assumption of Normality of the differences is satisfied, perform a two-tailed paired t-test to investigate if the mean change is significant (α=0.05).\nStep 1: State the null hypothesis and alternative hypothesis\n\\(H_{0}\\): the mean difference of sbp is zero (\\(μ_{d}=0\\)).\n\\(H_{1}\\): the mean difference of sbp is non-zero (\\(μ_{d} \\neq 0\\)).\nStep 2: We set the level of significance α = 0.05.\nStep 3: We calculate the test statistic.\nThe test t-statistic from Equation 6.6 is:\n\\[t = \\frac{\\bar{d}}{s_{\\bar{d}}/ \\sqrt{n}} = \\frac{6.6}{6/ \\sqrt{16}} = \\frac{6.6}{6/ 4} = 4.4\\]\nThe t-statistic follows the t-distribution with 15 degrees of freedom (df).\nUsing a statistical calculator for t-distribution (such as the distrACTION module from Jamovi), we can compute the probability \\(Pr(T \\geq 4.4)= 0.0003\\) (Figure 6.4). Then, the p-value for a two tailed test is 2*0.0003=0.0006 < 0.05 (reject \\(H_{0}\\)).\n\n\n\nFigure 6.4: We compute the probability \\(Pr(T \\geq 4.4)= 0.0003\\). The orange area is half of the p-value.\n\n\nThe 95% confidence interval (CI) of the mean difference at α=0.05, for the two-tailed t-test and for 15 degrees of freedom is (Equation 6.7):\n\\[ 95\\% \\ CI = \\bar{d} \\pm t_{15;0.05/2} \\cdot SE_{\\bar{d}}= 6.6 \\pm 2.131 \\cdot 1.5= 6.6 \\pm 3.197 \\approx [3.4, 9.8] \\ mmHg\\]\nTherefore, the 95% confidence interval for the mean difference ranges from 3.4 mmHg to 9.8 mmHg. Note that the zero is not included in the 95% CI of the mean difference.\nStep 5: Interpretation of the results.\nThe mean difference (after-before) in systolic blood pressure following exercise was 6.6 mmHg (sd=6.0 mmHg) which was a significant increase (p<0.001).\n\n\n\n\n\n\n\n\nWilcoxon Signed-Rank test\n\n\n\nWhen there is violation of normality in the distribution of the differences, Wilcoxon Signed-Rank test can be used. This test is based on the sign and the magnitude of the rank of the differences between pairs of measurements. It is often considered the non-parametric equivalent to the paired t-test."
  },
  {
    "objectID": "more_samples.html",
    "href": "more_samples.html",
    "title": "7  Inference for numerical data: >2 samples",
    "section": "",
    "text": "In this chapter, we introduce the use of one-way analysis of variance (ANOVA) for hypothesis testing of more than two sample means with normal distributions. ANOVA was first introduced by the statistician R.A. Fisher in 1921 and is now widely applied in the biomedical and other research fields.\n\n\n\n\n\n\nThe mathematical model\n\n\n\nSuppose we want to compare the means of \\(k\\) different populations (groups). Any individual score, \\(x_{ij}\\), can be written as follows:\n\\[x_{ij} = μ + α_j + ε_{ij} \\tag{7.1}\\]\nwhere,\n\\(j\\): the index over groups (j=1,2,3,…, k)\n\\(i\\): the index over the individuals (i=1,2,3,…, \\(n_{j}\\))\n\\(μ\\): expected mean of individuals in all groups\n\\(α_j\\): the effect of the \\(j^{th}\\) treatment\n\\(ε_{ij}\\): an error term (the residual of the model)\nAn alternative way to write the model in Equation 7.1 is:\n\\[x_{ij} = μ_j + ε_{ij} \\tag{7.2}\\]\nwhere \\(μ_j= μ + α_j\\) is the expected mean of the \\(j^{th}\\) group\n\n\n \nANOVA is a single global test to determine whether the means differ in any groups. In addition, the term “one-way” is used because the participants are separated into groups by one factor (e.g., intervention).\n\n\n\n\n\n\nAssumptions for conducting a one-way ANOVA test\n\n\n\n\nThe groups are independent\nThe outcome of interest is continuous\nThe data is normally distributed in all groups\nThe data in all groups have equal variances (homogeneity of variance)"
  },
  {
    "objectID": "more_samples.html#the-basic-idea-of-anova",
    "href": "more_samples.html#the-basic-idea-of-anova",
    "title": "7  Inference for numerical data: >2 samples",
    "section": "\n7.2 The basic idea of ANOVA",
    "text": "7.2 The basic idea of ANOVA\nANOVA separates the total variability in the data into that which can be attributed to differences between the individuals from different groups (between-group or explained variation), and to the random variation between the individuals within each group (within-group or unexplained variation). Then we can test whether the variability in the data comes mostly from the variability within each group or can truly be attributed to the variability between groups.\n\n\nFigure 7.1: We can separate the “total” variance into variance across groups (between groups) and in the same groups (within group).\n\n\nThese components of variation are measured using variances, hence the name analysis of variance (ANOVA). Under the null hypothesis “the group means are the same”, the between-group variance will be similar to the within group variance. If, however, there are differences between the groups, then the between-group variance will be larger than the within-group variance. The F-ratio, which is the test statistic for ANOVA, is obtained by dividing between-group variability by within-group variability.\nThe calculation of the F-ratio involves to calculate the Sum of Squares, SS, the degrees of freedom, df, and finally the variances (Mean Squares, MS). Let’s see each of them more analytically:\n\n\n\n\n\n\n Sum of Squares components\n\n\n\nTotal sum of squares can be partitioned into between sum of squares and within sum of squares:\n\\[SS_{T} = SS_{B} + SS_{W} \\tag{7.3}\\]\n\nTotal sum of squares (\\(SS_{T}\\)): The total sum of squares, denoted by \\(SS_{T}\\) , measures the total variation of all observations.\n\n\\[SS_{T} = \\sum_{j=1}^k \\sum_{i=1}^{n_{j}} ( x_{ij}-{\\bar{x_{g}})^2 } \\tag{7.4}\\]\nwhere,\nk: the number of groups (independent samples)\nj: the index over groups (j=1,2,3,…, k)\n\\(n_{j}\\): the sample size of the jth group\ni: the index over the individuals (i=1,2,3,…, \\(n_{j}\\))\n\\(x_{ij}\\): the score for an individual, i, within a particular group j\n\\(\\bar{x_{g}}\\): grand mean (mean for all groups combined)\n\nBetween-group sum of squares (\\(SS_{B}\\)): The between groups sum of squares, denoted by \\(SS_{B}\\), measures the variation between group means at various levels of the treatment, that is, the between-group variation.\n\n\\[SS_{B} = \\sum_{j=1}^k n_{j}( \\bar{x}_j-{\\bar{x_{g}})^2 } \\tag{7.5}\\]\nwhere \\(\\bar{x}_j\\): sample mean of group j.\n\nWithin-group or error sum of squares (\\(SS_{W}\\)): The within groups sum of squares, denoted by \\(SS_{W}\\), measures the variation of observations within treatment groups, that is, the within-group variation. This reflects the random error of an experiment not related to the treatment factor.\n\n\\[SS_{W} = \\sum_{j=1}^k \\sum_{i=1}^{n_{j}} ( x_{ij}-{\\bar{x_{j}})^2 } \\tag{7.6}\\]\n\n\n\n\n\n\n\n\n Degrees of freedom\n\n\n\nEach sum of square (\\(SS_{T}\\), \\(SS_{B}\\), \\(SS_{W}\\)) has different degrees of freedom associated with it. The degrees of freedom for the total variance is \\(df_{T}=n-1\\) (where n is the total number of observations in the study, which means that \\(n = n_{1} + n_{2} + . . . + n_{k}\\)), the degrees of freedom for the between-groups variance is \\(df_{B}=k-1\\), the degrees of freedom for the within-groups variance is \\(df_{w}=n-k\\). Obviously:\n\\[df_{T}=df_{B}+df_{W} \\tag{7.7}\\]\n\n\n\n\n\n\n\n\n Means of Squares\n\n\n\nThe estimate of the between-groups variance is given by dividing the \\(SS_{B}\\) by the \\(df_{B}\\):\n\\[MS_{B}= \\frac{SS_{B}}{df_{B}} = \\frac{SS_{B}}{k-1} \\tag{7.8}\\]\nand for within-groups variance:\n\\[MS_{W}= \\frac{SS_{W}}{df_{W}} = \\frac{SS_{W}}{n-k} \\tag{7.9}\\]"
  },
  {
    "objectID": "more_samples.html#hypothesis-testing-of-anova",
    "href": "more_samples.html#hypothesis-testing-of-anova",
    "title": "7  Inference for numerical data: >2 samples",
    "section": "\n7.3 Hypothesis testing of ANOVA",
    "text": "7.3 Hypothesis testing of ANOVA\n\n\n\n\n\n\nSteps of hypothesis testing for one-way ANOVA test\n\n\n\nStep 1: State the null hypothesis and alternative hypothesis\n\\(H_{0}\\): all group means are equal (\\(μ_{1}=μ_{2}=...=μ_{k}\\)).\n\\(H_{1}\\): at least one group mean differs from the others (\\(μ_{i} \\neq μ_{j}, \\ i,j = 1, 2, 3, ...\\))\nStep 2: Set the level of significance α = 0.05.\nStep 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic using the data.\nΤhe appropriate parametric statistical test, for testing \\(H_{0}\\), is the ANOVA test. (NOTE: first check for normal distributions and homogeneity of variance). The F-statistic is calculated by creating a ratio of the between-groups variance (Equation 7.8) to the within-groups variance (Equation 7.9):\n\\[F = \\frac{MS_{B}}{MS_{W}} \\tag{7.10}\\]\nThis statistic follows an F-distribution with a pair of degrees of freedom \\(df_{B} =k−1\\) (numerator), \\(df_{W} =n−k\\) (denominator).\nStep 4: Decide whether or not the result is statistically significant.\nBased on the calculated F-statistic (Equation 7.10), we have to decide whether to reject or fail to reject the \\(H_{0}\\). If the computed F-value falls in the rejection region (area of the two red tails), we reject \\(H_{0}\\).\n\n\nFigure 7.2: Illustration of rejection and non-rejection regions for the F-test for one-way ANOVA.\n\n\nAn interesting point about the F-ratio is that because it is the ratio of between variance to within variance, if its value is less than 1 then it must, by definition, represent a non-significant effect.\nIn practice, we use the p-value (as generated by Jamovi based on the value of the F-statistic) to guide our decision:\n\nIf p − value < 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ α, do not reject the null hypothesis, \\(H_{0}\\).\n\nStep 5: Interpretation of the results. If we reject \\(H_{0}\\) in ANOVA, there is at least one mean that differs from the others. It is often of interest to further examine which of the means are significantly different and which are not. However, it is invalid to employ multiple two independent samples t-test to examine the difference of means for each pair of groups because this will inevitably lead to a rapid increase in the probability of the familywise Type I error rate (FWER). In this case, we can employ a post-hoc procedure (e.g. Tukey test, Games-Howell tets) to compare three or more groups while controlling the probability of making at least one Type I error.\n\n\n\n\n\n\n\n\nThe familywise error rate (FWER)\n\n\n\n\nThe familywise Type I error rate (represented by the notation \\(α_{FW}\\)) is the likelihood that there will be at least one Type I error in a set of c comparisons.\nThe per comparison Type I error rate (represented by the notation \\(α_{PC}\\)) is the likelihood that any single comparison will result in a Type I error.\n\nThe following equation defines the relationship between the familywise Type I error rate and the per comparison Type I error rate, where c the number of comparisons:\n\\[ α_{FW} = 1-(1-α_{PC})^c \\tag{7.11}\\]\nFor an example, when we are interested in comparing means of A, B and C groups, we may consider performing a set of three comparisons as following:\nHypothesis 1: mean values of group A and group B are equal (comparison of A and B).\nHypothesis 2: mean values of group A and group C are equal (comparison of A and C).\nHypothesis 3: mean values of group B and group C are equal (comparison of B and C).\nThe three aforementioned comparisons can be conceptualized as a family/set of comparisons, with c=3. If for each of the comparisons the value αPC equals to 0.05, the familywise Type I error rate becomes (Equation 7.11):\n\\[ α_{FW} = 1-(1-0.05)^3= 1-0.95^3=1-0.857= 0.143\\]\nThis result indicates that the likelihood of committing at least one Type I error in the set of three comparisons is 0.143 instead of 0.05 (almost three times greater).\n\n\n\n\n\n\n\n\nNot equal variances-Welch’s ANOVA test\n\n\n\nIf the assumption of equal variances is not satisfied, we use the Welch’s ANOVA test.\n\n\n\n\n\n\n\n\nKruskal-Wallis test\n\n\n\nWhen there is violation of normality, the Kruskal-Wallis test can be used. This test compares multiple independent samples based on the ranks of the values and is often considered the non-parametric equivalent to the ANOVA test."
  },
  {
    "objectID": "categorical.html",
    "href": "categorical.html",
    "title": "8  Inference for categorical data",
    "section": "",
    "text": "Different data types require different analytical approaches. In the previous chapters we discussed a number of ways to analyze and extract conclusions from continuous data [give some examples here]. Here we look at ways to perform analyses involving categorical data. “Categorical” are data where each observation implies membership to some group. For example, categorical data can be string of character values such as ‘M’ or ‘F’, dictating whether a person’s sex is male or female, or they can be numbers, such as 1, 2 or 3, symbolizing smoking status (where 1 might represent current smokers, 2, previous smokers and 3 people who never smoked). In medical research, categorical data frequently take on the form of membership in a response category (e.g., 1 for responders and 0 for non-responders), or exposure to some risk factor (e.g., 1 for exposed and 0 for not exposed), the occurrence of disease and so on.\nWhen recording categorical data it is good practice to use numbers representing each category or group and then assigning a label to each number. The reason for this is that, unlike character values, numbers are, barring data entry error, virtually impossible to misspell. For this reason, all software packages include routines that assign labels to numbers which represent membership in some group. Here is an example, where we create a variable called smoking, which contains codes of the smoking status of 10 individuals:\nCoded data: 1, 2, 3, 3, 2, 1, 1, 2, 3, 3\nwhere 1= Current, 2= Previous, and 3= Never.\n\n\n\n(Labeled data: Current, Previous, Never, Never, Previous, Current, Current, Previous, Never, Never)\nIt is crucial to understand that, while using numbers to represent membership in some category, the numbers are labels and not actual numerical data. In other words, non-smokers do not have three times and previous smokers two times the value of current smokers! On the other hand, numbers are useful, both because of accuracy but also as labels of group membership when the data are ordinal. Ordinal are categorical data where there is an inherent ordering in the groups. In the example of smoking status, one may consider that previous smokers have lower exposure to smoking than current smokers and non-smokers even less exposure than previous smokers. So the sequence \\(1 \\rightarrow 2 \\rightarrow 3\\) implies a decreasing cumulative exposure to smoking. This may have significant implications later on, if we want to assess the risk of a population in terms of experiencing cardiovascular disease for example, based on cumulative exposure to smoking. In fact, finding out that relationships follow some predictable ordering provides stronger evidence that the association of a categorical factor and some outcome is causal, that is the risk factor is the cause of the outcome and not related to the outcome in some random manner. For example, it is more definitive to establish that previous smokers have intermediate risk for lung cancer, between current smokers and non-smokers, than simply establishing that the risk in these three groups is different."
  },
  {
    "objectID": "categorical.html#statistical-modeling-of-categorical-data",
    "href": "categorical.html#statistical-modeling-of-categorical-data",
    "title": "8  Inference for categorical data",
    "section": "\n8.2 Statistical modeling of categorical data",
    "text": "8.2 Statistical modeling of categorical data\nThe most common ways that categorical data are analyzed is through the binomial distribution. Recall from (Chapter 3), the discussion about the binomial distribution. There we said that the probability that an outcome of interest occurs \\(X=x\\) out of \\(n\\) times is \\[\nP(X=x)=\\left (\\begin{array}{c} n\\\\x\\end{array}\\right )p^x(1-p)^{n-x}\n\\] where \\(p\\) is the probability of observing a single such event. An example of this is modeling response to a medication. If \\(n=20\\) people take this medication, the probability of observing \\(X=8\\) with favorable response is, assuming the probability of each person responding is \\(p=30\\%\\) \\[\nP(X=8)=\\left (\\begin{array}{c} 20\\\\8\\end{array}\\right )0.3^8(1-0.3)^{20-8}=0.114\n\\] In other words, if we carry out a large number of similar clinical studies, we would expect to see 8/20 responders in only 11.4% of them. The reason is that, if the underlying response rate is \\(p=0.3\\) we would expect to see, on average, \\(np=20\\times 0.3=6\\) responders. So, to see two more responders than expected on average has lower probability. Similarly, the probability that we would see \\(x=10\\) responders is \\(P(X=10)=0.031\\), which is even lower than seeing \\(x=8\\) responders. The same is true for the case where the number of responders is below the mean. For example, the probability of seeing \\(x=2\\) responders is \\(P(X=2)=0.028\\). Plotting these probabilities we get the following trends:\n\n\n\n\nFigure 8.1: Probability of response among 20 patients receiving a medication.\n\n\n\n\nSo, as we get away from the mean (shown in purple in the figure) in either direction, the probability of observing either too large or too small a number of responders decreases.\nEstimating \\(p\\)\n\nIn many cases we do not know what \\(p\\) is. For example, when we evaluate the response to a new drug, we do not know what the probability of a favorable response will be for someone taking the experimental medication. All we will be able to observe is the number of people with some given characteristic (e.g., a cancer diagnosis) who took the medication and the number among them that experienced a favorable response. In this case, we need to turn the problem on its head. Instead of asking what is the probability of seeing \\(x\\) responders out of \\(n\\) patients, if the response rate is \\(p\\), we ask what is the probability that \\(p\\) is such and such, given that I have observed \\(x\\) responders out of \\(n\\) people who received the drug.\nAs we saw above, the probability of seeing a number of responders that is much higher or much lower than the average response \\(np\\) is low. Conversely the probability that the population response is much higher or lower than the observed response in our study, i.e., \\(\\hat p=\\frac{x}{n}=\\frac{8}{20}=0.40\\), will also be low. We can get an idea of these probabilities by performing a simulation. We will repeatedly generate virtual clinical trials like ours, with \\(n=20\\) patients receiving a drug with and \\(x=8\\) of them responding. In each of these simulations, I will consider response rates between 5% and 95% in increments of 5%. The proportion of time that \\(x=8\\) responders were observed in the simulated trials when the population response rate is \\(p=5\\%, 10\\%, 20\\%, \\cdots\\) will be the empirical probability distribution of \\(p\\). Here is this distribution:\n\n\n\n\n\n\n\nFigure 8.2: Empirical distribution of \\(p\\) from a study of \\(x=8\\) responders among \\(n=20\\) patients.\n\n\n\n\nIn the previous figure, we list the empirical (i.e., observed through simulation) distribution of \\(p\\), the population response inferred from \\(x=8\\) responders out of \\(n=20\\) study participants. The highest probability generated from these simulations corresponds to the observed proportion of responders \\(\\hat p=0.40\\). Empirical probabilities diminish as one moves farther from \\(\\hat p\\) in either direction.\nThe normal approximation to the binomial distribution\nA simple (and, back when computers weren’t as powerful as they are today, necessary) approximation method of estimating \\(p\\) is through the Central Limit Theorem. Without going too far into the weeds of the theory, the end result is that the quantity \\[\nZ=\\frac{\\hat p-p}{\\sqrt{p(1-p)/n}}\\sim N(0,1)\n\\] In other words, if we subtract from the observed response rate its (unknown) mean and divide it by its standard deviation, the resulting quantity will be distributed according to the normal distribution. Since \\(p\\) is unknown, \\(Z\\) can only be calculated if we make some assumption about the value of \\(p\\), the unknown population proportion. In the following figure, we consider several values for \\(p\\) ranging from \\(p=0.05,\\cdots, 0.95\\), so we can see what the distribution of \\(p\\) based on the normal approximation to the binomial might look like. We also plot the distribution resulting from the previous simulation for comparison.\n\n\n\n\nFigure 8.3: Exact (red) distribution of the response rate p and distribution based on the normal approximation (green).\n\n\n\n\nThe agreement between the exact and approximate results is very good. In general, if \\(p\\) is not too small or too large, and \\(n\\) the number of individuals under study is not too small, the normal approximation will be sufficient. A rule of thumb addressing both of these requirements is that the mean of the binomial distribution \\(np>5\\) and that \\(n(1-p)>5\\). Thus, if \\(p\\) is close to zero or one, the sample size must be higher to counteract the loss of precision in the normal approximation, but smaller sample sizes can be considered for values of \\(p\\) away from the edges of the zero-one continuum.\nConfidence intervals for \\(p\\)\n\nOne very important use of the normal approximation is the construction of approximate confidence intervals for \\(p\\), something that is difficult to calculate exactly. As we saw previously in [put chapter reference here], a 95% two-sided confidence interval based on the normal distribution is bounded by \\(\\pm 2\\) standard deviations on either side of the mean. This is the case here as well, where the mean is the population mean and the standard deviation is \\(\\sigma=\\sqrt{p(1-p)/n}\\). So, an approximate 95% confidence interval for \\(p\\) is \\[\n\\left (\\hat p-1.96\\times \\sqrt{p(1-p)/n}), \\hat p+1.96\\times \\sqrt{p(1-p)/n)}\\right )\n\\] where \\(\\hat p=\\frac{x}{n}\\) is the observed proportion. As \\(p\\) in the population is not known, the standard deviation is estimated by plugging in \\(\\hat p\\) for \\(p\\) in the above equation. In the example above, \\(\\hat p=\\frac{8}{20}=0.40\\) and thus, the 95% confidence interval is \\[\\left ( 0.40-1.96\\sqrt{(0.4)(0.6)/20}, 0.40+1.96\\sqrt{(0.4)(0.6)/20}  \\right )\\] \\[\n\\Downarrow\n\\] \\[(0.185, 0.615)\\] The interpretation of this confidence interval is that, based on our observed \\(x=8\\) responders out of \\(n=20\\) individuals studied, we are 95% confident that the unknown true response rate is between \\(p_l=18.5\\%\\) and \\(p_u=61.5\\%\\).\nTests of hypotheses involving \\(p\\)\n\nWe can use the ideas presented in the last section to perform hypothesis testing involving \\(p\\). In medical statistics, such tests take on the form of assessing whether the observed data are consistent with an a priori hypothesis of what the response rate might be. This is very useful in clinical research for example, because we can use these methods to ascertain whether the data generated by a study are consistent with the response rate of a new drug exceeding some baseline level \\(p_0\\). Construction of such tests follows our usual process of forming a working or “null” hypothesis where the response rate of a new drug \\(p\\) does not exceed \\(p_0\\) and hope our data produce evidence to the contrary (what we call the alternative hypothesis). For example, if the historical response rate with current medications is 20%, our working or null hypothesis is to assume that the response rate \\(p\\) of the experimental drug under study is no higher than baseline, i.e., \\(H_0: p\\leq p_0=0.20\\). Then, using the data collected during the clinical trial, we can estimate \\(p\\), the response of the new drug, and see whether it is much higher than one would expect if the true response rate were \\(p_0\\).\nAs an example, let’s say that we undertake a clinical trial with \\(n=20\\) participants and observe \\(x=8\\) responses. One way to decide whether \\(p\\leq p_0\\) can be based on the lower bound of a 95% one-sided confidence interval for \\(p\\) generated by the observed data. We will discuss how this interval is constructed in a second. To see why this is a viable approach, recall that confidence intervals contain all the plausible scenarios about the response rate (for the given confidence level of course). Thus, if the lower bound of these plausible values is higher than \\(p_0\\), then all values of \\(p\\) one can envision are higher than \\(p_0\\) as well. If our desired confidence level were, say, 95% and we observed this, we would be 95% “confident” that the response rate is \\(p>p_0\\). In that case we would conclude that the new drug improves upon the current baseline response rate.\nTo construct such a confidence interval we can use either the exact binomial distribution or the normal approximation. In the first case, the lower bound of the one-sided 95% confidence interval for \\(p\\), the response rate of the experimental drug, based on observed data of \\(x=8\\) responses out of \\(n=20\\) study participants,\nAn alternative way to go about this is by using the normal approximation to the binomial. We saw in [cite chapter here] that the lower bound of a one-sided 95% confidence interval, based on the normal distribution, is 1.645 standard deviations below the mean. If the mean in our case is estimated by \\(\\hat p=\\frac{8}{20}=0.40\\), and the standar deviation by \\(s=\\sqrt{\\hat p(1-\\hat p)/n}=0.11\\), then the lower bound of the one-sided 95% confidence interval is \\(p_l=0.22\\). This very close to the exact bound shown above.\nThe conclusion from both types of confidence intervals is that the smallest value for the response rate of the experimental drug that anyone could envision with 95% confidence is around \\(22\\%-23\\%\\). This is higher than the baseline rate \\(p_0=20\\%\\). In turn, this means that, based on the observed number of \\(x=8\\) responses in our study out of \\(n=20\\) participants, the unknown population response rate for the drug under study is higher than \\(p_0=20\\%\\)."
  },
  {
    "objectID": "categorical.html#comparing-two-proportions-p_1-and-p_2",
    "href": "categorical.html#comparing-two-proportions-p_1-and-p_2",
    "title": "8  Inference for categorical data",
    "section": "\n8.3 Comparing two proportions \\(p_1\\) and \\(p_2\\)\n",
    "text": "8.3 Comparing two proportions \\(p_1\\) and \\(p_2\\)\n\nComparing a proportion, like the rate of response to a medication, to a historical or external baseline rate is an efficient way to estimate it without involving large samples. However, the level of evidence obtained from such an analysis is not as high as comparing two populations head to head. In clinical research for example, data at the source of the baseline estimate of the response rate were not collected under circumstances which are identical to our current study, they were collected in different time periods, on different populations and so on. The more definitive approach is to compare two concurrently sampled populations, one taking the current standard therapy and one receiving the experimental therapy. The same is true when we assess the risk of disease based on some exposure. We can compare the observed frequency of the occurrence of the disease to a historical rate. However, more definitive conclusions can be obtained when we compare some exposure between two populations and compare the frequency of occurrence of some outcome between them (e.g., occurrence of melanoma to populations at different exposure levels to sunlight).\nEstimating the difference \\(p_1-p_2\\)\n\nThe ability to estimate the difference of two proportions is important for a number of reasons. It provides an idea of whether \\(p_1\\) and \\(p_2\\) are equal. In clinical investigation, this addresses the question whether the response to a new drug is higher compared to the standard of care. In epidemiology, these proportions may reflect whether the risk of occurrence of some disease is different among individuals who were or were not exposed to some risk factor.\nAs the true proportions in the populations under study are not known, we will need to estimate them from sampled data. The usual strategy can be applied, where we sample \\(n_1\\) people in the first group (people receiving a specific medication or being exposed to some factor) and \\(n_2\\) in the second group and count the number of times \\(x_1\\) and \\(x_2\\), that the characteristic of interest (response to a medication or presence of disease) has occurred in each group respectively. From these two samples we can obtain our best (“point”) estimates for \\(p_1\\) and \\(p_2\\) as \\(\\hat p_1=\\frac{x_1}{n_1}\\) and \\(\\hat p_2=\\frac{x_2}{n_2}\\). Consequently, our best estimate for the difference \\(p_1-p_2\\) will be \\(\\hat p_1-\\hat p_2\\). None of this is too complicated. Where matters start getting interesting is when we want to estimate the distribution of the difference of the two proportions. Deriving a distribution is a crucial task because it gives us an idea of what differences are more likely to occur and which are less likely. In its more immediate use, obtaining this distribution allows us to construct confidence intervals for the difference in the two proportions. It can also help us construct a criterion based on which we will decide whether the two proportions are equal or not.\nThe normal approximation to the distribution of \\(\\hat p_1-\\hat p_2\\)\n\nThe simplest way to estimate the distribution of the difference of two proportions is by using the normal approximation to the binomial distribution. Statistical theory, which we will not go into here, prescribes that, assuming that \\(n_1\\) and \\(n_2\\) are not too small and \\(p_1\\) and \\(p_2\\) too small or two large, the distribution of \\(\\hat p_1-\\hat p_2\\) is close to a normal distribution with mean \\(p_1-p_2\\) and standard deviation \\(\\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n_2}}\\).\nFor example, consider the trial of belotecan versus topotecan in recurrent ovarian cancer (Kim et al., Br J Cancer, 2021). In that study, \\(n_1=71\\) women diagnosed with recurrent ovarian cancer received belotecan 0.5 mg/m2 and \\(n_2=69\\) topotecan 1.5 mg/m2 for five consecutive days every 3 weeks. The study based its conclusion on the difference in overall response rate between the two treatment groups. At the completion of the study, \\(x_1=21\\) subjects in the belotecan arm had a partial or complete response (so \\(\\hat p_1=29.6\\%\\)), compared to \\(x_2=18\\) in the topotecan arm (\\(\\hat p_1=26.1\\%\\)). The normal approximation to the distribution of the difference is shown in the next figure.\n\n\n\n\nFigure 8.4: Normal approximation to the distribution of the difference in overall response rates in the ovarian cancer study plus observed overall response rate difference in the two treatment arms (dashed line).\n\n\n\n\nConfidence intervals for \\(p_1-p_2\\)\n\nConfidence intervals for the difference of two proportions can be calculated as usual by appealing to properties of the normal distribution. For example, a 95% two-sided confidence interval in the previous example will be \\[\n\\small\n\\left ( \\hat p_1-\\hat p_2-1.96\\times \\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n-2}},\n\\hat p_1-\\hat p_2+1.96\\times \\sqrt{\\frac{p_1(1-p_1)}{n_1}+\\frac{p_2(1-p_2)}{n-2}}\\right )\n\\] In the above, \\(p_1\\) and \\(p_2\\) are not known, so we substitute \\(\\hat p_1\\) and \\(\\hat p_2\\) for them respectively in order to calculate the confidence interval. Theory tells us that this will work if \\(n_1\\) and \\(n_2\\) are not too small. In the ovarian cancer study example, the 95% two-sided confidence interval of the difference in overall response rates between belotecan and topotecan is \\[\n(-0.113,\n0.183)\n\\] The interpretation of the above confidence interval is that we expect that 95% of the time, the unknown difference in the two response rates will be between \\(11.3\\%\\) in favor of topotecan (implying that the topotecan response rate is larger than belotecan’s) and \\(18.3\\%\\) in favor of belotecan (implying that the response rate of belotecan is larger than topotecan’s).\nTesting hypotheses involving \\(p_1\\) and \\(p_2\\)\n\nFollowing our usual strategy for performing tests of statistical hypotheses, our working or null hypothesis is that there is no difference between the response rates corresponding to the two groups, i.e., \\(H_0: p_1=p_2\\) or, equivalently, \\(H_0:p_1-p_2=0\\). An immediate way to decide whether the null hypothesis is true is through the confidence interval of \\(p_1-p_2\\) constructed in the previous section. Recall that confidence intervals include all the plausible values which are anticipated based on some pre-specified confidence level. So, one way to check whether the null hypothesis is correct, is by looking to see whether the value of the difference under the null hypothesis is contained within the confidence interval. If the answer is “yes”, then the evidence from the data is consistent with the null hypothesis, so we do not reject \\(H_0\\). Conversely, if this value is not included in the confidence interval, then it is not among the plausible values implied by our working hypothesis, so we reject it. Here note that the difference between \\(p_1\\) and \\(p_2\\) does not have to be zero under \\(H_0\\). In a number of situations (most notable among them are non-inferiority study designs), the difference may be some nonzero number. All of the methods discussed here can also be applied in those cases as well.\nWorking through this logic in the previous example, we see that \\(p_1-p_2=0\\) is included in the 95% two-sided confidence interval, since its bounds straddle zero. This is evidence consistent with the hypothesis that there is no difference in the overall response rate between the two drugs. A pictorial representation of what we just said follows.\n\n\n\n\nFigure 8.5: Two-sided 95% confidence interval of the difference in overall response rates in the ovarian cancer study versus its value under \\(H_0\\) (dashed line).\n\n\n\n\nWe can also carry out this test based on a single p value associated with the following statistic \\[\nZ=\\frac{\\hat p_1-\\hat p_2}{\\sqrt{\\hat p(1-\\hat p) \\left ( \\frac{1}{n_1}+\\frac{1}{n_2}\\right )}}\\sim N(0,1)\n\\] where \\(\\hat p=\\frac{x_1+x_2}{n_1+n_2}\\) is a pooled estimate of the common response rate \\(p\\). Note that, if \\(p_1=p_2\\), as it is assumed under the null hypothesis, the observed response rates in the two groups are simply random manifestations of the true single population response rate. So our best estimate is to use the entire data set to estimate the common rate. We do so by obtaining the single response rate resulting from pooling the observed responses \\(x_1+x_2\\)and dividing this sum by the total sample size \\(n_1+n_2\\) in the two groups.\nThe p value is calculated as the probability of seeing the observed \\(Z\\) value or any other value that is more extreme (i.e., to the right of a \\(Z\\) and/or to the left of a negative \\(Z\\) value) if our working hypothesis is true. If this probability is low (i.e., below our a priori specified alpha level) then we reject the null hypothesis, as low p values suggest that the evidence provided by the observed data are not consistent with the working hypothesis. In this case, we reject \\(H_0:p_1=p_2\\) and conclude that \\(p_1\\neq p_2\\). Conversely, if the p value is high, then the probability that we see a difference like the one observed in this study is high, so the evidence produced by the data is consistent with the null hypothesis. In this case we do not reject the null hypothesis. The “and/or” in the definition of the p value above comes into play when the null hypothesis is two-sided. In this case, both the areas under the curve to the right of \\(|Z|\\) and the left of \\(-|Z|\\) are summed up. In the ovarian cancer example, our null hypothesis is two-sided (i.e., it does not imply direction of the superiority or inferiority in the difference) so the p value is calculated as shown in the colored areas of the next figure.\n\n\n\n\nFigure 8.6: Calculation of a p-value associated with the difference in overall response rates in the ovarian cancer study. Both the areas under the curve of the shaded regions are summed to produce the two-sided p value.\n\n\n\n\nIn the above figure, summing up the two shaded regions to the right of \\(Z\\) and to the left of \\(-Z\\) gives us a p value \\(p=0.645\\). Since the p value is not below \\(\\alpha=0.05\\), the data are consistent with the working hypothesis of no difference between the response rates in the belotecan and topotecan arms. We conclude that there is no evidence that the overall response rates are different in the two drugs."
  },
  {
    "objectID": "categorical.html#contingency-tables",
    "href": "categorical.html#contingency-tables",
    "title": "8  Inference for categorical data",
    "section": "\n8.4 Contingency tables",
    "text": "8.4 Contingency tables\nA very useful way of presenting data that express the frequency of the occurrence of some event, is through cross-tabulations. For example, response to treatment, among women participating in the ovarian cancer clinical trial described in the previous section, can be presented as a \\(2\\times 2\\) cross-tabulation of those who received belotecan or topotecan versus those who experienced a partial or complete response of their tumors and those who did not.\n\n\n\n\n\n\n\nTable 8.1:  Response frequency by treatment arm in the reccurrent ovarian cancer study. \n\n\n\nTreatment group\n\n\n\nResponse\nBelotecan \nTopotecan\nTotal\n\n\n\n\nYes\n21\n18\n39\n\n\nNo\n50\n51\n101\n\n\nTotal\n71\n69\n140\n\n\n\n\n\n\n\nContingency tables are also used when we summarize data which are related to the occurrence of disease separated by whether an individual had been exposed to some risk factor or not. Here is an example of data from the Electricity Generating Authority of Thailand (EGAT) cohort (Vathesatogkit et al., Int J Epi, 2012). This study (also see Epidemiology: Study Design and Data Analysis, Third Edition by Mark Woodward, Chapman & Hall/CRC Texts in Statistical Science, 2014, page 91) followed a cohort of employees of the Electricity Generating Authority of Thailand. Study participants filled out a questionnaire at entry into the study where, among other information, they were asked about whether they smoked. Twelve years later, data on deaths from cardiovascular disease (CVD) were available on 3,315 individuals (over 95% of the original study cohort). Here is a summary of these data.\n\n\n\n\n\n\n\nTable 8.2:  CVD by smoking status in the EGAT study. \n\n\n\nSmoker?\n\n\n\nDeath from CVD?\nYes \nNo\nTotal\n\n\n\n\nYes\n31\n15\n46\n\n\nNo\n1,386\n1,883\n3,269\n\n\nTotal\n1,417\n1,898\n3,315\n\n\n\n\n\n\n\nIn both of the above examples, the scientific question is about whether exposure to a certain medication (belotecan or topotecan in the ovarian cancer trial) or a risk factor (smoking in the EGAT cohort study) is associated with the probability that a person will experience a certain event. In the case of the clinical trial, the event of interest is overall response to treatment. In the EGAT observational cohort study it is the risk of dying with evidence of cardiovascular disease. While the interpretation of the two outcomes is different (probability of response to treatment versus risk of disease occurrence), conclusions on the potential association of exposure with the outcome is based on the proportion of individuals who experienced the event of interest (response or death from CVD) out of all persons under study. Consequently, the same methods can be applied to address both of these questions.\nThe chi-square test of association\nLet’s go back to the ovarian cancer study. The investigators were keen on finding out whether response rates to belotecan were different from those of topotecan. We discussed previously how this question might be answered by comparing the response rates in the two treatment arms. Another way, which will turn out to produce equivalent results, is to address whether taking a specific medication is associated to the overall rate of response. Seen from the perspective of an individual patient, this is the probability that they respond to medication. How might this associational question be answered? Well let’s see what are the implications of a working hypothesis of no association. Consider the table from the ovarian cancer data, with the internal counts removed:\n\n\n\n\nFigure 8.7: TEXT TEXT.\n\n\n\n\nAs we discussed earlier, in the absence of any association between drug exposure and response, the best estimate of the probability of response is \\(\\hat p=\\frac{x_1+x_2}{n_1+n_2}=\\frac{21+18}{71+69}=0.279\\). Given properties of proportions described in chapter [cite chapter here], when discussing the binomial distribution, we would expect that, on average, \\(n_1\\hat p=19.8\\) individuals will respond in the belotecan arm and \\(n_2\\hat p=19.2\\) subjects will respond in the topotecan arm. Conversely, \\(n_1(1-\\hat p)=51.2\\) and \\(n_2(1-\\hat p)=49.8\\) study participants will be expected to not respond in the two arms respectively. Consequently, under the assumption of no association between treatment and response, Table 1 above should have looked like this:\n\n\n\n\nFigure 8.8: TEXT TEXT.\n\n\n\n\nSo slightly less people would be expected to have responded in the belotecan arm, 19.8 versus 21, and slightly more people would be expected to have responded in the topotecan arm, 19.2 versus 18, if there were no association between which treatment was taken and response to the drug. In addition, slightly more people, 51.2 versus 50 would be expected not to respond in the belotecan arm and slightly less, 49.8 versus 51, in the topotecan arm.\nThe question which will help us reach a conclusion is whether the observed average counts are too different from what would be expected, if there were no association between treatment and response, beyond what would be attributable to chance alone. To test this formally, we use a statistic which aggregates the squared differences between the counts that would be expected and those that are actually observed as proportions of the expected counts. This is, \\[\nX=\\sum_{i=1}^{r\\times c}\\frac{(O_i-E_i)^2}{E_i}\n\\] where \\(O_i\\) and \\(E_i\\) are, respectively, the observed and expected counts in each of the \\(r\\times c\\) interior cells of the contingency table. In our example, the number of rows and columns in the table, \\(r=2\\) and \\(c=2\\), but this is not always the case. This methodology will work for other combinations of \\(r\\) and \\(c\\) as well. Taking the squared differences underlines our lack of preference about whether the deviation from what is expected from the null hypothesis is in one direction or the other, so squared differences are useful as they correspond to our understanding of the problem.\nHowever, most importantly, the sum of the squared differences between observed counts and expected counts, has a known distribution. The distribution of the statistic, generated by aggregating the square differences of observed and expected counts is the chi-square distribution, indexed by degrees of freedom which are equal to \\((c-1)\\times (r-1)\\). Having a known distribution is important, because we can quantify how likely it is to observe the test statistic that is generated by adding together these square counts if there is no association between the two factors in the table (here treatment and response). If the observed statistic is unlikely to have occurred, we will reject the null hypothesis in favor of the alternative that there is a significant association between the factors under consideration. We can also generate p values using this statistic. These correspond to the area under the chi-square curve to the right of the observed statistic.\nLet’s implement the chi-square test in the ovarian cancer example. The chi-square statistic is \\[X=\\frac{(21 -19.8)^2}{19.8}+ \\frac{(18-19.2)^2}{19.2} + \\frac{(50-51.2)^2}{51.2} + \\frac{(51 -49.8)^2}{49.8}= 0.212\\] The p value associated with this test statistic, compared to a chi-square distribution with \\(1=(2-1)\\times (2-1)\\) degrees of freedom, is \\(p=0.645\\). The pictorial representation of this test is shown in the following figure.\n\n\n\n\nFigure 8.9: The chi-square distribution and observed test statistic in the ovarian cancer trial.\n\n\n\n\nThe conclusion from this analysis is that there does not appear to be sufficient evidence in these data supporting an association between taking belotecan or topotecan and responding to the medication or not.\nThese results are consistent with our earlier conclusion that there is no difference between the response rates in the two treatment arms. As it turns out, the two tests are equivalent! Recall that our test statistic in the comparison of \\(p_1\\) versus \\(p_2\\), the response rates in the ovarian cancer study, was \\(Z=0.461\\). This is equal to \\(\\sqrt{X}=\\sqrt{0.212}=0.461\\)). That is, the \\(Z\\) statistic in the test of the difference between two proportions is equal to the square root of the chi-square statistic from the \\(2\\times 2\\) table testing the association between two factors. So, in the special case of \\(2\\times 2\\) contingency tables, the \\(Z\\) test of difference between \\(p_1\\) and \\(p_2\\) and the chi-square test of the association of two factors are equivalent as they address the same question from two different perspectives. This relationship is not generally true for chi-square tests involving arbitrary \\(r\\times c\\) contingency tables."
  },
  {
    "objectID": "categorical.html#exposure-and-disease-the-concept-of-risk",
    "href": "categorical.html#exposure-and-disease-the-concept-of-risk",
    "title": "8  Inference for categorical data",
    "section": "\n8.5 Exposure and disease: the concept of risk",
    "text": "8.5 Exposure and disease: the concept of risk\nIn epidemiology, a major concept that people study is that of risk. Risk is the probability that one would observe some (usually adverse) event. We talk about the risk of heart disease, or the risk of cancer for example. Risk can also be the probability of experiencing a positive event such as disease remission or response to a medication. This latter interpretation applies to the idea of risk as a proportion or probability rather than the negative meaning of the everyday use of the word.\n\n\n\nIn the EGAT cohort study, the investigators assessed the risk of cardiovascular disease (CVD). In the \\(2\\times 2\\) table summarizing the frequency of CVD-related deaths and smoking status, the risk of CVD among the smokers is defined as the frequency of persons dying with evidence of CVD out of all smokers. Similarly, the risk of CVD among the non-smokers is the proportion of CVD-related deaths among people who were not smoking at the start of the study. This means that the risk of CVD among smokers is \\(r_1=0.0219\\) and the same among non-smokers is \\(r_2=0.0079\\). The pictorial representation of this is as follows:\n\n\n\n\nFigure 8.10: Percent of CVD deaths among smokers and non-smokers in the EGAT cohort study.\n\n\n\n\nAt first glance, it appears that the risk of CVD among smokers is almost three-fold higher compared to non-smokers. However, is an absolute difference of 1.4% sufficient for us to conclude that smoking is associated with a higher CVD risk?\nTo understand whether smoking predisposes someone to a higher risk for CVD death, researchers may compare the risk between smokers and non-smokers (the exposure factor). Of course we can address this question by testing for a possible association between smoking and CVD. The chi-square test for this association is as follows:\n\n\n\n\n\n\n\nTable 8.3:  Chi-square test of the association between CVD and smoking status in the EGAT study. \n\nTest statistic\nObs. value\nDF\nP value\n\nN\n\n\nχ²\n11.57784\n1\n0.0006674236\nN\n3,315\n\n\n\n\n\n\nWe see here that the observed chi-square statistic in the EGAT study is \\(X=11.6\\) with an associated p value \\(p=0.000667<0.05=\\alpha\\). So we conclude that the evidence from the data is not consistent with the null hypothesis of no association between smoking and CVD. There appears to be strong evidence of an association between smoking death from cardiovascular disease.\nComparison of risks: the risk ratio\nThe previous analysis is addressing the question of association but does not provide direct information about the actual levels of risk and, more importantly, the difference in risk between the two exposure groups (i.e., smokers and non-smokers). We addressed this question with chi-square test of association in the previous chapter. This, as we discussed earlier, is equivalent with the test of the difference between two proportions. So the chi-square test addresses the question of whether the levels of CVD risks are the same among smokers and non-smokers. Another way to address the same question is to estimate the risk ratio. This is the ratio of the risk in the exposed group (smokers here) over the risk in the unexposed group (non-smokers). In the EGAT study, the risk ratio is \\[\nRR=\\frac{\\frac{31}{1417}}{\\frac{15}{1898}}=2.77\\]\nIn other words, the relative risk of CVD is almost three-fold higher compared to non-smokers. To see whether this apparent enormous increase in risk is statistically significant (i.e., it is larger than what would be expected by chance), we have to test this formally. To do so, we need to recast the null hypothesis that CVD risk is not different between smokers and non-smokers in terms of a ratio of the two risks. So the null hypothesis \\(H_0:r_1=r_2 \\equiv r_1-r_2=0\\), for the risk of CVD among smokers and non-smokers respectively, is expressed as \\(H_0: RR=\\frac{r_1}{r_2}=1\\). Here, \\(RR\\) is the relative risk or the CVD risk ratio between the two exposure groups.\nWe will reach a conclusion about whether the observed relative risk \\(RR=2.77>1\\), more than what would be expected by chance, by constructing a 95% confidence interval. If this confidence interval does not contain \\(RR=1\\), then we will reject \\(H_0\\) in favor of the alternative \\(RR\\neq 1\\). Let’s consult the relevant output in the \\(2\\times 2\\) contingency table above (see Table 2). The output generating the point estimate and 95% confidence interval of the relative risk of CVD between smokers and non-smokers in the EGAT study is as follows:\n\n\n\n\n\n\n\nTable 8.4:  Estimation of the relative CVD risk between smokers and non-smokers in the EGAT study. \n\n\n\n95% confidence interval\n\n\n\nPoint estimate\nLower bound\nUpper bound\n\n\n\nRelative risk\n2.768196\n1.500194\n5.107944\n\n\n\n\n\n\nSince the 95% confidence interval \\((1.5, 5.11)\\) does not include the value \\(RR=1\\), which is explicitly anticipated by the null hypothesis of no difference in CVD risk, we reject \\(H_0:RR=1\\). The conclusion is that the risk of CVD is different between smokers and non-smokers. A straightforward inspection of the point estimate \\(RR=2.77\\), suggests that the risk of CVD is significantly elevated among smokers compared to non-smokers."
  },
  {
    "objectID": "categorical.html#the-odds-and-odds-ratio",
    "href": "categorical.html#the-odds-and-odds-ratio",
    "title": "8  Inference for categorical data",
    "section": "\n8.6 The odds and odds ratio",
    "text": "8.6 The odds and odds ratio\nAnother way to go about understanding differences in risk is using the odds ratio. Before we get to that, let’s define the concept of odds. Odds is the ratio between the proportion of those who experienced an event over the proportion of those who did not among the exposed and similarly among the unexposed. In the EGAT study (Table 2), the odds of CVD among smokers is \\[odds_1=\\frac{\\frac{31}{1417}}{\\frac{1386}{1417}}=\\frac{31}{1386}=0.0224\\]\nSimilarly, the odds of CVD among non-smokers is \\[odds_2=\\frac{\\frac{15}{1898}}{\\frac{1883}{1898}}=\\frac{15}{1883}=0.00797\\]\nSimilarly to the concept of a relative risk, we have the idea of an odds ratio. This is the ratio of the odds among exposed and unexposed individuals, i.e.,\n\n\n\n\\[OR=\\frac{odds_1}{odds_2}=\\frac{31\\times 1883}{15\\times 1386}=2.81\\]\nGoing back to Table 2 we see that we can estimate the odds ratio as the ratio of the product of the counts in the main diagonal of the table (cells [1,2] and [2,3]) divided by the product of the counts on the secondary diagonal (cells [1,3] and [2,2]).\nCase-control studies\nFrom these calculations we understand that the odds is not a measure of risk. It’s simply a measure of the increase or decrease of the proportion of some characteristic of interest (e.g., a disease, response to therapy, etc.) between two groups. So, it would not be unreasonable to ask why we need the odds ratio at all, given that we have the relative risk to work with. So what’s the attraction here? Why has the odds ratio, as a measure of the relative risk persisted over time and is used widely to this day? There are three reasons for the persistence of the odds ratio as a measure of risk: case-control studies; case-control studies; case-control studies!\nLet’s recall the EGAT study. This study, despite having recruited almost 3,500 people and followed them for twelve years, based its conclusions about whether smoking is associated with increased risk of death from cardiovascular disease on a mere 46 CVD-related deaths! As a result, the lower bound of the 95% confidence interval of the relative risk was barely above 1. The conclusion that smoking is associated with an increased risk of CVD death was statistically significant only because of the dramatic increase in the risk associated with smoking. Any lower value of the risk ratio would have resulted in an unclear result. The same is true in most such studies. The design involves the selection of two or more cohorts of study participants who have some exposure (e.g., receive some medication to prevent the occurrence of an event or are exposed to some risk factor). As in the EGAT study, we then enroll these subjects in the study and then … we wait; and wait; and wait for the event of interest to occur in a sufficient number of study participants so that we can compare the two groups and extract a conclusion about whether taking a medication or being exposed to some risk factor alters the likelihood of experiencing the event of interest.\nAlternatively, we can go on some hospital database and select the number of events we are interested in (“cases”) and then select in some manner individuals who did not experience the event of interest (“controls”) who are otherwise as similar to the cases as possible. For example, consider the following study (Newton-Bishop et al., Eur J Cancer 2011), which explores the possible association of sun exposure with melanoma. There, 960 cases (people who had melanoma) were identified in Yorkshire and the Nothern Region of the river Tyne in the UK) and were matched to 513 individuals from the surrounding population who did not have melanoma. The \\(2\\times 2\\) table summarizing previous sun exposure (expressed as having experienced at least one sunburn after the age of 20) was available on 1,335 of the 1,473 individuals who participated in the study and is shown below (bottom of Table 2 in paper describing the melanoma study):\n\n\n\n\n\n\n\nTable 8.5:  Occurrence of melanoma by sun exposure. \n\n\n\nSun burn after 20?\n\n\n\nMelanoma?\nYes \nNo\nTotal\n\n\n\n\nYes\n297\n561\n858\n\n\nNo\n130\n347\n477\n\n\nTotal\n427\n908\n1,335\n\n\n\n\n\n\n\nFrom this table, we can calculate the relative risk as \\(RR=1.13\\) with an associated 95% confidence interval \\((1.04, 1.22)\\) and go on to conclude that there is some weak evidence of a statistically significant elevation of melanoma risk among those who had experienced a sunburn after their 20th year. But something is wrong.\nIf these data are to be believed, there is a devastating epidemic of melanoma in Yorkshire! After all, (858/1335) x 100= \\(64.3\\%\\) of the study population had experienced melanoma. Is it sun spots? Is it the famous English fair complexion in combination with summer vacations in sunnier environs? What is going on? What is happening is that we are in effect analyzing a retrospective study as if it were a prospective study. A retrospective study is one where the event has already occurred, and we look back for the reasons. A prospective study by contrast is one where we don’t know whether the event has happened at the start of the study and we look forward in time to see if it occurs. In the melanoma study, researchers identified almost 900 melanoma cases, but these cases did not occur among the \\(1335\\) subjects participating in the study. The denominator of these melanoma cases could be hundreds of thousands or even millions of people. There were 14.9 million people living in Northern England as of the 2011 census, the same period when this study took place. Consequently, the estimation of the relative risk is not accurate. By contrast, the odds ratio is \\(OR=1.41\\). This number is calculated as a function of proportions within the cases and the controls. So it is valid, despite the retrospective nature of the study design (because we don’t need to know the size of the population that our cases and controls came from as these numbers cancel in the calculation of the odds ratio). And this is why the odds ratio is so popular: it does not care whether the study is prospective or retrospective.\nHowever, can the odds ratio be interpreted as a relative risk? To answer that, let’s return to the EGAT study. This is a prospectively designed study from the perspective that we did not know at the start who would die from CVD. In that study, the relative risk was \\(RR=2.77\\). The odds ratio was \\(OR=2.81\\), which is very close to the relative risk. How come? This is the case because death from CVD is a fairly rare event. So the odds ratio \\[\nOR=\\frac{\\frac{31}{1417}\\times\n           \\frac{1883}{1898}}\n          {\\frac{1386}{1417}\\times\n           \\frac{15}{1898}} \\approx\n      \\frac{\\frac{31}{1417}}\n           {\\frac{15}{1898}}=\n           2.77=RR\\] The above approximation is possible because the proportions of those who did not experience CVD are almost 100% (i.e., \\(\\frac{1883}{1898}\\approx 1\\) and \\(\\frac{1386}{1417} \\approx 1\\)). So, for rare events, \\(OR \\approx RR\\) and the odds ratio can be interpreted as a relative risk!\nGiven that melanoma is fairly rare cancer, the odds ratio in the melanoma study will be close to the true relative risk in the population; a lot closer than the relative risk produced by the case-control study. So our point estimate of the odds ratio \\(OR=1.41\\), suggests that the elevation of melanoma risk associated with sun exposure is \\(41.3\\%\\) and not \\(12.6\\%\\) as would be suggested by the relative risk estimate."
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "9  Correlation",
    "section": "",
    "text": "When we have finished this chapter, we should be able to:"
  },
  {
    "objectID": "correlation.html#what-is-correlation",
    "href": "correlation.html#what-is-correlation",
    "title": "9  Correlation",
    "section": "\n9.1 What is correlation?",
    "text": "9.1 What is correlation?\nCorrelation is a statistical method used to assess a possible association between two numeric variables. There are several statistics that we can use to quantify correlation depending on the underlying relation of the data. In this chapter, we’ll learn about two correlation coefficients:\n\nPearson’s \\(r\\)\nSpearman’s \\(r_{s}\\)\n\nPearson’s coefficient measures linear correlation, while the Spearman’s coefficient compare the ranks of data and measures monotonic associations."
  },
  {
    "objectID": "correlation.html#linear-correlation-pearsons-r-coefficient",
    "href": "correlation.html#linear-correlation-pearsons-r-coefficient",
    "title": "9  Correlation",
    "section": "\n9.2 Linear correlation (Pearson’s \\(r\\) coefficient)",
    "text": "9.2 Linear correlation (Pearson’s \\(r\\) coefficient)\nGraphical display with a scatter plot\nThe most useful graph for displaying the association between two numeric variables is a scatter plot. Figure 9.1 shows the association between systolic blood pressure (sbp) and diastolic blood pressure (dpb) in 96 patients with carotid artery disease, aged 42-89, prior to surgery. (Note that sbp and dpb can be plotted on either axis).\n\n\n\n\n\n\nExample-Association between systolic and diastolic blood pressure\n\n\n\n\n\n\n\nFigure 9.1: Scatter plot of the association between systolic blood pressure (sbp) and diastolic blood pressure (dbp) in 96 patients with carotid artery disease, aged 42-89, prior to surgery.\n\n\n\n\n\n\nFrom the scatter plot, there appears to be a linear association between sbp and dbp, with higher values of dbp being associated with higher values of sbp. How can we summarize this association simply? We could calculate the Pearson’s correlation coefficient, \\(r\\), which is a measure of the linear association between two numeric variables. The Pearson’s correlation coefficient is based on the sum of products about the mean of the two variables, so we shall start by considering the properties of the sum of products.\n\n\n\n\nFigure 9.2: Scatter plot with axes through the mean point.\n\n\n\n\nFigure 9.2 shows the scatter diagram of Figure 9.1 with two blue new axes drawn through the mean point. The distances of the points from these axes represent the deviations from the mean.\nPositive product: In the top right section of Figure 9.2, the deviations from the mean of both variables, dbp and sbp, are positive. Hence, their products will be positive. In the bottom left section, the deviations from the mean of the two variables will both be negative. Again, their product will be positive.\nNegative product: In the top left section of Figure 9.2, the deviation of dbp from its mean will be negative, and the deviation of sbp from its mean will be positive. The product of these will be negative. In the bottom right section, the product will again be negative.\nSo in Figure 9.2 most of these products will be positive, and their sum will be positive. We say that there is a positive correlation between the two variables; as one increases so does the other. If one variable decreased as the other increased, we would have a scatter diagram where most of the points lay in the top left and bottom right sections. In this case the sum of the products would be negative and there would be a negative correlation between the variables. When the two variables are not related, we have a scatter diagram with roughly the same number of points in each of the sections. In this case, there are as many positive as negative products, and the sum is zero. There is zero correlation or no correlation. The variables are said to be uncorrelated.\n \nPearson’s \\({r}\\) correlation coefficient\nThe Pearson’s correlation coefficient, \\({r}\\), can be calculated for any dataset with two numeric variables. However, before we calculate the Pearson’s \\({r}\\) coefficient we should make sure that the following assumptions are met:\n\n\n\n\n\n\nAssumptions for Pearson’s \\(r\\) coefficient\n\n\n\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\n\nThere is a linear association between the two variables.\nFor a valid hypothesis testing and calculation of confidence intervals both variables should have an approximately normal distribution.\n\nAbsence of outliers in the data set.\n\n\n\n \n\n\n\n\n\n\nCharacteristics of Pearson’s correlation coefficient \\(r\\)\n\n\n\nFormula\nGiven a set of \\({n}\\) pairs of observations \\((x_{1},y_{1}),\\ldots ,(x_{n},y_{n})\\) with means \\(\\bar{x}\\) and \\(\\bar{y}\\) respectively, \\(r\\) is defined as:\n\\[r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n(y_i - \\bar{y})^2}} \\tag{9.1}\\]\nThe \\(r\\) statistic shows the direction and measures the strength of the linear association between the variables.\nRange of values\nCorrelation coefficient is a dimensionless quantity that takes a value in the range -1 to +1.\n \nDirection of the association\nA negative correlation coefficient indicates that one variable decreases in value as the other variable increases (and vice versa), a zero value indicates that no association exists between the two variables, and a positive coefficient indicates that both variables increase (or decrease) in value together.\n\n\n\n\nFigure 9.3: The direction of association can be (a) negative, (b) no association, or (c) positive.\n\n\n\n\n \nMagnitude of the association\nThe magnitude of association can be anywhere between -1 and +1. The stronger the correlation, the closer the correlation coefficient comes to ±1 (Figure 9.4). A correlation coefficient of -1 or +1 indicates a perfect negative or positive association, respectively (Figure 9.4 c and f).\n\n\n\n\nFigure 9.4: The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\n\n\n\n \nInterpretation of the association\nThe Table 9.1 demonstrates how to interpret the strength of an association.\n\n\nTable 9.1: Interpretation of the values of the sample estimate of the correlation coefficient\n\nValue of r\nStrength of association\n\n\n\n\\(|r| \\geq{0.8}\\)\nvery strong association\n\n\n\\(0.6\\leq|r| < 0.8\\)\nstrong association\n\n\n\\(0.4\\leq|r| < 0.6\\)\nmoderate association\n\n\n\\(0.2\\leq|r| < 0.4\\)\nweak association\n\n\n\\(|r| < 0.2\\)\nvery weak association\n\n\n\n\n \nAnscombe’s Quartet\nAnscombe’s quartet comprises four datasets that have nearly identical simple descriptive statistics, yet have very different distributions and appear very different when graphed. Each dataset consists of eleven (x,y) points. They were constructed in 1973 by the statistician Francis Anscombe to demonstrate both the importance of graphing data when analyzing it, and the effect of outliers and other influential observations on statistical properties.\n\n\n\n\nFigure 9.5: Anscombe’s quartet. All datasets have a Pearson’s correlation of r = 0.82.\n\n\n\n\nThough all datasets have a Pearson’s correlation of \\(r = 0.82\\), when plotted the four datasets look very different. Graph I is a standard linear association where a Pearson’s correlation would be suitable. Graph II would appear to be a non-linear association and a non-parametric analysis would be appropriate. Graph III again shows a linear association (approaching r = 1) where an outlier has lowered the correlation coefficient. Graph IV shows no association between the two variables (X, Y) but an oultier has inflated the correlation higher.\n\n\n \nUsing the data in Figure 9.1 and the Equation 9.1 we find the Pearson’s correlation coefficient r=0.62. However, correlation does not mean causation.\n\n\n\n\n\n\nCorrelation is not causation\n\n\n\nAny observed association is not necessarily assumed to be a causal one- it may be caused by other factors. Correlation indicated only association, but it does not indicate cause-effect association.\nAs an example, suppose we observe that people who daily drink more than four cups of coffee have a decreased chance of developing skin cancer. This does not necessarily mean that coffee confers resistance to cancer; one alternative explanation would be that people who drink a lot of coffee work indoors for long hours and thus have little exposure to the sun, a known risk. If this is the case, then the number of hours spent outdoors is a confounding variable—a cause common to both observations. In such a situation, a direct causal link cannot be inferred; the association merely suggests a hypothesis, such as a common cause, but does not offer proof. In addition, when many variables in complex systems are studied, spurious associations can arise. Thus, association does not imply causation (Altman and Krzywinski 2015).\n\n\n \nHypothesis Testing for Pearson’s \\(r\\) correlation coefficient\n\n\n\n\n\n\nStep 1: Determine the appropriate null hypothesis and alternative hypothesis\n\n\n\n\nThe null hypothesis, \\(H_{0}\\), states that the population correlation, ρ, is zero (\\(ρ = 0\\)). There is not association between dbp and spb.\nThe alternative hypothesis, \\(H_{1}\\), states that the population correlation, ρ, is zero (\\(ρ \\neq 0\\)). There is association between dbp and spb.\n\n\n\n\n\n\n\n\n\nStep 2: Set the level of significance, α\n\n\n\nWe set the value α=0.05 for the level of significance (type I error).\n\n\n\n\n\n\n\n\nStep 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic.\n\n\n\nTo test whether ρ is significantly different from zero, \\(ρ \\neq 0\\), we calculate the test statistic:\n\\[t = \\frac{r}{SE_{r}}=\\frac{r}{\\sqrt{(1-r^2)/(n-2)}} \\tag{9.2}\\]\nwhere n is the sample size and \\(SE_{r}=\\sqrt{ \\frac{(1-r^2)}{(n-2)}}\\).\nFor the data in our example, the number of observations are n= 96, r= 0.62 and \\(SE_{r}=\\sqrt{ \\frac{(1-0.62^2)}{(96-2)}}= \\sqrt{ \\frac{(1-0.3844)}{94}} = \\sqrt{\\frac{0.6156}{94}}= 0.081\\).\nAccording to Equation 10.7:\n\\[t = \\frac{r}{SE_{r}}= \\frac{0.62}{0.081}= 7.65\\]\n\n\n\n\n\n\n\n\nStep 4: Decide whether or not the result is statistically significant\n\n\n\nWhen we perform the test, we get a value for the t-statistic (here t= 7.65) that we compare with the t-distribution with n-2 degrees of freedom (here df=94). Using a statistical calculator for t-distribution (such as the distrACTION module from Jamovi), we can compute the probability \\(Pr(T \\geq 7.65)\\). Then, the p-value for a two tailed test is \\(2 \\cdot Pr(T \\geq 7.65)\\). In our example, the p-value < 0.001 which is less than α=0.05 (so, we reject \\(H_{0}\\)).\nNote that the significance of correlation also depends upon the sample size. If the sample size is large, even a weak correlation may be significant, and for a small sample size, even a strong association may or may not be significant.\nTo find a 95% confidence interval for ρ we have to use a Fisher’s z transformation to get a quantity \\(Z_{r}\\) that has approximately Normal distribution. The Fisher’s z transformation of sample correlation coefficient r is:\n\\[ Z_{r}= \\frac{1}{2} ln \\frac{1+r}{1-r}  \\tag{9.3}\\]\nThe 95% CI of the \\(Z_{r}\\) is:\n\\[ 95\\%CI= z_{r} \\ \\pm 1.96 \\cdot SE{z_{r}}= z_{r} \\ \\pm \\frac{1.96}{\\sqrt{n-3}}=[z_{r_{L}}, z_{r_{U}}] \\tag{9.4}\\]\nwhere \\(SE{z_{r}}=\\frac{1}{\\sqrt{n-3}}\\) and \\(z_{r_{L}}, z_{r_{U}}\\) the lower and upper limits of the 95%CI of \\(Z_{r}\\) respectively.\nFinally, we invert the confidence limits of \\(Z_{r}\\); then the lower and upper limits of the 95%CI of ρ are:\n\\[ ρ_{L}= \\frac{e^{2 \\cdot z_{r_{L}}}-1}{e^{2 \\cdot z_{r_{L}}}+1}= 0.48  \\tag{9.5}\\] \\[ ρ_{L}= \\frac{e^{2 \\cdot z_{r_{U}}}-1}{e^{2 \\cdot z_{r_{U}}}+1}= 0.73  \\tag{9.6}\\]\nThe 95% CI calculated from Equation 9.5 and Equation 9.6 is 0.48 to 0.73, so there are quite a wide range of plausible correlation values associated with these data. Additonally, note that the 95% CI of ρ is asymmetric.\n\n\n\n\n\n\n\n\nStep 5: Interpret the results\n\n\n\nThere is evidence of a strong positive linear association between dbp and sbp (r= 0.62, 95% CI: 0.48 to 0.73, p < 0.001)."
  },
  {
    "objectID": "correlation.html#rank-correlation-spearmans-r_s-coefficient",
    "href": "correlation.html#rank-correlation-spearmans-r_s-coefficient",
    "title": "9  Correlation",
    "section": "\n9.3 Rank correlation (Spearman’s \\(r_{s}\\) coefficient)",
    "text": "9.3 Rank correlation (Spearman’s \\(r_{s}\\) coefficient)\nThe basic idea of Spearman’s rank correlation is that the ranks of X and Y are obtained by first separately ordering their values from small to large and then computing the correlation between the two sets of ranks. The strength of correlation is denoted by the coefficient of rank correlation, named Spearman’s rank correlation coefficient, \\(r_{s}\\).\n\n\n\n\n\n\nAssumptions for Spearman’s \\(r_{s}\\) coefficient\n\n\n\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a monotonic association between the two variables (Figure 9.6 a and b)\n\nIn a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. So all linear correlations are monotonic but the opposite is not always true, because we can have also monotonic non-linear associations.\n\n\n\n\nFigure 9.6: The association can be (a) linear monotonic (b) monotonic non-linear, or (c) non-monotonic.\n\n\n\n\n\n\n \n\n\n\n\n\n\nCharacteristics of Spearman’s rank correlation coefficient \\(r\\)\n\n\n\nFormula\nSuppose a set of \\({n}\\) pairs of observations \\((x_{1},y_{1}),\\ldots ,(x_{n},y_{n})\\). Let \\(x_{i}\\) and \\(y_{i}\\) be arranged in ascending order, and the ranks of \\(x_{i}\\) and \\(y_{i}\\) in their respective order be denoted by \\(R_{x_{i}}\\) and \\(R_{y_{i}}\\), respectively. Spearman’s rank correlation coefficient of the sample is defined as:\n\\[r_{s} = \\frac{\\sum_{i=1}^n (R_{x_i} - \\bar{R_{x}})(R{y_i} - \\bar{R{y}})}{\\sqrt{\\sum_{i=1}^n (R_{x_i} - \\bar{R_x})^2 \\sum_{i=1}^n(R_{y_i} - \\bar{R_{y}})^2}} \\tag{9.7}\\]\nwhere \\(\\bar{R_{x}}= \\frac{1}{n} \\cdot \\sum_{i=1}^n R_{x_i}\\) and \\(\\bar{R_{y}}= \\frac{1}{n} \\cdot \\sum_{i=1}^n R_{y_i}\\)\nRange of values\nThe interpretation of Spearman’s rank correlation coefficient \\(r_{s}\\) is similar to the Pearson correlation coefficient \\(r\\) , and \\(r_{s}\\) takes values from −1 to 1.\nThe closer \\(r_{s}\\) is to 0, the weaker is the correlation; \\(r_{s}=1\\) indicates a perfect correlation of ranks, \\(r_{s}=-1\\) indicates a perfect negative correlation of ranks, and \\(r_{s}=0\\) indicates no monotonic correlation between ranks.\n\n\nUsing the data in Figure 9.1 and the Equation 9.7 we find the Spearman’s correlation coefficient \\(r_{s}= 0.65\\).\n\n\n\n\nAltman, Naomi, and Martin Krzywinski. 2015. “Association, Correlation and Causation.” Nature Methods 12 (10): 899–900. https://doi.org/10.1038/nmeth.3587."
  },
  {
    "objectID": "linear.html",
    "href": "linear.html",
    "title": "10  Simple linear regression",
    "section": "",
    "text": "When we have finished this chapter, we should be able to:"
  },
  {
    "objectID": "linear.html#introduction-to-simple-linear-regression",
    "href": "linear.html#introduction-to-simple-linear-regression",
    "title": "10  Simple linear regression",
    "section": "\n10.1 Introduction to simple linear regression",
    "text": "10.1 Introduction to simple linear regression\nSimple linear regression involves a numeric dependent (or response) variable \\(Y\\) and one independent (or explanatory) variable \\(X\\) that is either numeric or categorical.\nOften it is of interest to quantify the linear association between two numeric variables, \\(X\\) and \\(Y\\), and given the value of one variable for an individual, to predict the value of the other variable. This is not possible from the correlation coefficient as it simply indicates the strength of the association as a single number; in order to describe the association between the values of the two variables, a technique called regression is used. In regression, we assume that a change in the independent variable, \\(X\\), will lead directly to a change in the dependent variable \\(Y\\). However, the term “dependent” does not necessarily imply a cause-and-effect relationship between the two variables.\nWe may recall from secondary/high school algebra that the equation of a line is: \\[y = \\alpha + \\beta \\cdot x   \\tag{10.1}\\]\n\n\nFigure 10.1: The equation of line.\n\n\nThe Equation 10.1 is defined by two coefficients (parameters) \\(\\alpha\\) and \\(\\beta\\).\n\nThe intercept coefficient \\(\\alpha\\) is the value of \\(y\\) when \\(x = 0\\) (the point where the fitted line crosses the y-axis; Figure 10.1).\nThe slope coefficient \\(\\beta\\) for \\(x\\) is the mean change in \\(y\\) for every one unit increase in \\(x\\) (Figure 10.1)."
  },
  {
    "objectID": "linear.html#example-association-between-weight-and-height",
    "href": "linear.html#example-association-between-weight-and-height",
    "title": "10  Simple linear regression",
    "section": "\n10.2 Example: Association between weight and height",
    "text": "10.2 Example: Association between weight and height\nLet’s say that we want to explore the association between height and weight for the sample of 550 infants of 1 month age.\nA first step that is usually useful in studying the association between two continuous variables is to prepare a scatter plot of the data (Figure 10.2). The pattern made by the points plotted on the scatter plot usually suggests the basic nature and strength of the association between two variables.\n\n\n\n\nFigure 10.2: Scatter plot with weight and height of infants of 1 month age.\n\n\n\n\nAs we can see in Figure 10.2, the points seem to be scattered around an invisible line . The scatter plot also shows that, in general, infants with high height tend to have high weight (positive association). Additionally, Figure 10.3 presents the results of the correlation analysis:\n\n\n\n\n\nvar1\n\n\nvar2\n\n\ncor\n\n\nstatistic\n\n\np\n\n\nconf.low\n\n\nconf.high\n\n\nmethod\n\n\n\n\nheight\n\n\nweight\n\n\n0.71\n\n\n23.81\n\n\n0\n\n\n0.67\n\n\n0.75\n\n\nPearson\n\n\n\nFigure 10.3: Correlation table for weight and height.\n\n\n\nThere is a high positive linear correlation (r=0.71, 95% CI: 0.67 to 0.75, p <0.001) between weight and height for infants of 1 month age which is significant.\nTo select the best fitting straight line of the data set, it is necessary to determine the estimated values \\(a\\) and \\(b\\) of parameters \\(\\alpha\\) and \\(\\beta\\) in Equation 10.1. The regression equation of the model becomes:\n\\[\\widehat{y} = a  + b \\cdot x \\tag{10.2}\\]\nWhy do we put a “hat” on top of the \\(y\\)? It’s a form of notation commonly used in regression to indicate that we have a predicted value, or the value of \\(y\\) on the regression line for a given \\(x\\) value.\nThe values of the intercept \\(a\\) and the slope \\(b\\) of height are presented in the following table (Figure 10.4):\n\n\n\n\n\n\n\n\nterm\n\n\nestimate\n\n\nstd_error\n\n\nstatistic\n\n\np_value\n\n\nlower_ci\n\n\nupper_ci\n\n\n\n\n\nintercept\n\n\n-5412.15\n\n\n411.04\n\n\n-13.17\n\n\n0\n\n\n-6219.55\n\n\n-4604.74\n\n\n\n\nheight\n\n\n178.31\n\n\n7.49\n\n\n23.81\n\n\n0\n\n\n163.60\n\n\n193.02\n\n\n\n\nFigure 10.4: The linear regression table.\n\n\n\nWe draw the regression line that fits the data in Figure 10.5:\n\n\n\n\nFigure 10.5: Scatter plot of weight and height with the regression line.\n\n\n\n\nNow, let’s focus on interpreting the regression table in Figure 10.4. In the estimate column are the intercept \\(a\\) = -5412.15 and the slope \\(b\\) = 178.31 for height. Thus the equation of the regression line becomes:\n\\[\n\\begin{aligned}\n\\widehat{y} &= a + b \\cdot x\\\\\n\\widehat{\\text{weight}} &= a + b \\cdot\\text{height}\\\\\n\\widehat{\\text{weight}}&= -5412.15 + 178.31\\cdot\\text{height}\n\\end{aligned}\n\\]\n \nThe intercept \\(a\\)\nThe intercept \\(a\\) = -5412.15 is the average weight for those infants with height of 0. In graphical terms, it’s where the line intersects the \\(y\\) axis when \\(x\\) = 0 (Figure 10.6). Note, however, that while the intercept of the regression line has a mathematical interpretation, it has no physical interpretation here, since observing a weight of 0 is impossible.\n\n\n\n\nFigure 10.6: Data of infants’ body height-body weight with fitted line crossing the y-axis.\n\n\n\n\n \nThe slope \\(b\\)\nOf greater interest is the slope of height, \\(b=178.31\\), as it summarizes the association between the height and weight variables.\n\n\n\n\nFigure 10.7: Scatter plot of infants’ body height-body weight and graphically calculation of the slope.\n\n\n\n\nThe graphical calculation of the slope from two points of the fitted line is (Figure 10.7):\n\\[  \nb =\\frac{dy}{dx}=\\frac{5270-4560}{60-56}= \\frac{710}{4} \\approx 178\n\\] Note that, in this example, the coefficient has units g/cm.\nAdditionally, note that the sign is positive, suggesting a positive association between these two variables, meaning infants with higher height also tend to have higher weight. Recall from earlier that the correlation coefficient was \\(r = 0.71\\). They both have the same positive sign, but have a different value. Recall further that the correlation’s interpretation is the “strength of linear association”. The slope’s interpretation is a little different:\n\nFor every 1 cm increase in height, there is on average an associated increase of 178 g of weight.\n\nWe only state that there is an associated increase and not necessarily a causal increase. In other words, just because two variables are strongly associated, it doesn’t necessarily mean that one causes the other. This is summed up in the often quoted phrase, “correlation is not necessarily causation.”\nFurthermore, we say that this associated increase is on average 178 g of weight, because we might have two infants whose height differ by 1 cm, but their difference in weight won’t necessarily be exactly 178. What the slope of 178 is saying is that across all possible infants, the average difference in weight between two infants whose height differ by 1 cm is 178 g.\n\n\n\n\n\n\nInterpretation of linear regression\n\n\n\nIn summary, we can say that the regression coefficient of the height (178) is significantly different from zero (p < 0.001) and indicates that there’s on average an increase of 178 g (\\(95\\%\\)CI: 164 to 193) in weight for every 1 cm increase in height. Note that the \\(95\\%\\)CI does not include the hypothesized null value of zero for the slope."
  },
  {
    "objectID": "linear.html#the-standard-error-se-of-the-regression-slope",
    "href": "linear.html#the-standard-error-se-of-the-regression-slope",
    "title": "10  Simple linear regression",
    "section": "\n10.3 The Standard error (SE) of the regression slope",
    "text": "10.3 The Standard error (SE) of the regression slope\nThe third column of the regression table in Figure 10.4 corresponds to the standard error of our estimates. We are interested in understanding the standard error of the slope (\\(SE_{b}\\)).\n\nSay we hypothetically collected 1000 samples of pairs of weight and height, computed the 1000 resulting values of the fitted slope \\(b\\), and visualized them in a histogram. This would be a visualization of the sampling distribution of \\(b\\). The standard deviation of the sampling distribution of \\(b\\) has a special name: the standard error of \\(b\\).\n\nThe coefficient for the independent variable ‘height’ is 178.31. The standard error is 7.49, which is a measure of the variability around this estimate for the regression slope."
  },
  {
    "objectID": "linear.html#test-statistic-and-confidence-intervals-for-the-slope",
    "href": "linear.html#test-statistic-and-confidence-intervals-for-the-slope",
    "title": "10  Simple linear regression",
    "section": "\n10.4 Test statistic and confidence intervals for the slope",
    "text": "10.4 Test statistic and confidence intervals for the slope\nThe fourth column of the regression table in Figure 10.4 corresponds to a t-statistic. The hypothesis testing for the slope is:\n\\[\n\\begin{aligned}\nH_0 &: \\beta = 0\\\\\n\\text{vs } H_1&: \\beta \\neq 0.\n\\end{aligned}\n\\]\nThe null hypothesis, \\(H_{0}\\), states that the coefficient of the independent variable (height) is equal to zero, and the alternative hypothesis, \\(H_{1}\\), states that the coefficient of the independent variable is not equal to zero.\nThe t-statistic for the slope is defined by the following equation:\n\\[\\ t = \\frac{\\ b}{\\text{SE}_b} \\tag{10.3}\\]\nIn our example:\n\\[\\ t = \\frac{\\ b_1}{\\text{SE}_{b_1}}=\\frac{\\ 178.31}{\\text{7.49}} = 23.81\\]\nThe \\(95\\%\\) CI of the coefficient \\(b\\) for a significance level α = 0.05, \\(df=n-2\\) degrees of freedom and for a two-tailed t-test is given by:\n\\[ 95\\% \\ \\text{CI}_{b} = b \\pm t_{df; 0.05/2} \\cdot \\text{SE}_{b} \\tag{10.4}\\]\nIn our example:\n\\[ 95\\% \\ \\text{CI}_{b} = 178.31 \\pm 1.96 \\cdot \\text{7.49}= 178.31 \\pm 14.68 \\Rightarrow 95\\% \\text{CI}_{b}= \\ (163.6, 193)\\]"
  },
  {
    "objectID": "linear.html#observed-predicted-fitted-values-and-residuals",
    "href": "linear.html#observed-predicted-fitted-values-and-residuals",
    "title": "10  Simple linear regression",
    "section": "\n10.5 Observed, predicted (fitted) values and residuals",
    "text": "10.5 Observed, predicted (fitted) values and residuals\nWe define the following three concepts:\n\n\nObserved values \\(y\\), or the observed value of the dependent variable for a given \\(x\\) value\n\nPredicted (or fitted) values \\(\\widehat{y}\\), or the value on the regression line for a given \\(x\\) value\n\nResiduals \\(y - \\widehat{y}\\), or the error (ε) between the observed value and the predicted value for a given \\(x\\) value\n\n\n\nFigure 10.8: The equation of line.\n\n\nThe residuals are exactly the vertical distance between the observed data point and the associated point on the regression line (predicted value) (Figure 10.8). Positive residuals have associated y values above the fitted line and negative residuals have values below. We want the residuals to be small in magnitude, because large negative residuals are as bad as large positive residuals.\nFigure 10.9 shows these values:\n\n\n\n\n\nID\n\n\nweight\n\n\nheight\n\n\nweight_hat\n\n\nresidual\n\n\n\n\n\n1\n\n\n3950\n\n\n55.5\n\n\n4483.939\n\n\n-533.939\n\n\n\n\n2\n\n\n4630\n\n\n57.0\n\n\n4751.401\n\n\n-121.401\n\n\n\n\n3\n\n\n4750\n\n\n56.0\n\n\n4573.093\n\n\n176.907\n\n\n\n\n4\n\n\n3920\n\n\n56.0\n\n\n4573.093\n\n\n-653.093\n\n\n\n\n5\n\n\n4560\n\n\n55.0\n\n\n4394.785\n\n\n165.215\n\n\n\n\n6\n\n\n3640\n\n\n51.5\n\n\n3770.708\n\n\n-130.708\n\n\n\n\n7\n\n\n3550\n\n\n56.0\n\n\n4573.093\n\n\n-1023.093\n\n\n\n\n8\n\n\n4530\n\n\n57.0\n\n\n4751.401\n\n\n-221.401\n\n\n\n\n9\n\n\n4970\n\n\n58.5\n\n\n5018.863\n\n\n-48.863\n\n\n\n\n10\n\n\n3740\n\n\n52.0\n\n\n3859.862\n\n\n-119.862\n\n\n\n\nFigure 10.9: Regression points (first 10 out of 550 infants).\n\n\n\nObserve in the above table that weight_hat contains the predicted (fitted) values \\(\\widehat{y}\\) = \\(\\widehat{\\text{weight}}\\).\nThe residual column is simply \\(e_i = y - \\widehat{y} = weight - weight\\_hat\\).\nLet’s see, for example, the values for the first infant and have a visual representation:\n\nThe observed value \\(y\\) = 3950 is infant’s weight for \\(x\\) = 55.5.\nThe predicted value \\(\\widehat{y}\\) is the value 4483.939 on the regression line for \\(x\\) = 55.5. This value is computed using the intercept and slope in the previous regression in Figure 10.4: \\[\\widehat{y} = α + b \\cdot x = -5412.145 + 178.308 \\cdot 55.5 = 4483.9\\]\nThe residual is computed by subtracting the predicted (fitted) value \\(\\widehat{y}\\) from the observed value \\(y\\). The residual can be thought of as a model’s error or “lack of fit” for a particular observation. In the case of this infant, it is \\(y - \\widehat{y}\\) = 3950 - 4483.9 = -533.9 .\n\nA “best-fitting” line refers to the line that minimizes the sum of squared residuals (RSS), also known as sum of squared estimate of errors (SSE) out of all possible lines we can draw through the points. The method of least squares is the most popular method used to calculate the coefficients of the regression line.\n\\[ min(RSS) =min\\sum_{i=1}^{n}(y_i - \\widehat{y}_i)^2  \\tag{10.5}\\]\nIn Figure 10.10, we have found the minimum value of RSS (it turns out to be 97723317) and have drawn a horizontal dashed green line. At the point where this minimum touches the graph, we have read down to the x axis to find the best value of the slope. This is the value 178.\n\n\n\n\nFigure 10.10: The sum of the squares of the residuals against the value of the coefficient of the slope which we are trying to estimate."
  },
  {
    "objectID": "linear.html#quality-of-a-linear-regression-fit",
    "href": "linear.html#quality-of-a-linear-regression-fit",
    "title": "10  Simple linear regression",
    "section": "\n10.6 Quality of a linear regression fit",
    "text": "10.6 Quality of a linear regression fit\nThe quality of a linear regression fit is typically assessed using two related quantities: residual standard error (RSE) and the coefficient of determination R\\(^2\\).\nResidual standard error (RSE)\nRSE represents the average distance that the observed values fall from the regression line. Conveniently, it tells us how wrong the regression model is on average using the units of the response variable. Smaller values are better because it indicates that the observations are closer to the fitted line. In our example:\n\\[\\ RSE = \\sqrt{\\frac{\\ RSS}{n-2}}= \\sqrt{\\frac{\\ 97723317}{550-2}}= 422.3 \\tag{10.6}\\]\n \nCoefficient of determination R\\(^2\\)\nThe R\\(^2\\) is the fraction of the total variation in \\(y\\) that is explained by the regression.\n\\[\\ R^2 = \\frac{\\ explained \\ \\ variation}{total \\ \\ variation} \\tag{10.7}\\]\nThe R\\(^2\\) value is called the coefficient of determination and indicates the percentage of the variance in the dependent variable that can be explained or accounted for by the independent variable. Hence, it is a measure of the ‘goodness of fit’ of the regression line to the data. It ranges between 0 and 1 (it won’t be negative). An R\\(^2\\) statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain much of the variability in the response.\nIn our example takes the value 0.5085. It indicates that about 51% of the variation in infant’s body weight can be explained by the variation of the infant’s body height. In simple linear regression \\(\\sqrt{0.5085} = 0.713\\) which equals to the Pearson’s correlation coefficient, r."
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "11  LAB I: Introduction to Jamovi",
    "section": "",
    "text": "When we have finished this Lab, we should be able to:"
  },
  {
    "objectID": "lab1.html#why-jamovi",
    "href": "lab1.html#why-jamovi",
    "title": "11  LAB I: Introduction to Jamovi",
    "section": "\n11.1 Why Jamovi?",
    "text": "11.1 Why Jamovi?\nJamovi is a new fee open “3rd generation” statistical software that is built on top of the programming language R (Figure 11.1). Designed from the ground up to be easy to use, Jamovi is a compelling alternative to costly statistical products such as SPSS and SAS.\n\n\nFigure 11.1: Jamovi is free and open statistical software\n\n\n\n\n\n\n\n\nSome other advantages are:\n\n\n\n\nPoint-and-click environment\nIt provides informative tables and neat visuals\nEnables integration with R\nGives access to a user guide and community resources from the Jamovi website"
  },
  {
    "objectID": "lab1.html#downloading-and-installing-jamovi",
    "href": "lab1.html#downloading-and-installing-jamovi",
    "title": "11  LAB I: Introduction to Jamovi",
    "section": "\n11.2 Downloading and installing Jamovi",
    "text": "11.2 Downloading and installing Jamovi\nJamovi is available for Windows (64-bit), macOS, Linux and ChromeOS. Installation on desktop is quite straight-forward. Just go to the Jamovi download page https://www.jamovi.org/download.html, and download the latest version (current release) for your operating system."
  },
  {
    "objectID": "lab1.html#navigating-jamovi",
    "href": "lab1.html#navigating-jamovi",
    "title": "11  LAB I: Introduction to Jamovi",
    "section": "\n11.3 Navigating Jamovi",
    "text": "11.3 Navigating Jamovi\nWhen jamovi first opens, we will see a screen something like in Figure 11.2.\n\n\n\n\nFigure 11.2: Jamovi starts up!.\n\n\n\n\nTo the left is the spreadsheet view, and to the right is where the results of statistical tests appear. Down the middle is a bar separating these two regions, and this can be dragged to the left or the right to change their sizes.\n \nLet’s take a quick look at the Jamovi Main Menu, referred to hereafter as the Menu, as shown in Figure 11.3. This Menu is displayed at the very top of the Jamovi screen:\n\n\n\n\nFigure 11.3: The menu bar provides access to all functions of the program.\n\n\n\n\nThere are six tabs in the Menu (from left to right): File (a layer with three horizontal levels \\(\\equiv\\)), Variables, Data, Analyses, Edit, and Settings (the three dots \\(\\vdots\\) at the top right of the window) tabs. A toolbar appears whenever we click on a Menu tab (Table 11.1).\n\n\nTable 11.1: Menu and toolbars of Jamovi\n\nMenu tab\nToolbar\n\n\n\n\n\nFile tab (\\(\\equiv\\))\nThe file tab  allows us to open/import existing files, save and export our files.\n\n\n\n\n\n\n\nVariables tab\nThis allows us to view and search our variables in a list view.\n\n\n\n\n\n\n\nThis view allows us to easily navigate our variables and do the following:\n\nSearch for a variable by scrolling through the list or search for one by name.\nEdit the variable names and descriptions by double-clicking in the relevant field.\nEdit our variable details by double-clicking on the data symbol - the screen will appear for us to add all the necessary information.\nCreate a new variable by clicking on the + in the bottom right corner.\n\n\n\n\n\n\nData tab\nHere we will see our raw data which are organised like Excel in rows and columns. We can also manipulate our data and add new variables when necessary.\n\n\n\n\n\n\n\nSpecifically, this tab allows us to do the following:\n\nRename and add details to existing variables. Click on the Setup button, or double-click on the variable we want to manage.\nCompute and transform variables\nAdd and/or Delete variables (columns)\nAdd Filters\nAdd and/or Delete Rows\n\n\n\n\n\n\nAnalyses tab\nIt includes the available statistical analyses that can be performed by Jamovi.\n\n\n\n\n\n\n\nWe will spend most of our time in the Analyses Tab. The following six modules are pre-installed:\n\nExploration\nT-Tests\nANOVA\nRegression\nFrequencies\nFactor\n\nFor example, if we want to perform regression analysis, we simply click the ’’Regression” button.\nAll other modules need to be installed using the Modules button (Plus button) in your top-right \n\n\n\n\n\nEdit tab\nIt includes a toolbar similar to a word processor.\n\n\n\n\n\n\n\nWe can add extra information to our results using the buttons that are very similar to what we would find in Word (though there are fewer options).\n\n\n\n\n\nSettings tab\n(the three dots \\({\\vdots}\\) at the top right of the window)\n\n\n\n\nIt includes the application settings that can be manged by the users according to their preferences.\n\n\n\n\n\n\n\nWe can apply our preferences for a number of settings such as:\n\nHow many decimal numbers we want.\nIf we want to learn R, we can also display the R syntax.\nOur graph colour scheme\nOur default missing value."
  },
  {
    "objectID": "lab1.html#types-of-variables-in-jamovi",
    "href": "lab1.html#types-of-variables-in-jamovi",
    "title": "11  LAB I: Introduction to Jamovi",
    "section": "\n11.4 Types of Variables in Jamovi",
    "text": "11.4 Types of Variables in Jamovi\nData variables can be one of four measure types:\n\n Nominal: This type is for nominal categorical variables.\n Ordinal: This type is for ordinal categorical variables.\n Continuous : this type is for variables with numeric values which are considered to be of Interval or Ratio scales.\n ID: This will usally be our first column. This can be text or numbers, but it should be unique to each row.\n\n \nand one of three data types:\n\nInteger: These are full numbers e.g. 1, 2, 3, ... 100, etc. - Integers can be used for all three measure types . When used for Nominal/Ordinal data numbers will represent labels e.g. male=1; female=2.\nDecimal: These are numbers with decimal points. e.g. 1.3, 5.6, 7.8, etc. - This will usually only be used for continuous data.\nText: This can be used for ordinal and nominal data.\n\nThe measure types are designated by the symbol in the header of the variable’s column. Note that some combinations of data-type and measure-type don’t make sense, and Jamovi won’t let us choose these.\n\n\nTable 11.2: Types of data and measures\n\n\n\n\n\n\n\n\n\nMeasure\n\n\n\n\nData\nNominal\nOrdinal\nContinuous\n\n\nInteger\n\\({\\checkmark}\\)\n\\({\\checkmark}\\)\n\\({\\checkmark}\\)\n\n\nDecimal\n\n\n\\({\\checkmark}\\)\n\n\nText\n\\({\\checkmark}\\)\n\\({\\checkmark}\\)"
  },
  {
    "objectID": "lab1.html#importing-data",
    "href": "lab1.html#importing-data",
    "title": "11  LAB I: Introduction to Jamovi",
    "section": "\n11.5 Importing data",
    "text": "11.5 Importing data\nThe dataset\nIt is possible to simply begin typing values into the Jamovi spreadsheet as we would with any other spreadsheet software. Alternatively, existing data sets in a range of formats (OMV, Excel, CSV, SPSS, R data, Stata, SAS) can be opened in Jamovi. We will use the dataset named arrhythmia as an example (Figure 11.4).\n\n\n\nFigure 11.4: Table with raw data of arrhythmia data set.\n\n\n\nThe meta-data (data about the data) for this dataset are as following:\n\nage: age (yrs)\nsex: sex (0=male, 1=female)\nheight: height (cm)\nweight: weight (kg)\nQRS: mean duration of QRS (ms) \n\nheart_rate: heart rate (beats/min)\nOpening the file\nTo open this Jamovi file, click on the File tab  at the top left hand corner (just left of the ‘Variables’ tab) (Figure 11.5).\n\n\nFigure 11.5: Click on the File tab\n\n\nThis will open the menu shown in Figure 11.6. Select ‘Open’ and then ‘This PC’, and choose the excel file ‘arrhythmia’ from the files listed on ‘Browse’ which are stored on our computer folders:\n\n\n\n\nFigure 11.6: Open an existing file stored on our computer into Jamovi.\n\n\n\n\n \nThe flowchart of the process is:\n\n\n\n\n\nflowchart LR\n  A(File tab) -.-> B(Open) -.-> C(This PC) -.-> D(Browse) -.-> E(Open \\n 'arrhythmia.xlsx')\n\n\n\n\n\n\n\n\n \nWe should see data now in the Spreadsheet view (Figure 11.7).\n\n\n\n\nFigure 11.7: The dataset ‘arrhythmia’.\n\n\n\n\nAs we can see this is a data set with 452 observations and 6 variables. The categorical variable sex is coded as 0 for males and 1 for females, and it is stored as nominal . All other variables are continuous ."
  },
  {
    "objectID": "lab1.html#transforming-data",
    "href": "lab1.html#transforming-data",
    "title": "11  LAB I: Introduction to Jamovi",
    "section": "\n11.6 Transforming data",
    "text": "11.6 Transforming data\nAdding labels to codes\nWe can add labels to numerically coded values of categorical variables. Double-click on the variable name sex. We will get an extra menu opened at the top of the Jamovi screen Figure 11.8:\n\n\nFigure 11.8: Adding labels to numerically coded Values\n\n\nAt this extra menu there is the Levels setup. We can set the labels that should appear for each level of that category. Click on the number ‘0’ in the Levels box to enter the label for this level. Change this from a ‘0’ to the word ‘male’. Click on the number ‘1’ and change it to ‘female’.\nNotice how the numbers 0 and 1 move to the lower right under the text we’re typing, so we can still see which label refers to which number. Click anywhere outside the labels box to save these labels.\nWe close the variable settings by pressing the arrow in the top-right corner .\n \nFiltering data\nNext, we select the Filters button  from the Data tab. This opens the “Row FILTERS” view at the top of the Jamovi screen where we can add a filter called Filter 1 (Figure 11.9). Let’s say that we want to study only the adults from the participants.\n\n\nFigure 11.9: Adding a filter\n\n\nIn order to access functions, press the  icon in the filter settings and from VARIABLES double-click on age (or we just type the variable name but if the variable has a space, we must use '..' around the variable name). Then type the condition age >= 18 and press ENTER from the keyboard in the formula box.\n\n\n\n\n\n\nRelational (or comparison) operators in Jamovi\n\n\n\n\n\nsymbol\nread as\n\n\n\n<\nless than\n\n\n>\ngreater than\n\n\n==\nequal to\n\n\n<=\nless than or equal to\n\n\n>=\ngreater than or equal to\n\n\n!=\nnot equal to\n\n\n\n\n\nNotice that a column named Filter 1 has been added to the Spreadsheet view. Cells that meet the condition age >= 18 are checked with a green tick , while the rest have a red x symbol . Lines with an X are grayed out indicating that these observations are now outside of the current dataset.\nThere is also a switch where we can activate  or inactivate  the filter (note that an inactivate filter will remain visible and can be toggled to active at any time).\nIt is also possible to hide all filter columns by clicking on the eye symbol  of the filters. In this case, all filters and the filtered data will remain active but will be invisible.\nFinally, if we want to delete the filter permanently, click on the cross of the filter.\nBut we can do more complicated filters than this! Let’s say that we’re interested in the adult females. In fact we can specify this in three ways:\na) by using the and operator in ‘Filter 1’ which means that both conditions (adults and females) in the expression must be true at the same time. Therefore, we type the expression age >= 18 and sex == 'female' (Figure 11.10):\n\n\n\n\nFigure 11.10: Combining conditions with and operator in one expression.\n\n\n\n\nNote that in the above expression we can also use double quotes around the \"female\" (i.e. age >= 18 and sex == \"female\") or even the coded value (i.e. age >= 18 and sex == 1).\nb) by adding the second condition (i.e. sex == \"female\") as another expression to Filter 1 (by clicking the small + beside the first expression) (Figure 11.11):\n\n\n\n\nFigure 11.11: Multiple expressions in the same filter.\n\n\n\n\nThis additional expression comes to be represented with its own column F1(2) (Figure 11.11), and by looking at the ticks and crosses, we can see which expression is responsible for excluding each row.\nc) adding a new Filter 2 (by selecting the large + to the left of the filters dialog box) (Figure 11.12). In this case, we can activate or inactivate the filters separately.\n\n\n\n\nFigure 11.12: Multiple expressions using multiple filters.\n\n\n\n\nWe close the filter settings by pressing the arrow in the top-right corner .\n\n\n\n\n\n\nImportant\n\n\n\nFilters in Jamovi exclude the rows for which the expression is not true. When filters are active, all results will be based on the filtered data. If we want to see unfiltered results we will either need to delete the filter or toggle it to inactive.\n\n\n \nComputing a new variable\nSometimes we may need to compute a new variable based on existing information (from other variables) in our data. For example, we may want to compute the body mass index (BMI), from weight (Kg) and height (m) using the formula:\n\\[BMI = weight/height^2 \\tag{11.1}\\]\n\n\n\n\n\n\nBody Mass Index (BMI)\n\n\n\nBMI is a person’s weight in kilograms divided by the square of height in meters. A high BMI can indicate high body fatness.\n\n\nAdding computed variables to a Jamovi spreadsheet is straightforward. Click on Add button available on the Data tab and then select Append from the Computed Variable list. (Alternatively, select the Compute button ). An empty column has been created at the end of our dataset (Figure 11.13). The black dot symbol in the right of the column header indicates that this is a computed variable.\n\n\n\n\nFigure 11.13: Adding computed variables to a Jamovi spreadsheet.\n\n\n\n\nTo set up the computed variable, either double-click on the column header, or select Setup from the Data tab. This opens the “COMPUTED VARIABLE” view where we can set the name of the computed variable as BMI and specify the Equation 11.1 in the formula box (note that we must divide the height by 100 and use the exponentiation operator ^ or **) (Figure 11.14):\n\n\n\n\nFigure 11.14: Setting up the computed variable.\n\n\n\n\nFinally, we can round the variable BMI to one decimal place using the ROUND() function:\n\n\n\n\nFigure 11.15: Rounding up the computed variable.\n\n\n\n\nTransforming a variable\nSometimes it can be useful to convert a quantitative variable into a qualitative variable with levels. For example, the BMI is often categorized as follows:\n\nless than 18.5  underweight\n18.5 to <25.0  healthy weight\n25.0 to <30.0  overweight\n30.0 or higher  obesity\n\nClick on Add button available on the Data tab and select Append from the Transformed Variable list. (Alternatively, select the Transform button ). An empty column has been created at the end of our dataset (Figure 11.16).\n\n\n\n\nFigure 11.16: Adding transformed variables to a Jamovi spreadsheet.\n\n\n\n\nTo set up the transformed variable, either double-click on the column header, or select Setup from the Data tab. This opens the “TRANSFORMED VARIABLE” view where we can set the name of the transformed variable such as BMI2 and create the transformation by selecting the source variable, here the BMI, in the Source Variable field, and creating a new transformation using transform field (Figure 11.17).\n\n\n\n\n\nflowchart LR\n  A(Source variable) -.-> B[[BMI]]\n  C(using transform) -.-> D[[Create New Transform...]]\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.17: Setting up the transformation of a variable.\n\n\n\n\nThis opens the “TRANSFORM” view where Jamovi gives each transformation a name (e.g., Transform 1; if we want we can change it). This allows us to use it again later on other variables if we wish. We can also add a description (Figure 11.18).\n\n\n\n\nFigure 11.18: Adding conditions window.\n\n\n\n\nNow we need to add conditions. Jamovi uses simple if ... else statements and executes each statement starting from the top. So let’s start!\nFirst, select + Add recode condition. Second, we need to populate the boxes with the information as follows (Figure 11.19):\n\nThe $source is the variable we want to transform (here BMI)- don’t change this.\nSelect the appropriate comparison operator (here < )\nIn the next box, we will put the BMI value we want as the cut off point.\nAfter the use, add our new label (here 'underweight'). If we are using text we must enclose it in '...'\n\n\n\n\n\n\nFigure 11.19: Adding the first condition.\n\n\n\n\nWe can add as many conditions as we want by selecting + Add recode condition. This will add a new if $source line into the box. Remember they will be executed in order.\n\n\n\n\nFigure 11.20: Adding more conditions.\n\n\n\n\nFinally, after the else use box just add the label (here 'obese') for the data that does not meet the above conditions.\n\n\n\n\nFigure 11.21: Adding the last label for the data that does not meet the above conditions.\n\n\n\n\n\n\n\n\n\n\nEditing the transformed variable\n\n\n\nThe red dot symbol in the right of the column header indicates that this is a transformed variable. Additionally, we can edit existing transformations by selecting Edit (Figure 11.22).\n\n\n\n\nFigure 11.22: The transformed variable is recognized by a red dot in the right of the column header"
  },
  {
    "objectID": "lab1.html#exercises",
    "href": "lab1.html#exercises",
    "title": "11  LAB I: Introduction to Jamovi",
    "section": "\n Exercises",
    "text": "Exercises\n\n\n\n\n\n\nExercise\n\n\n\nDownload and import the birthwt dataset into Jamovi. This dataset includes the birth weight (in grams) of 189 newborn babies along with some characteristics (e.g., age, smoking status) of their mothers. The data were collected at Baystate Medical Center, Springfield, MA, during 1986.\nThe dataset includes the following variables:\n\nlow: indicator of birth weight less than 2.5 kg (0 = normal birth weight, 1 = low birth weight).\nage: mother’s age in years.\nlwt: mother’s weight in pounds at last menstrual period.\nrace: mother’s race (1 = white, 2 = African-American, 3 = other).\nsmoke: smoking status during pregnancy (0 = not smoking, 1 = smoking).\nptl: number of previous premature labors.\nht: history of hypertension (0 = no, 1 = yes).\nui: presence of uterine irritability (0 = no, 1 = yes).\nftv: number of physician visits during the first trimester.\nbwt: birth weight in grams.\n\nTry to:\nA. Describe the variables of the dataset.\nB. Add labels to the codes of categorical variables.\nC. Compute a new variable named bwt.kg in which the values of birth weight are given in kilograms (Kg)."
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "12  LAB II: Descriptive Statistics",
    "section": "",
    "text": "When we have finished this Lab, we should be able to:\nIn this Lab, we will use the data from arrhythmia dataset that we have already downloaded in our PC. (Note: This starts by assuming we know how to get data into Jamovi)."
  },
  {
    "objectID": "lab2.html#summarizing-categorical-data-frequency-statistics-for-one-categorical-variable",
    "href": "lab2.html#summarizing-categorical-data-frequency-statistics-for-one-categorical-variable",
    "title": "12  LAB II: Descriptive Statistics",
    "section": "\n12.1 Summarizing Categorical Data (Frequency Statistics for one categorical variable)",
    "text": "12.1 Summarizing Categorical Data (Frequency Statistics for one categorical variable)\nConsider the binary sex variable in arrhythmia dataset. A simple way for summarizing the data is to create a table that shows the number of times each category has been observed.\nFirstly, we add labels to numerically coded values of sex variable (see Chapter 11) (0 = male, 1 = female).\nThen we select:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-> B(Exploration) -.-> C(Descriptives)\n\n\n\n\n\n\n\n\nfrom the top menu, as shown below in Figure 12.1.\n\n\nFigure 12.1: In the Analyses Tab select Exploration and click on Descriptives\n\n\nThe Descriptives dialogue box opens, as shown below Figure 12.2:\n\n\nFigure 12.2: The Descriptives dialogue box\n\n\nTransfer the sex variable from the box on the left-hand side of the screen into the Variables box on the right-hand side by highlighting the variable and pressing the Arrow Button (alternatively, drag and drop the variable sex). Then, we check the Frequency tables box and we also unchecked all the other options for descriptive statistics. We will end up with the following screen:\n\n\nFigure 12.3: Transfer the sex variable into the Variables box. Check the Frequency tables box and unchecked all the other options for descriptive statistics.\n\n\nThe output should look like the following:\n\n\nFigure 12.4: The output of frequency distribution of sex\n\n\nThe percentage of female patients (55%, 249/452) is larger than the percentage of male patients (45%, 203/452) in our data.\nTo generate a basic descriptive plot navigate to the Plots section and check the Bar Plot, as shown below:\n\n\nFigure 12.5: From the Plots section check the Bar Plot\n\n\nThe Bar Plot produced here presents the absolute frequencies (counts).\n\n\nFigure 12.6: The Bar Plot produced by Jamovi for a binary variable\n\n\nMore descriptive plots are available for all data types using the surveymv and JJStatsPlot modules."
  },
  {
    "objectID": "lab2.html#summarizing-numerical-data",
    "href": "lab2.html#summarizing-numerical-data",
    "title": "12  LAB II: Descriptive Statistics",
    "section": "\n12.2 Summarizing Numerical Data",
    "text": "12.2 Summarizing Numerical Data\nNavigate to:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-> B(Exploration) -.-> C(Descriptives)\n\n\n\n\n\n\n\n\nfrom the top menu, as shown in Figure 12.1.\nNow, select the variables age and QRS and drag them in the empty Variables box, as shown below:\n\n\nFigure 12.7: select the variables age and QRS\n\n\nNext, we decide how to display our data tables. We have got the following two options:\n\nVariables across columns\nVariables across rows\n\nLet’ select variable across columns (the default).\nWe can now select the relevant descriptive statistics in the Statistics section:\n\n\nFigure 12.8: select the descriptive statistics of interest\n\n\nOnce we have selected our descriptive statistics, a table will appear in the output window on our right, as shown below:\n\n\nFigure 12.9: Descriptive statistics of interest for age and QRS variables\n\n\nFor continuous data we can select Histograms, Box Plots, Violin and Data as shown below:\n\n\nFigure 12.10: Select Histograms, Box Plots, and Violin plots.\n\n\n\n\nFigure 12.11: Histograms of age\n\n\n\n\nFigure 12.12: Violin plot of age\n\n\nAge is a symmetrical distribution.\n\n\nFigure 12.13: Histograms of QRS\n\n\n\n\nFigure 12.14: Violin plot of QRS\n\n\nQRS is an asymmetrical distribution (the data are skewed to the right)."
  },
  {
    "objectID": "lab3.html",
    "href": "lab3.html",
    "title": "13  LAB III: Probability Distributions-Normal Distribution",
    "section": "",
    "text": "When we have finished this Lab, we should be able to:"
  },
  {
    "objectID": "lab3.html#installing-distraction-module-in-jamovi",
    "href": "lab3.html#installing-distraction-module-in-jamovi",
    "title": "13  LAB III: Probability Distributions-Normal Distribution",
    "section": "\n13.1 Installing distrACTION module in Jamovi",
    "text": "13.1 Installing distrACTION module in Jamovi\nSome useful analyses are not pre-installed in Jamovi, but we can access them very easily by installing modules. This is one of the very useful features of Jamovi. While other programs are very difficult to modify after you install them, it is very easy to add new analyses to Jamovi.\nTo install a new module, we want to click on the button in the upper-right corner labeled “Modules” . Then click on “Jamovi library” (Figure 13.1). This will show us the modules that Jamovi has preloaded.\n\n\nFigure 13.1: “Jamovi library” includes the modules that the program has preloaded\n\n\n \nFrom here, we can see the pre-loaded modules that are available for installation. To install one, we need to scroll down until we find it. We are going to install the module, “distrACTION”. Once we find it, we click on the “INSTALL” button under it (Figure 13.2).\n\n\nFigure 13.2: By clicking on the “Install” button we can download modules\n\n\nIf successful, the “INSTALL” button should now read, “INSTALLED”. To see if our module was successfully installed, we need to go back to the main screen. To do so, click on the “up” arrow  in the upper-right corner of the library that hides the Jamovi library.\nNow we can see the module called “distrACTION”  on our Analyses tab at the top. If we can see it, then we successfully installed our module.\nThe “distrACTION” module has two discrete distributions (Binomial and Poisson) and four continuous (normal distribution, T-distribution, chi-squared and F-distribution) to show what happens to the cumulative density function and the quartile density function when you modify parameters."
  },
  {
    "objectID": "lab3.html#binomial-distribution-example-breast-cancer",
    "href": "lab3.html#binomial-distribution-example-breast-cancer",
    "title": "13  LAB III: Probability Distributions-Normal Distribution",
    "section": "\n13.2 Binomial distribution: Example-breast cancer",
    "text": "13.2 Binomial distribution: Example-breast cancer\nSuppose that we recruited a group of 50 patients with breast cancer and we study their survival within five years from diagnosis. Additionally, assume that all patients have the same five-year survival probability, p = 0.8, and the survival status of one patient does not affect the probability of survival for another patient. Let’s answer the following two questions:\nA. What is the probability of 40 survivals (out of 50) in five years from diagnosis?\nNavigate to:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-> B(distrACTION) -.-> C(Binomial Distribution)\n\n\n\n\n\n\n\n\nas shown below (Figure 13.3):\n\n\nFigure 13.3: Selection of the Binomial Distribution from the distrACTION\n\n\nNow, define the parameters Size = 50 and Probability = 0.8. Then check the Compute probability box, select \\(P(X = x1)\\) (the default) and set x1 = 40, as shown below (Figure 13.4):\n\n\nFigure 13.4: Define the parameters of the binomial distribution and compute the probability for x1=40\n\n\nA small table and the probability distribution will appear in the output window, as shown below (Figure 13.5):\n\n\nFigure 13.5: The binomial distribution X ∼ Binomial(50, 0.8), P(X=40)= 0.14\n\n\nΤhe probability of 40 survivals is P(X= 40) = 0.14.\n \nB. What is the probability that either 34 or 35 or 36 patients survive for five years from diagnosis?\nTo answer this question select \\(P(x1≤ X ≤x2)\\) and set x1 = 34 and x2 = 36, as shown below (Figure 13.6):\n\n\nFigure 13.6: Define the parameters of the binomial distribution and compute the probability for x1=34 and x2=36\n\n\nThe results will appear in the output window, as shown below (Figure 13.7):\n\n\nFigure 13.7: The binomial distribution X ∼ Binomial(50, 0.8), P(34≤ X ≤36)= 0.1\n\n\nSince the underlying event include three possible outcomes, 34, 35, and 36, we obtain the probability by adding the individual probabilities for these outcomes:\n\\[P(34≤ X ≤36) = P(X = 34) + P(X = 35) + P(X = 36) = 0.02 + 0.03 + 0.05 = 0.1\\]"
  },
  {
    "objectID": "lab3.html#poisson-distribution-example-physician-visits-per-year",
    "href": "lab3.html#poisson-distribution-example-physician-visits-per-year",
    "title": "13  LAB III: Probability Distributions-Normal Distribution",
    "section": "\n13.3 Poisson distribution: Example-physician visits per year",
    "text": "13.3 Poisson distribution: Example-physician visits per year\nAssume that the rate of physician visits per year is λ = 2.5.\nA. What is the probability of one visit per year?\nNavigate to:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-> B(distrACTION) -.-> C(Poisson Distribution)\n\n\n\n\n\n\n\n\nas shown below (Figure 13.8):\n\n\nFigure 13.8: Selection of the Poisson Distribution from the distrACTION\n\n\nNow, define the parameter λ= 2.5. Then check the Compute probability box, select \\(P(X = x1)\\) (the default) and set x1 = 1, as shown below (Figure 13.9):\n\n\nFigure 13.9: Define the parameters of the binomial distribution and compute the probability for x1=1\n\n\nThe results will appear in the output window, as shown below (Figure 13.10):\n\n\nFigure 13.10: The poisson distribution X ∼ Poisson(2.5), P(X = 1) = 0.21\n\n\nThe probability of one visit per year is P(X = 1) = 0.21.\n \nB. What is the probability of up to three visits per year?\nTo answer this question select \\(P(X ≤x1)\\) and set x1 = 3 , as shown below (Figure 13.11):\n\n\nFigure 13.11: Define the parameters of the binomial distribution and compute the probability for X ≤ 3\n\n\nThe results will appear in the output window, as shown below (Figure 13.12):\n\n\nFigure 13.12: The poisson distribution X ∼ Poisson(2.5), P(X ≤ 3)= 0.76\n\n\nThis is the probability that a person visit her/his physician 0, or 1, or 2, or 3 times within one year. As before, we add the individual probabilities for the corresponding outcomes: P(X ≤ 3) = 0.08 + 0.21 + 0.26 + 0.21 = 0.76."
  },
  {
    "objectID": "lab3.html#normal-distribution-example-systolic-blood-pressure",
    "href": "lab3.html#normal-distribution-example-systolic-blood-pressure",
    "title": "13  LAB III: Probability Distributions-Normal Distribution",
    "section": "\n13.4 Normal distribution: Example-systolic blood pressure",
    "text": "13.4 Normal distribution: Example-systolic blood pressure\nSuppose we know that the population mean and standard deviation for systolic blood pressure (sbp) are μ = 125 mmHg and σ = 15 mmHg, respectively.\nA. About what percentage of population has sbp in the range μ±σ ?\nNavigate to:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-> B(distrACTION) -.-> C(Normal Distribution)\n\n\n\n\n\n\n\n\nas shown below (Figure 13.13):\n\n\nFigure 13.13: Selection of the Normal Distribution from the distrACTION\n\n\nNow, define the parameters Mean = 125 and SD = 15. Then check the Compute probability box, select \\(P(x1≤ X ≤x2)\\) and set x1 = 110 (125-15) and x2 = 140 (125 + 15), as shown below (Figure 13.14):\n\n\nFigure 13.14: Define the parameters of the normal distribution and compute the probability for 110≤X≤ 140\n\n\nThe results will appear in the output window, as shown below (Figure 13.15):\n\n\nFigure 13.15: The normal distribution X ∼ N(125, \\(15^2\\)), P(110≤X≤ 140)= 0.68\n\n\nThus, the percentage of the distribution between -1 \\(\\sigma\\) below the mean and +1 \\(\\sigma\\) above the mean is:\n\\[ P(125 −15≤X≤ 125+15) = P(110≤X≤ 140) = 0.68\\ or\\ 68\\% \\]  \nB. Calculate a 95% reference range for the sbp.\nTo answer this question check the Compute quantiles box, select Central interval quartiles, and set p= 0.95, as shown below (Figure 13.16):\n\n\nFigure 13.16: Define the parameters of the normal distribution and set p=0.95 and click on central interval quantiles\n\n\nThe results will appear in the output window, as shown below (Figure 13.17):\n\n\nFigure 13.17: The normal distribution X ∼ N(125, \\(15^2\\)), 95% ref. range = (95.6, 154.4)\n\n\n\\[ 95\\% \\ ref. \\ range \\equiv (125 - 1.96\\times 15,\\ 125 + 1.96\\times 15) = (95.6, 154.4) \\]"
  },
  {
    "objectID": "lab3.html#standard-normal-distribution-example-calculate-the-area-under-the-curve",
    "href": "lab3.html#standard-normal-distribution-example-calculate-the-area-under-the-curve",
    "title": "13  LAB III: Probability Distributions-Normal Distribution",
    "section": "\n13.5 Standard Normal distribution: Example-calculate the area under the curve",
    "text": "13.5 Standard Normal distribution: Example-calculate the area under the curve\nCalculate the area between \\(z_1\\)=1 and \\(z_2\\)=2 of a Standard Normal distribution.\nCheck the Compute probability box, select \\(P(x1≤ X ≤x2)\\) and set x1 = 1 and x2 = 2, as shown below (Figure 13.18):\n\n\nFigure 13.18: Define the parameters of the standard normal distribution and set x1=1 and x2=2\n\n\nThe results will appear in the output window, as shown below (Figure 13.19):\n\n\nFigure 13.19: The standard normal distribution X ∼ Z(0, \\(1^2\\)), P(1≤X≤2)\n\n\nWe can also easily calculate the area using the properties of the (standard) normal distribution:\n\\(E = P(0\\leq Z\\leq 2) - P(0\\leq Z\\leq 1) \\approx 0.48 - 0.34 \\approx 0.14\\)"
  },
  {
    "objectID": "lab4.html",
    "href": "lab4.html",
    "title": "14  LAB IV: Estimation and Confidence Interval",
    "section": "",
    "text": "When we have finished this Lab, we should be able to:"
  },
  {
    "objectID": "lab4.html#the-sampling-distribution-of-mean-and-the-central-limit-theorem-clt",
    "href": "lab4.html#the-sampling-distribution-of-mean-and-the-central-limit-theorem-clt",
    "title": "14  LAB IV: Estimation and Confidence Interval",
    "section": "\n14.1 The Sampling Distribution of mean and the Central Limit Theorem (CLT)",
    "text": "14.1 The Sampling Distribution of mean and the Central Limit Theorem (CLT)\nIn this Lab we will learn the Central Limit Theorem (CLT), which is the basis for many statistical concepts. We are going to explore this concept with the help of a Shiny application. So, clink on the following link CLM.\nA Shiny app opens in a web window as shown below (Figure 14.1):\n\n\nFigure 14.1: The Shiny application simulating the Central limit Theorem for Means\n\n\nTo the left is the interactive panel with radio buttons and slider bars, and to the right there are three tabs: Population Distribution, Samples, and Sampling Distribution.\nFirst we are asked to choose from a Normal, Uniform, Right Skewed or Left Skewed Population distribution (Parent distribution) from the left panel. Let’s select Right skewed and then High skew from the drop down menu with the name Skew (Figure 14.2).\n\n\nFigure 14.2: The case of a high right skewed population distribution\n\n\nNext we set the value of the Sample size slider bar to 5 and the Number of samples to 1000 and we select the Sampling distribution tab. We observe that the sampling distribution is right skewed with mean approximately equal to population mean (Figure 14.3).\n\n\nFigure 14.3: Distribution of means of 1000 random samples, each consisting of 5 observations from a high right skewed population distribution\n\n\nNow, try to increase the sample size to 30 (Figure 14.4):\n\n\nFigure 14.4: Distribution of means of 1000 random samples, each consisting of 30 observations from a high right skewed population distribution\n\n\nand then increase it to 200 (Figure 14.5):\n\n\nFigure 14.5: Distribution of means of 1000 random samples, each consisting of 200 observations from a high right skewed population distribution\n\n\nWe observe that the sampling distribution becomes closer and closer to Normal and the standard error of the mean, SE, (the standard deviation of sample means) gets smaller as the sample size increases. The important point is that whatever the parent distribution of a variable, the distribution of the sample means will be nearly Normal, as long as the samples are large enough.\n \n\n\n\n\n\n\nProperties of the sampling distribution of the mean\n\n\n\n\nThe mean of the sampling distribution is the same as the mean of the population.\nThe standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases.\n\nAccording to the Central Limit Theorem (CLM), the shape of the sampling distribution becomes normal as the sample size increases regardless of the variable’s population distribution."
  },
  {
    "objectID": "lab4.html#the-confidence-interval-of-mean",
    "href": "lab4.html#the-confidence-interval-of-mean",
    "title": "14  LAB IV: Estimation and Confidence Interval",
    "section": "\n14.2 The confidence interval of mean",
    "text": "14.2 The confidence interval of mean\nWe are going to explore the concept of confidence interval (CI) of mean with the help of a Shiny application. So, clink on the following link CIs.\nA Shiny app opens in a web window as shown below (Figure 14.6):\n\n\nFigure 14.6: Shiny application that simulates the concept of confidence interval (CI) of mean\n\n\nTo the left is the interactive panel with radio buttons and drop down menus, and to the right there are two tabs: Plots and About.\nFirst we are asked to choose if we want the Confidence Interval Graph only or the Confidence Interval Graph Plus Sampling Distribution of the Mean. Let’s select the first choice and set the value of the Number of Simulated Samples to one and the Sample Size to 10 from the drop down menus (Figure 14.7).\nWe observe at the Plot tab that a horizontal bar has been created which represents the confidence interval (CI), centered on the sample mean (point). In this case, the 95%CI for the sample mean includes the true value of the population mean (it crosses the solid vertical line) and it is drawn as a black line (Figure 14.7).\n\n\nFigure 14.7: Confidence Interval Graph with one random sample of 10 observations selected from a normal population distribution\n\n\nNow, try to increase the Number of Simulated Samples to 100 (Figure 14.8):\n\n\nFigure 14.8: Confidence Interval Graph with 100 random samples, each consisting of 10 observations from a normal population distribution\n\n\nWe observe that 5 out of 100 confidence intervals (red horizontal lines) do not include the true population mean (the solid vertical line) (Figure 14.8). This is what we would expect – that the 95% confidence interval will not include the true population mean 5% of the time.\n\n\n\n\n\n\n95%CI of the mean interpretation\n\n\n\nFor 95% of the calculated confidence intervals it will be true to say that the population mean, μ, lies within this interval. The problem is, as Figure 14.8 shows, with a single study we just do not know which one of these 100 intervals we will obtain and hence we will not know if it includes μ. Unfortunately, the CIs ARE NOT pre-labelled with “I am poor CI and do not include the population mean: do not choose me!”. Therefore, we usually interpret a 95% confidence interval of mean as the range of values within which we are 95% confident that the true population mean lies.\n\n\nNext, we create the confidence intervals of 100 randomly generated samples of size = 50 from the population (Figure 14.9):\n\n\nFigure 14.9: Confidence Interval Graph with 100 random samples, each consisting of 50 observations from a normal population distribution\n\n\nWe observe that the sample means are closer to the true population mean and the 95%CIs of the mean become narrower (Figure 14.8) increasing the sample size."
  },
  {
    "objectID": "lab5.html",
    "href": "lab5.html",
    "title": "15  LAB V: Foundations for Statistical Inference",
    "section": "",
    "text": "When we have finished this Lab, we should be able to:\nIn this Lab we are going to test whether the sample mean and the mean of the population of a variable of interest differ significantly (Figure 15.1) using the theory of hypothesis testing.\nWe are going to answer in the research question with the help of a Shiny application. So, clink on the following link Z-Test.\nA Shiny app opens in a web window as shown below (Figure 15.2):\nTo the left is the interactive panel with drop down menus, radio buttons and slider bars, and to the right is the place where the results appear (Figure 15.2)."
  },
  {
    "objectID": "lab5.html#the-research-question",
    "href": "lab5.html#the-research-question",
    "title": "15  LAB V: Foundations for Statistical Inference",
    "section": "\n15.1 The Research question",
    "text": "15.1 The Research question\nResearch question: Does a new experimental treatment affect the number of days it takes for patients to recover from a disease?\nAs reported by hospitals across the nation, the patients in the population who received the usual care have a mean number of days to recover equal to 8 (μ= 8 days) with standard deviation 2 days (σ= 2 days). Additionally, assume that the the variable of “number of days for recovering from the disease” is normally distributed.\nAs we run the experiment, we track how long it takes for each patient to fully recover from the disease and we record for 10 patients the following values: 7, 6, 9, 8, 8, 7, 7, 6, 8, 6.\nWe summarize the previous information in Table 15.1.\n\n\nTable 15.1: Organize the information of the example\n\n\n\n\n\nResearch question\nDoes a new experimental treatment affect the number of days it takes for patients to recover from a disease?\n\n\n\nVariable of interest\nTime to recover from the disease in days (discrete numerical variable; normally distributed).\n\n\nPopulation group (μ=8 days, σ=2 days)\nPatients in the population received the usual care as reported by hospitals across the nation.\n\n\nTreatment group\n\nTen patients received the experimental medical treatment with the following times in days to recover from the disease:\n7, 6, 9, 8, 8, 7, 7, 6, 8, 6"
  },
  {
    "objectID": "lab5.html#applying-the-theory-of-hypothesis-testing-one-sample-z-test",
    "href": "lab5.html#applying-the-theory-of-hypothesis-testing-one-sample-z-test",
    "title": "15  LAB V: Foundations for Statistical Inference",
    "section": "\n15.2 Applying the theory of Hypothesis Testing (One Sample Z-Test)",
    "text": "15.2 Applying the theory of Hypothesis Testing (One Sample Z-Test)\n\n\n\n\n\n\nStep 1: Determine the appropriate null hypothesis and alternative hypothesis\n\n\n\nThe null hypothesis, \\(H_{0}\\), states that the treatment group and the population group will recover from the disease in about the same number of days, on average (\\(μ = 8\\)).\nThe alternative hypothesis, \\(H_{1}\\), states that the experimental medical treatment will affect the number of days it takes for patients to recover from the disease. The two groups will differ significantly (\\(μ \\neq 8\\)).\n\n\nFirst, we select from the drop down menu Inference for one mean. Then we set the Null hypothesis: \\(H_{0}: μ= 8\\) and the Alternative \\(\\neq\\) (two sided test). We also check the box of variance of the population, where we type \\(\\sigma^2 = 4\\), as shown below (Figure 15.3):\n\n\nFigure 15.3: Settings for one sample Z-test\n\n\n\n\n\n\n\n\nStep 2: Set the level of significance, α\n\n\n\nWe set the value α=0.05 for the level of significance (type I error).\n\n\nAt the slider bar we keep the default value α=0.05 for the level of significance (Figure 15.4):\n\n\nFigure 15.4: We set the value α=0.05 for the level of significance\n\n\n\n\n\n\n\n\nStep 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic.\n\n\n\nSupposing the distribution of number of days until the recover is Normal, the appropriate test statistic is the z-statistic:\n\\[z = \\frac{\\bar{x} - \\mu_{o}}{SE_{z}}= \\frac{\\bar{x} - \\mu_{o}}{\\sigma/ \\sqrt{n}} \\] To calculate the z-statistic we also need the data from the treatment group.\n\n\nIn the Sample box type the values of the treatment group 7, 6, 9, 8, 8, 7, 7, 6, 8, 6 (Figure 15.5):\n\n\nFigure 15.5: The sample data of the treatment group\n\n\nThe sample mean is equal to \\(\\bar{x} = 7.2\\) days. Therefore:\n\\[z = \\frac{\\bar{x} - \\mu_{o}}{\\sigma/ \\sqrt{n}}= \\frac{7.2 - 8}{2/ \\sqrt{10}}= \\frac{- 0.8}{2/ \\sqrt{10}}= \\frac{- 0.8}{0.632}= -1.265\\]\n\n\n\n\n\n\nStep 4: Decide whether or not the result is statistically significant\n\n\n\nWhen we run the analysis, we get a z-value and a p-value.\n\nThe z-value is a measure of how different the two groups are on our variable of interest.\nA p-value is the probability of obtaining the observed results, or something more extreme, if the null hypothesis is true. In our example, the chance of observing our results assuming that the new treatment does not differ from the usual care in population regarding the number of recovery days.\n\nA p-value less than 0.05 means that our result is statistically significant and there is evidence that the difference in recovery days is not due to chance alone (reject \\(H_{0}\\)).\nA p-value greater than or equal to 0.05 means that our result is not statistically significant. There is no evidence of a significant difference between the mean recovery days in the treatment group and the population (not reject \\(H_{0}\\)).\n\n\nIn our example, we observe that the p-value for a two-sided test is equal to 2*0.103= 0.206 > 0.05 (Figure 15.6). So, the sample mean and the mean of the population does not differ significantly in days to recover from the disease, we can not reject the null hypothesis, \\(H_{0}\\).\n\n\nFigure 15.6: The blue vertical line of Test stististic=-1.265 is outside of the rejection region. Therefore, we failed to reject Ho.\n\n\n\n\n\n\n\n\nStep 5: Interpret the results\n\n\n\nThere is no evidence that the new experimental treatment affect significantly the number of days it takes for patients to recover from the disease compared to the usual care in the population.\n\n\nFinally, we can calculate the 95%CI of mean as following:\n\\[ 95\\%CI= \\bar{x} \\ \\pm 1.96  \\frac{\\sigma}{\\sqrt{n}}= 7.2 \\ \\pm 1.96  \\frac{2}{\\sqrt{10}}= 7.2 \\ \\pm \\frac{3.92}{3.162} = 7.2 \\ \\pm 1.24 = [5.96, 8.44]\\] Note that, in our example, the value of population mean (μ=8) is included in the range of values of the 95%CI."
  },
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "16  LAB VI: Inference for numerical data (2 samples)",
    "section": "",
    "text": "When we have finished this Lab, we should be able to:"
  },
  {
    "objectID": "lab6.html#two-sample-t-test-students-t-test",
    "href": "lab6.html#two-sample-t-test-students-t-test",
    "title": "16  LAB VI: Inference for numerical data (2 samples)",
    "section": "\n16.1 Two-sample t-test (Student’s t-test)",
    "text": "16.1 Two-sample t-test (Student’s t-test)\nTwo sample t-test (Student’s t-test) can be used if we have two independent (unrelated) groups (e.g., males-females, treatment-non treatment) and one quantitative variable of interest.\nOpening the file\nOpen the dataset named depression from the file tab in the menu:\n\n\nFigure 16.1: The depression dataset\n\n\nThe dataset depression has 76 patients and includes two variables. The treatment variable and the HDRS variable (Figure 16.1). Double-click on the variable name HDRS and change the measure type from nominal  to continuous .\nResearch question\nIn an experiment designed to test the effectiveness of paroxetine for treating bipolar depression, the participants were randomly assigned into two groups (intervention Vs placebo).\nThe researchers used the Hamilton Depression Rating Scale (HDRS) to measure the depression state of the participants and wanted to find out if the HDRS score is different in paroxetine group as compared to placebo group at the end of the experiment. The significance level α was set to 0.05.\nNote A score of 0–7 in HDRS is generally accepted to be within the normal range, while a score of 20 or higher indicates at least moderate severity.\nHypothesis Testsing for the Student’s t-test\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the means of HDRS in the two groups are equal (\\(\\mu_{1} = \\mu_{2}\\))\n\n\\(H_1\\): the means of HDRS in the two groups are not equal (\\(\\mu_{1} \\neq \\mu_{2}\\))\n\n\n\nAssumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in both groups\nThe data in both groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the descriptive characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) and for equality of variances (e.g., Levene’s test) can be used.\nWe select:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-> B(Exploration) -.-> C(Descriptives)\n\n\n\n\n\n\n\n\nDrag the variable HDRS into the Variables box and split it by the treatment variable, as shown below (Figure 16.2):\n\n\nFigure 16.2: Split the variable HDRS by treatment group\n\n\nWe can now select the relevant descriptive statistics such as Percantiles, Skewness, Kurtosis and the Shapiro-Wilk test from the Statistics section:\n\n\nFigure 16.3: In the Statistics section select the descriptive statistics of interest.\n\n\nOnce we have selected our descriptive statistics, a table will appear in the output window on our right-hand side, as shown below:\n\n\nFigure 16.4: Descriptive statistics of HDSR by treatment group\n\n\nThe means are close to medians (20.3 vs 21 and 21.5 vs 21). The skewness is approximately zero (symmetric distribution) and the (excess) kurtosis is close to zero (mesokurtic distribution) indicating normal distributions for both groups.\nAdditionally, the Shapiro-Wilk tests of normality suggest that the data for the HDRS in both groups, paroxetine and placebo, are normally distributed (p=0.67 >0.05 and p=0.61 >0.05, respectively). (NOTE: If the \\(p \\geq 0.05\\), then the data came from a normally distributed population).\n\n\n\n\n\n\nRemember: Hypothesis testing for Shapiro-Wilk test for normality\n\n\n\n\\(H_{0}\\): the data came from a normally distributed population.\n\\(H_{1}\\): the data tested are not normally distributed.\n\nIf p − value < 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nThen we can check the Density from Histograms in the Plot section, as shown below (Figure 16.6):\n\n\nFigure 16.5: In the Plot section select Density from Histograms.\n\n\nA graph is generated in the output window on our right-hand side, as shown below:\n\n\nFigure 16.6: In the Plots section select Density from Histograms.\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable.\nB. Homogeneity of variance\nThe second assumption that should be satisfied is the homogeneity of variance. We observe in the summary table of Figure 16.4 that the two standard deviations (3.65 vs 3.41) are similar (see also below the Levene’s test for equality of variances in Figure 16.10).\n \nRun the Student’s t-test\n\n\n\n\n\n\nPerform a Student’s t-test\n\n\n\nWe will perform a Student’s t-test to test the null hypothesis that the mean HDRS score is the same for both groups (paroxetine and placebo).\nWe select:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-> B(T-Tests) -.-> C(Independent Samples T-Test)\n\n\n\n\n\n\n\n\n\n\nFigure 16.7: Conducting an Independent Samples T-Test.\n\n\nThe Independent Samples T-Test dialogue box opens. Drag and drop the numeric variable HSDR to Dependent Variables and the independent variable treatment to Grouping Variable, as shown below Figure 16.8:\n\n\nFigure 16.8: The Independent Samples T-Test dialogue box\n\n\nWe observe that we can select between the following three Tests: Students’s (the default), Welch’s, or Mann-Whitney U. At the moment, we keep the default choice of Students’s test. From Additional Statistics check the Mean difference, Confidence Intervals, Descriptive, and Descriptive plots boxes. Finally, from Assumption Checks tick the Homogeneity test. We will end up with the following screen:\n\n\nFigure 16.9: Additional statistics and tests.\n\n\nFirst, we look at the table of Levene's test for equality of variances (Figure 16.10):\n\n\nFigure 16.10: Levene’s test.\n\n\n\n\n\n\n\n\nRemember: Hypothesis testing for Levene’s test for equality of variances\n\n\n\n\\(H_{0}\\): the variances of HDRs in two groups are equal\n\\(H_{1}\\): the variances of HDRs in two groups are not equal\n\nIf p − value < 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nSince p = 0.646 > 0.05, the \\(H_0\\) of the Levene’s test is not rejected and we keep the default choice of Students’s test (Figure 16.9). (NOTE: If the \\(p \\geq 0.05\\), then the population variances of HDRS in two groups groups are assumed equal).\nIf the assumption of equal variances is not satisfied (Levene’s test gives p < 0.05, reject \\(H_0\\)), the Welch’s test should be used from the available Tests in Jamovi (Figure 16.9).\nNext, we can inspect again the results in the group descriptives table (Figure 16.11) and pertinent plots (Figure 16.12):\n\n\nFigure 16.11: Group descriptives.\n\n\n\n\nFigure 16.12: Plot of mean (95% CI) and median of HDRS by treatment.\n\n\nFinally, we present the results of the Student’s t-test in the table of the Figure 16.13:\n\n\nFigure 16.13: The results of the Student’s t-test.\n\n\nThe p-value = 0.16 is greater than 0.05. There is no evidence of a significant difference in mean HDRS between the two groups (failed to reject \\(H_0\\)). The difference between means (20.33 - 21.49) equals to -1.16 units of the HDRS and note that the 95% confidence interval of the difference in means (-2.78 to 0.47) includes the hypothesized null value of 0. Based on these results, there is not evidence that paroxetine is effective as a treatment for bipolar depression.\nNote that the paroxetine sample (n= 33) has 32 (33-1) degrees of freedom and the placebo sample (n= 43) has 42 (43-1), so we have 74 (32 + 42) df in total. Another way of thinking of this is that the complete sample size is 76, and we have estimated two parameters from the data (the two means), so we have 76-2 = 74 df.\nThe Student t-test for two independent samples does not have any restrictions on \\(n_1\\) and \\(n_2\\) —they can be equal or unequal. However, equal samples are preferred because when a total of 2n subjects are available, their equal division among the groups maximizes the power to detect a specified difference.\n\n\n\n\n\n\n\n\nMann-Whitney U test\n\n\n\nWhen there is violation of normality, the Mann-Whitney U test can be selected from the available Tests in Jamovi (Figure 16.10). This test compares two independent samples based on the ranks of the values and is often considered the non-parametric equivalent to the Student’s t-test."
  },
  {
    "objectID": "lab6.html#paired-samples-t-test",
    "href": "lab6.html#paired-samples-t-test",
    "title": "16  LAB VI: Inference for numerical data (2 samples)",
    "section": "\n16.2 Paired samples t-test",
    "text": "16.2 Paired samples t-test\nThe paired samples design can effectively reduce the effect of non-treatment factors and improve the efficiency of the experiment. A paired samples t-test is used to estimate whether the means of two related measurements are significantly different from one another.\nOpen the dataset named weight from the file tab in the menu:\n\n\nFigure 16.14: The weight dataset\n\n\nThe dataset weight contains the birth and discharge weight of 25 newborns (Figure 16.14). Double-click on the name of the variables birth_weight and discharge_weight to change the measure type from nominal  to continuous .\nResearch question\nWe might ask if the mean difference of the weight in birth and in discharge equals to zero or not. If the differences between the pairs of measurements are normally distributed, a paired t-test is the most appropriate statistical test.\nHypothesis Testsing for the paired samples t-test\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\nH0: the mean difference in weight is zero (\\(\\mu_{d} = 0\\))\nH1: the mean difference in weight is non-zero (\\(\\mu_{d} \\neq 0\\))\n\n\n\nAssumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\n\nThe differences between the pairs of measurements, \\(d_{i}\\)s, are normally distributed. (NOTE: It is not essential for the original observations to be normally distributed).\n\n\n\nExplore the characteristics of the distribution of differences, \\(d_{i}\\)\nFirst, we have to calculate the differences \\(d_{i}= birth\\_weight_i - discharge\\_weight_i\\) (Figure 16.15). For more details go to the section 11.6 Transforming data: Computing a new variable in Chapter 11.\n\n\nFigure 16.15: Calculation of the variable of differences d\n\n\nThe distributions of the differences,\\(d_{i}\\), can be explored with appropriate plots and summary statistics.\nWe select:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-> B(Exploration) -.-> C(Descriptives)\n\n\n\n\n\n\n\n\nDrag the variable d into the Variables box, as shown below (Figure 16.16):\n\n\nFigure 16.16: Drag the variable of the differences d into the Variables box\n\n\nWe can now select the relevant descriptive statistics such as Percantiles, Skewness, Kurtosis and the Shapiro-Wilk test from the Statistics section:\n\n\nIn the Statistics section select the descriptive statistics of interest.\n\n\nOnce we have selected our descriptive statistics, a table will appear in the output window on our right-hand side, as shown below:\n\n\nFigure 16.17: Descriptive statistics of the differences.\n\n\nThe mean is close to median (39.6 vs 40). Moreover, both skewness and (excess) kurtosis are approximately zero indicating a symmetric and mesokurtic distribution of the weight differences.\nThen we can check the Density from Histograms in the Plot section, as shown below (Figure 17.5):\n\n\nIn the Plot section select Density from Histograms.\n\n\nA graph is generated in the output window on our right-hand side, as shown below:\n\n\nFigure 16.18: In the Plots section select Density from Histograms.\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable.\nAdditionally, the Shapiro-Wilk test of normality suggests that the data for the differences, \\(d_{i}\\), are normally distributed (p=0.74 >0.05). (NOTE: If the \\(p \\geq 0.05\\), then the data came from a normally distributed population).\nRun the paired samples t-test\n\n\n\n\n\n\nPerform a paired samples t-test\n\n\n\nWe will perform a paired samples t-test to test the null hypothesis that the mean difference in weight is zero.\nWe select:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-> B(T-Tests) -.-> C(Paired Samples T-Test)\n\n\n\n\n\n\n\n\n\n\nFigure 16.19: Conducting a Paired Samples T-Test.\n\n\nThe Paired Samples T-Test dialogue box opens. Drag and drop the variables birth_weight and discharge_weight to Paired Variables, as shown below Figure 16.20:\n\n\nFigure 16.20: The Paired Samples T-Test dialogue box\n\n\nWe observe that we can select between the following two Tests: Students’s or Wilcoxon rank. We keep the default choice of Students’s paired t-test. Moreover, from Additional Statistics check the Mean difference, Confidence Intervals, Descriptive, and Descriptive plots boxes. Finally, from Assumption Checks tick the Normality test. We will end up with the following screen:\n\n\nFigure 16.21: Additional statistics and tests.\n\n\nNext, we can inspect the results in the table with descriptive statistics (Figure 16.22) and plots (Figure 16.12):\n\n\nFigure 16.22: Table with descriptive statistics.\n\n\n\n\nFigure 16.23: Plot of mean and median of birth weigt and discharge_weight.\n\n\nThe Shapiro-Wilk test of normality of the differences has previously calculated (Figure 16.17) and is also presented below:\n\n\nFigure 16.24: Test of normality of the differences.\n\n\nFinally, we present the results of the Student’s paired samples t-test in the table of the Figure 16.25:\n\n\nFigure 16.25: The results of the Paired Samples t-test.\n\n\nThere was a significant reduction in weight (39.6 g) after the discharge (p-value <0.001 that is lower than 0.05; reject \\(H_0\\)). Note that the 95% confidence interval (26.3 to 52.9) doesn’t include the null hypothesized value of 0. However, is this reduction of clinical importance?\n\n\n\n\n\n\n\n\nWilcoxon Signed-Rank test\n\n\n\nWhen there is violation of normality, the Wilcoxon Rank test can be selected from the available Tests in Jamovi (Figure 16.21). This test is based on the sign and the magnitude of the rank of the differences between pairs of measurements, rather than the actual values. It is often considered the non-parametric equivalent to the Student’s paired samples t-test."
  },
  {
    "objectID": "lab7.html",
    "href": "lab7.html",
    "title": "17  LAB VII: Inference for numerical data (>2 samples)",
    "section": "",
    "text": "When we have finished this Lab, we should be able to:"
  },
  {
    "objectID": "lab7.html#introduction",
    "href": "lab7.html#introduction",
    "title": "17  LAB VII: Inference for numerical data (>2 samples)",
    "section": "\n17.1 Introduction",
    "text": "17.1 Introduction\nThe one-way analysis of variance (one-way ANOVA) or the non-parametric Kruskal-Wallis test are used to detect whether there are any differences between more than two independent (unrelated) samples.\nAlthought, these tests can detect a difference between several groups they do not inform about which groups are different from the others. At first sight we might clarify the question by comparing all groups in pairs with t-tests or Mann-Whitney U tests. However, that procedure may lead us to the wrong conclusions (known as multiple comparisons problem).\nWhy is this procedure inappropriate? Quite simply, because we would be wrongly testing the null hypothesis. Each comparison one conducts increases the likelihood of committing at least one Type I error within a set of comparisons (famillywise Type I error rate).\nThis is the reason why, after an ANOVA or Kruskal-Wallis test concluding on a difference between groups, we should not just compare all possible pairs of groups with t-tests or Mann-Whitney U tests. Instead we perform statistical tests that take into account the number of comparisons (post hoc tests). Some of the more commonly used ones are Tukey test, Games-Howell test, and Bonferroni correction."
  },
  {
    "objectID": "lab7.html#one-way-analysis-of-variance-anova",
    "href": "lab7.html#one-way-analysis-of-variance-anova",
    "title": "17  LAB VII: Inference for numerical data (>2 samples)",
    "section": "\n17.2 One-way Analysis of Variance (ANOVA)",
    "text": "17.2 One-way Analysis of Variance (ANOVA)\nOne-way analysis of variance, usually referred to as one-way ANOVA, is a statistical test used when we want to compare several means. We may think of it as an extension of Student’s t-test to the case of more than two samples.\nOpening the file\nOpen the dataset named dataDWL from the file tab in the menu:\n\n\nFigure 17.1: The dataDWL dataset\n\n\nThe dataset dataDWL has 60 participants and includes two variables (Figure 17.1). The numeric WeightLoss variable and the Diet variable (with levels “A”, “B”, “C” and “D”).\nResearch question\nConsider the example of the variations between weight loss according to four different types of diet (A, B, C, and D). The question that may be asked is: does the average weight loss (units in kg) differ according to the diet?\nHypothesis Testsing for the ANOVA test\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): all group means are equal (the means of weight loss in the four diets are equal: \\(\\mu_{A} = \\mu_{B} = \\mu_{C} = \\mu_{D}\\))\n\n\\(H_1\\): at least one group mean differs from the others (there is at least one diet with mean weight loss different from the others)\n\n\n\nAssumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe dependent variable, WeightLoss, should be approximately normally distributed for all groups\nThe data in groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the descriptive characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) and for equality of variances (e.g., Levene’s test) can be used.\nWe select:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-> B(Exploration) -.-> C(Descriptives)\n\n\n\n\n\n\n\n\nDrag the variable WeightLoss into the Variables box and split it by the Diet variable. Additionally, select Variable across columns, as shown below (Figure 17.2):\n\n\nFigure 17.2: Split the variable WeightLoss by Diet group\n\n\nWe can now select the relevant descriptive statistics such as Percantiles, Skewness, Kurtosis and the Shapiro-Wilk test from the Statistics section (Figure 17.3):\n\n\nFigure 17.3: In the Statistics section select the descriptive statistics of interest.\n\n\nOnce we have selected our descriptive statistics, a table will appear in the output window on our right-hand side, as shown below (Figure 17.4):\n\n\nFigure 17.4: Descriptive statistics of WeightLoss by Diet group (click on image to zoom in).\n\n\nThe means are close to medians and the standard deviations are also similar indicating normal distributions for all groups. Additionally, both shape measures, skewness and (excess) kurtosis, have values in the acceptable range [-1, 1] which indicate symmetric and mesokurtic distributions, respectively.\nThe Shapiro-Wilk tests of normality suggest that the data for the WeightLoss in all groups are normally distributed (p > 0.05 \\(\\Rightarrow H_0\\) is not rejected).\n\n\n\n\n\n\nRemember: Hypothesis testing for Shapiro-Wilk test for normality\n\n\n\n\\(H_{0}\\): the data came from a normally distributed population.\n\\(H_{1}\\): the data tested are not normally distributed.\n\nIf p − value < 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nThen we can check the Density from Histograms in the Plot section, as shown below (Figure 17.5):\n\n\nFigure 17.5: In the Plot section select Density from Histograms.\n\n\nA graph (Figure 17.6) is generated in the output window on our right-hand side:\n\n\nFigure 17.6: In the Plots section select Density from Histograms.\n\n\nThe above density plots show that the data are close to symmetry and the assumption of a normal distribution is reasonable for all diet groups.\nB. Homogeneity of variance\nThe second assumption that should be satisfied is the homogeneity of variance. We observe in the summary table of Figure 17.4 that the standard deviations are similar (see also below the Levene’s test for equality of variances in Figure 16.10).\n \nRun the ANOVA test\n\n\n\n\n\n\nPerform a ANOVA test\n\n\n\nWe will perform ANOVA to test the null hypothesis that the mean WeightLoss is the same for all diet groups.\nWe select:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-> B(ANOVA) -.-> C(One-Way ANOVA)\n\n\n\n\n\n\n\n\n\n\nFigure 17.7: Conducting ANOVA test.\n\n\nThe One-Way ANOVA dialogue box opens. Drag and drop the numeric variable WeightLoss to Dependent Variables and the independent variable Diet to Grouping Variable, as shown below Figure 17.8:\n\n\nFigure 17.8: One-Way ANOVA dialogue box\n\n\nWe observe that we can select between the following two Tests: Welch’s test (the default), or Fisher’s test. At the moment, we keep the default choice. Moreover, from Additional Statistics check the Descriptive, and Descriptive plots boxes. Finally, from Assumption Checks tick the Homogeneity test. We will end up with the following screen:\n\n\nFigure 17.9: Additional statistics and tests.\n\n\nFirst, we look at the table of Levene's test for equality of variances (Figure 17.10):\n\n\nFigure 17.10: Levene’s test.\n\n\n\n\n\n\n\n\nRemember: Hypothesis testing for Levene’s test for equality of variances\n\n\n\n\\(H_{0}\\): the variances of WeightLoss in all groups are equal\n\\(H_{1}\\): the variances of WeightLoss differ between groups\n\nIf p − value < 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nSince p = 0.583 > 0.05, the \\(H_0\\) of the Levene’s test is not rejected and we have to perform the Fisher’s test (assume equal variances) (Figure 17.11). (NOTE: If the \\(p \\geq 0.05\\), then the population variances of WeightLoss in all groups are assumed equal).\n\n\nFigure 17.11: Fisher’s ANOVA test.\n\n\nIf the assumption of equal variances is not satisfied (Levene’s test gives p < 0.05, reject \\(H_0\\)), the Welch’s test should be used from the available Tests in Jamovi (Figure 17.11).\nNext, we can inspect again the results in the group descriptives table (Figure 17.12) and pertinent plots (Figure 17.12):\n\n\nFigure 17.12: Group descriptives.\n\n\n\n\nFigure 17.13: Plot of mean (95% CI) of WeightLoss by Diet.\n\n\nFinally, we present the results of the Fisher’s ANOVA test in the table of the Figure 17.14:\n\n\nFigure 17.14: The results of the Fisher’s ANOVA test.\n\n\nIn Figure 17.14, F= 6.12 indicates the obtained F-statistic= (variation between sample means \\(/\\) variation within the samples). Note that we compare this value to an F-distribution (F-test). The degrees of freedom in the numerator (df1) and the denominator (df2) are 3 and 56, respectively (numarator: variation between sample means; denominator: variation within the samples).\nThe p-value=0.001 is less than 0.05 (reject \\(H_0\\) of the ANOVA test). There is at least one diet with mean weight loss which is different from the other means.\n\n\n \nRun post-hoc tests\n\n\n\n\n\n\nPerform post-hoc tests\n\n\n\nA significant one-way ANOVA is generally followed by post-hoc tests to perform multiple pairwise comparisons between groups. From the One-Way ANOVA dialogue click on Post-Hoc Tests bar. We have got the following two options:\n\nGames-Howell (unequal variances)\nTukey (equal variances)\n\nBased on the result of Levene’s test (p = 0.583 > 0.05, the \\(H_0\\) is not rejected) (Figure 17.10), we should select the Tukey (equal variances) post-hoc test. Additionally, check the Flag significant comparisons as shown below (Figure 17.15):\n\n\nFigure 17.15: Select the appropriate post-hoc test.\n\n\nOnce we have selected our post-hoc test, a table will appear in the output window on our right-hand side, as shown below (Figure 17.16):\n\n\nFigure 17.16: Table with the results of the post-hoc tests.\n\n\nInterpretation\nPairwise comparisons were carried out using the method of Tukey and the adjusted p-values were calculated. The weight loss from diet C seems to be significantly larger than diet A (mean difference = 2.93 kg, p=0.005 <0.05) and diet B (mean difference = 3.21 kg, p=0.002 <0.05)."
  },
  {
    "objectID": "lab9.html",
    "href": "lab9.html",
    "title": "19  LAB IX: Correlation",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUnderstand the concept of correlation of two numeric variables.\nCompute Pearson’s r (or Spearmans \\(r_{s}\\)) correlation coefficient between two numeric variables\nDiscuss the possible meaning of correlation that we observe.\n\n\n\n \nIn this Lab, we will use the data from LungCapacity dataset.(Note: This starts by assuming we know how to get data into Jamovi).\nOpening the file\nOpen the dataset named LungCapacity from the file tab in the menu:\n\n\nFigure 19.1: The LungCapacity dataset\n\n\nThe dataset LungCapacity has 725 participants and includes three variables. The numeric variables of interest are the Age and the LungCap (Figure 19.1). Double-click on the variable name Age and change the measure type from nominal  to continuous .\nResearch question\nLet’s say that we want to explore the association between age (in years) and lung capacity (in liters) for the sample of 725 participants in a survey.\nHypothesis Testsing\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_{0}:\\) there is not association between Age and LungCap (\\(ρ = 0\\)).\n\\(H_{1}:\\) there is association between Age and LungCap (\\(ρ \\neq 0\\)).\n\n\n\nGraphical display with a scatter plot\nA first step that is usually useful in studying the association between two continuous variables is to prepare a scatter plot of the data. The pattern made by the points plotted on the scatter plot usually suggests the basic nature and strength of the association between two variables.\nThe flowchart of the process is:\n\n\n\n\n\nflowchart LR\n  A(Analyses tab) -.-> B(Exploration) -.-> C(Scatter plot)\n\n\n\n\n\n\n\n\nfrom the top menu, as shown below in Figure 19.2.\n\n\nFigure 19.2: In the Analyses Tab select Exploration and click on Scatterplot\n\n\nThe Scatterplot dialogue box opens, as shown below Figure 19.3:\n\n\nFigure 19.3: The Scatterplot dialogue box\n\n\nTransfer the Age and LungCap variables from the box on the left side of the screen into the X-Axis and Y-Axis boxes on the right side by highlighting the variables and pressing the Arrow Button () (alternatively, drag and drop the variables), respectively. Finally, from Marginals click on the Densities radio button. We will end up with the following screen:\n\n\nFigure 19.4: Transfer the Age variable into the X-Axis box and the LungCap variable into the Y-Axis box\n\n\nThe output should look like the following graph (Figure 19.5):\n\n\nFigure 19.5: The Scatter of Age and Lung Capacity with the marginal density plots\n\n\nThe above density plots (light blue histograms) show that the data are approximately normally distributed for both Age and LungCap (we have a large sample so the graphs are reliable).\nAdditionally, the points in the scatter plot seem to be scattered around an invisible line. The scatter plot also shows that, in general, older participants tend to have higher lung capacity (positive association).\nThe Pearson’s correlation coefficient can quantify the strength of this linear association (alternative is Spearman’s correlation coefficients).\n \nApplying the Pearson’s correlation coefficient, r\nRunning correlation in Jamovi requires only a few steps once the data is ready to go. To start, click on the Regression and then on Correlation Matrix:\n\n\n\n\n\nflowchart LR\n  A(Analyses tab) -.-> B(Regression) -.-> C(Correlation Matrix)\n\n\n\n\n\n\n\n\nfrom the top menu, as shown below in Figure 19.6.\n\n\nFigure 19.6: In the Analyses Tab select Regression and click on Correlation Matrix\n\n\nThe Correlation Matrix dialogue box opens, as shown below Figure 19.7:\n\n\nFigure 19.7: The Correlation Matrix dialogue box\n\n\nTransfer the Age and LungCap variables from the box on the left-hand side of the screen into the box on the right-hand side by highlighting the variables and pressing the Arrow Button () (alternatively, drag and drop the variables), respectively.\nIn the Correlation Coefficients section we can select between the following three: Pearson’s, Spearman, or Kendall’s coefficient. We keep the default choice of Pearson. Finally, from Additional Options check the Confidence Intervals box. We will end up with the following screen:\n\n\nFigure 19.8: Transfer the Age and LungCap variables from the left-hand side to the right-hand side box and check the Confidence Intervals box\n\n\nThe output table should look like the following (Figure 19.9):\n\n\nFigure 19.9: The output table of correlation coefficient\n\n\n\n\n\n\n\n\nInterpretation of the results\n\n\n\nThere is evidence of a very strong, positive, linear association between Age and Lung Capacity (r= 0.82, 95% CI: 0.79 to 0.84, p < 0.001) which is significant."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Altman, Naomi, and Martin Krzywinski. 2015. “Association,\nCorrelation and Causation.” Nature Methods 12 (10):\n899–900. https://doi.org/10.1038/nmeth.3587.\n\n\nCao, Shiyi, Chen Yang, Yong Gan, and Zuxun Lu. 2015. “The Health\nEffects of Passive Smoking: An Overview of Systematic Reviews Based on\nObservational Epidemiological Evidence.” Edited by Yan Li.\nPLOS ONE 10 (10): e0139907. https://doi.org/10.1371/journal.pone.0139907.\n\n\nDoll, Richard, Richard Peto, Jillian Boreham, and Isabelle Sutherland.\n2004. “Mortality in Relation to Smoking: 50 Years’ Observations on\nMale British Doctors.” BMJ 328 (7455): 1519. https://doi.org/10.1136/bmj.38142.554479.ae.\n\n\nEnstrom, J. E. 2003. “Environmental Tobacco Smoke and Tobacco\nRelated Mortality in a Prospective Study of Californians,\n1960-98.” BMJ 326 (7398): 1057–50. https://doi.org/10.1136/bmj.326.7398.1057.\n\n\nFarndon, Lisa J, Wesley Vernon, Stephen J Walters, Simon Dixon, Mike\nBradburn, Michael Concannon, and Julia Potter. 2013. “The\nEffectiveness of Salicylic Acid Plasters Compared with\n‘Usual’ Scalpel Debridement of Corns: A Randomised\nControlled Trial.” Journal of Foot and Ankle Research 6\n(1): 40. https://doi.org/10.1186/1757-1146-6-40.\n\n\nHansen, Tine W., Lutgarde Thijs, Yan Li, Jose'Boggia, Masahiro Kikuya,\nKristina Bjor̈klund-Bodega, Tom Richart, et al. 2010. “Prognostic\nValue of Reading-to-Reading Blood Pressure Variability Over 24 Hours in\n8938 Subjects From 11 Populations.” Hypertension 55 (4):\n1049–57. https://doi.org/10.1161/hypertensionaha.109.140798.\n\n\nO’Sullivan, J J, G Derrick, P Griggs, R Foxall, M Aitkin, and C Wren.\n1999. “Ambulatory Blood Pressure in Schoolchildren.”\nArchives of Disease in Childhood 80 (6): 529–32. https://doi.org/10.1136/adc.80.6.529.\n\n\nRigby, Alan S, Gillian K Armstrong, Michael J Campbell, and Nick\nSummerton. 2004. “A Survey of Statistics in Three UK General\nPractice Journal.” BMC Medical Research Methodology 4\n(1). https://doi.org/10.1186/1471-2288-4-28.\n\n\nStrasak, Alexander M, Qamruz Zaman, Gerhard Marinell, Karl P Pfeiffer,\nand Hanno Ulmer. 2007. “The Use of Statistics in Medical\nResearch.” The American Statistician 61 (1): 47–55. https://doi.org/10.1198/000313007x170242.\n\n\nWilkens, Philip, Inger B. Scheel, Oliver Grundnes, Christian Hellum, and\nKjersti Storheim. 2010. “Effect of Glucosamine on Pain-Related\nDisability in Patients With Chronic Low Back Pain and Degenerative\nLumbar Osteoarthritis.” JAMA 304 (1): 45. https://doi.org/10.1001/jama.2010.893."
  }
]