# Correlation {#sec-correlation}

```{r}
#| include: false

library(tidyverse)
library(here)

library(stevemisc)
library(patchwork)

library(readr)
data_bp <- read_csv(here("data", "data_bp.csv"))
```

When we have finished this chapter, we should be able to:

::: {.callout-caution icon="false"}
## Learning objectives

-   Explain the concept of correlation of two numeric variables.
-   Understand the most commonly used correlation coefficients, Pearson's r and Spearman's $r_{s}$ coefficients.
:::

## What is correlation?

**Correlation** is a statistical method used to assess a possible **association** between two numeric variables. There are several statistics that we can use to quantify correlation depending on the underlying relation of the data. In this chapter, we'll learn about two correlation coefficients:

-   Pearson's $r$

-   Spearman's $r_{s}$

Pearson's coefficient measures **linear** correlation, while the Spearman's coefficient compare the **ranks** of data and measures **monotonic** associations.

 

## Linear correlation (Pearson's $r$ coefficient)

### Graphical display with a scatter plot

The most useful graph for displaying the association between two numeric variables is a scatter plot. @fig-correlation0 shows the association between systolic blood pressure (sbp) and diastolic blood pressure (dpb) in 96 patients with carotid artery disease, aged 42-89, prior to surgery. (Note that sbp and dpb can be plotted on either axis).

::: {.callout-note icon="false"}
## Example-Association between systolic and diastolic blood pressure

```{r}
#| echo: false
#| warning: false
#| label: fig-correlation0
#| fig-cap: Scatter plot of the association between systolic blood pressure (sbp) and diastolic blood pressure (dbp) in 96 patients with carotid artery disease, aged 42-89, prior to surgery.
#| fig-width: 9.0
#| fig-height: 6.5


# plot 0
data_bp|>
  ggplot(aes(dbp, sbp)) +
  geom_point(color = "deeppink", size = 3 ,alpha = 0.7) +
  #ggtitle(paste0("Negative Correlation r=", round(cor1, digits=1))) +
  theme_steve()
```
:::

From the scatter plot, there appears to be a linear association between sbp and dbp, with higher values of dbp being associated with higher values of sbp. How can we summarize this association simply? We could calculate the Pearson's correlation coefficient, $r$, which is a measure of the linear association between two numeric variables.



### Pearson's ${r}$ correlation coefficient 

The **Pearson's correlation coefficient**, ${r}$, can be calculated for any dataset with two numeric variables. However, before we calculate the Pearson's ${r}$ coefficient we should make sure that the following **assumptions** are met:

::: {.callout-warning icon="false"}
## **Assumptions for Pearson's** $r$ coefficient

1.  There is a linear association between the two variables
2.  The variables are observed on a random sample of individuals (each individual should have a pair of values).\
3.  For a valid hypothesis testing and calculation of confidence intervals both variables should have an approximately normal distribution.
4.  Absence of outliers in the data set.
:::

 

::: {.callout-tip icon="false"}
## Characteristics of Pearson's coefficient $r$

**Formula**

Given a set of ${n}$ pairs of observations $(x_{1},y_{1}),\ldots ,(x_{n},y_{n})$ with means $\bar{x}$ and $\bar{y}$ respectively, $r$ is defined as:

$$r = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2 \sum_{i=1}^n(y_i - \bar{y})^2}}$$ {#eq-r1}

The $r$ statistic shows the **direction** and measures the **strength** of the linear association between the variables.

**Range of values**

Correlation coefficient is a **dimensionless** quantity that takes a value in the range **-1** to **+1**.

 

**Direction of the association**

A **negative** correlation coefficient indicates that one variable decreases in value as the other variable increases (and vice versa), a **zero** value indicates that no association exists between the two variables, and a **positive** coefficient indicates that both variables increase (or decrease) in value together.

```{r}
#| echo: false
#| warning: false
#| label: fig-correlation1
#| fig-cap: The direction of association can be (a) negative, (b) no association, or (c) positive.
#| fig-width: 11.8
#| fig-height: 4.0


# plot 1
sigma1<-rbind(c(1,-0.8), c(-0.8,1))
mu1<-c(10, 5) 

set.seed(123)
dat1 <- data.frame(MASS::mvrnorm(n=500, mu=mu1, Sigma=sigma1))

cor1 <- cor(dat1$X1, dat1$X2)

pcor1 <- dat1|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3", alpha = 0.8) +
  ggtitle(paste0("Negative Correlation r=", round(cor1, digits=1))) +
  theme_steve()


# plot2
sigma2<-rbind(c(1,0.000), c(0.000,1))
mu2<-c(10, 5) 

set.seed(124)
dat2 <- data.frame(MASS::mvrnorm(n=500, mu=mu2, Sigma=sigma2))

cor2 <- cor(dat2$X1, dat2$X2)

pcor2 <- dat2|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "grey80") +
  ggtitle(paste0("Zero Correlation r=", round(cor2, digits=1))) +
  theme_steve()


# plot3
sigma3<-rbind(c(1,0.8), c(0.8,1))
mu3<-c(10, 5)

set.seed(125)
dat3 <- data.frame(MASS::mvrnorm(n=500, mu=mu3, Sigma=sigma3))

cor3 <- cor(dat3$X1, dat3$X2)

pcor3 <- dat3|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink", alpha = 0.8) +
  ggtitle(paste0("Positive Correlation r=", round(cor3, digits=1))) +
  theme_steve()


pcor1 + pcor2 + pcor3 + plot_annotation(tag_levels = 'a')
```

 

**Magnitude of the association**

The **magnitude** of association can be anywhere between -1 and +1. The **stronger** the correlation, the **closer** the correlation coefficient comes to ±1 (@fig-correlation2). A correlation coefficient of **-1** or **+1** indicates a **perfect** negative or positive association, respectively (@fig-correlation2 c and f).

```{r}
#| echo: false
#| warning: false
#| label: fig-correlation2
#| fig-cap: The **stronger** the correlation, the **closer** the correlation coefficient comes to ±1.
#| fig-width: 11.8
#| fig-height: 8.0


# plot 4
sigma4<-rbind(c(1,1), c(1,1))
mu4<-c(10, 5) 

set.seed(136)
dat4 <- data.frame(MASS::mvrnorm(n=500, mu=mu4, Sigma=sigma4))

cor4 <- cor(dat4$X1, dat4$X2)

pcor4 <- dat4|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink") +
  ggtitle(paste0("r=", round(cor4, digits=1))) +
  theme_steve()


# plot5
sigma5<-rbind(c(1,0.8), c(0.8,1))
mu5<-c(10, 5) 
set.seed(139)
dat5 <- data.frame(MASS::mvrnorm(n=500, mu=mu5, Sigma=sigma5))

cor5 <- cor(dat5$X1, dat5$X2)

pcor5 <- dat5|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink", alpha = 0.8) +
  ggtitle(paste0("r=", round(cor5, digits=1))) +
  theme_steve()




# plot6
sigma6<-rbind(c(1,0.6), c(0.6,1))
mu6<-c(10, 5) 
set.seed(138)
dat6 <- data.frame(MASS::mvrnorm(n=500, mu=mu6, Sigma=sigma6))

cor6 <- cor(dat6$X1, dat6$X2)

pcor6 <- dat6|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink", alpha = 0.6) +
  ggtitle(paste0("r=", round(cor6, digits=1))) +
  theme_steve()



# plot 7
sigma7<-rbind(c(1,-1), c(-1,1))
mu7<-c(10, 5) 

set.seed(146)
dat7 <- data.frame(MASS::mvrnorm(n=500, mu=mu7, Sigma=sigma7))

cor7 <- cor(dat7$X1, dat7$X2)

pcor7 <- dat7|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3") +
  ggtitle(paste0("r=", round(cor7, digits=1))) +
  theme_steve()


# plot8
sigma8<-rbind(c(1,-0.8), c(-0.8,1))
mu8<-c(10, 5) 
set.seed(149)
dat8 <- data.frame(MASS::mvrnorm(n=500, mu=mu8, Sigma=sigma8))

cor8 <- cor(dat8$X1, dat8$X2)

pcor8 <- dat8|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3", alpha = 0.8) +
  ggtitle(paste0("r=", round(cor8, digits=1))) +
  theme_steve()


# plot9
sigma9<-rbind(c(1,-0.6), c(-0.6,1))
mu9<-c(10, 5) 
set.seed(148)
dat9 <- data.frame(MASS::mvrnorm(n=500, mu=mu9, Sigma=sigma9))

cor9 <- cor(dat9$X1, dat9$X2)

pcor9 <- dat9|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3", alpha = 0.6) +
  ggtitle(paste0("r=", round(cor9, digits=1))) +
  theme_steve()


(pcor9 + pcor8 + pcor7) / (pcor6 + pcor5 + pcor4)   + plot_annotation(tag_levels = 'a')
```
:::


Using the data in @fig-correlation0 and the @eq-r1 we find the sample correlation coefficient is r=0.62.

 

::: callout-important
## **Correlation is not causation**

Any observed association is not necessarily assumed to be a causal one- it may be caused by other factors.

As an example, suppose we observe that people who daily drink more than 4 cups of coffee have a decreased chance of developing skin cancer. This does not necessarily mean that coffee confers resistance to cancer; one alternative explanation would be that people who drink a lot of coffee work indoors for long hours and thus have little exposure to the sun, a known risk. If this is the case, then the number of hours spent outdoors is a **confounding variable**---a cause common to both observations. In such a situation, a direct causal link cannot be inferred; the association merely suggests a hypothesis, such as a common cause, but does not offer proof. In addition, when many variables in complex systems are studied, spurious associations can arise. Thus, **association does not imply causation** [@altman2015].
:::

 

### Hypothesis Testing for Pearson's $r$ correlation coefficient

::: {.callout-note icon="false"}
## Step 1: Determine the appropriate null hypothesis and alternative hypothesis

- The null hypothesis, $H_{0}$, states that the population correlation, ρ, is zero ($ρ = 0$). There is not association between dbp and spb.


- The alternative hypothesis, $H_{1}$, states that the population correlation, ρ, is zero ($ρ \neq 0$). There is association between dbp and spb.
:::



::: {.callout-note icon="false"}
## Step 2: Set the level of significance, α

We set the value α=0.05 for the level of significance (type I error).
:::


::: {.callout-note icon="false"}
## Step 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic.

To test whether ρ is significantly different from zero, $ρ \neq 0$, we calculate the test statistic:

$$t = \frac{r}{SE_{r}}=\frac{r}{\sqrt{(1-r^2)/(n-2)}}$$ {#eq-r2}

where n is the sample size and $SE_{r}=\sqrt{ \frac{(1-r^2)}{(n-2)}}$.


For the data in our example, the number of observations are n= 96, r= 0.62 and $SE_{r}=\sqrt{ \frac{(1-0.62^2)}{(96-2)}}= \sqrt{ \frac{(1-0.3844)}{94}} = \sqrt{\frac{0.6156}{94}}= 0.081$.

According to @eq-r2:

$$t = \frac{r}{SE_{r}}= \frac{0.62}{0.081}= 7.65$$

:::



::: {.callout-note icon="false"}
## Step 4: Decide whether or not the result is statistically significant

When we run the test, we get a value for the t-statistic (t= 7.65) that we compare with the t-distribution with n-2 degrees of freedom. We also get a p-value < 0.001 which is less than α=0.05 (so, reject $H_{0}$). 

To find a 95% confidence interval for ρ we have to use a Fisher’s z transformation to get a quantity $Z_{r}$ that has approximately Normal distribution. The Fisher’s z transformation of sample correlation coefficient r is:

$$ Z_{r}= \frac{1}{2} ln \frac{1+r}{1-r} $$ {#eq-r3}




The 95% CI of the $Z_{r}$ is: 

$$ 95\%CI= z_{r} \ \pm 1.96 \cdot SE{z_{r}}= z_{r} \ \pm \frac{1.96}{\sqrt{n-3}}=[z_{r_{L}}, z_{r_{U}}]$$ {#eq-r4}


where $SE{z_{r}}=\frac{1}{\sqrt{n-3}}$ and $z_{r_{L}}, z_{r_{U}}$ the lower and upper limits of the 95%CI of $Z_{r}$ respectively.

Finally, we invert the confidence limits of $Z_{r}$; then the lower and upper limits of the 95%CI of ρ are:

$$ ρ_{L}= \frac{e^{2 \cdot z_{r_{L}}}-1}{e^{2 \cdot z_{r_{L}}}+1}= 0.48 $$ {#eq-r5}
$$ ρ_{L}= \frac{e^{2 \cdot z_{r_{U}}}-1}{e^{2 \cdot z_{r_{U}}}+1}= 0.73 $$ {#eq-r6}


The 95% CI calculated from @eq-r5 and @eq-r6 is 0.48 to 0.73, so there are quite a wide range of plausible correlation values associated with these data. Additonally, note that the 95% CI of ρ is asymmetric.

:::


::: {.callout-note icon="false"}
## Step 5: Interpret the results

There is evidence of a positive association between dbp and sbp (r= 0.62, 95%CI: 0.48, 0.73, p < 0.001).

:::


 

## Rank correlation (Spearman's $r_{s}$ coefficient)
